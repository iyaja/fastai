(window.webpackJsonp=window.webpackJsonp||[]).push([[63],{429:function(t,e,a){"use strict";a.r(e);var s=a(42),n=Object(s.a)({},(function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"transfer-learning-in-text"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#transfer-learning-in-text"}},[t._v("#")]),t._v(" Transfer learning in text")]),t._v(" "),a("blockquote",[a("p",[t._v("How to fine-tune a language model and train a classifier")])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" fastai"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("all")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n")])])]),a("p",[t._v("In this tutorial, we will see how we can train a model to classify text (here based on their sentiment). First we will see how to do this quickly in a few lines of code, then how to get state-of-the art results using the approach of the "),a("a",{attrs:{href:"https://arxiv.org/abs/1801.06146",target:"_blank",rel:"noopener noreferrer"}},[t._v("ULMFit paper"),a("OutboundLink")],1),t._v(".")]),t._v(" "),a("p",[t._v("We will use the IMDb dataset from the paper "),a("a",{attrs:{href:"(https://ai.stanford.edu/~amaas/data/sentiment/)"}},[t._v("Learning Word Vectors for Sentiment Analysis")]),t._v(", containing a few thousand movie reviews.")]),t._v(" "),a("h2",{attrs:{id:"train-a-text-classifier-from-a-pretrained-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#train-a-text-classifier-from-a-pretrained-model"}},[t._v("#")]),t._v(" Train a text classifier from a pretrained model")]),t._v(" "),a("p",[t._v("We will try to train a classifier using a pretrained model, a bit like we do in the "),a("a",{attrs:{href:"http://docs.fast.ai/tutorial.vision",target:"_blank",rel:"noopener noreferrer"}},[t._v("vision tutorial"),a("OutboundLink")],1),t._v(". To get our data ready, we will first use the high-level API:")]),t._v(" "),a("h2",{attrs:{id:"using-the-high-level-api"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#using-the-high-level-api"}},[t._v("#")]),t._v(" Using the high-level API")]),t._v(" "),a("p",[t._v("We can download the data and decompress it with the following command:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" untar_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("URLs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("IMDB"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ls"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[t._v("(#5) [Path('/home/sgugger/.fastai/data/imdb/unsup'),Path('/home/sgugger/.fastai/data/imdb/models'),Path('/home/sgugger/.fastai/data/imdb/train'),Path('/home/sgugger/.fastai/data/imdb/test'),Path('/home/sgugger/.fastai/data/imdb/README')]\n")])])]),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ls"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[t._v("(#4) [Path('/home/sgugger/.fastai/data/imdb/train/pos'),Path('/home/sgugger/.fastai/data/imdb/train/unsupBow.feat'),Path('/home/sgugger/.fastai/data/imdb/train/labeledBow.feat'),Path('/home/sgugger/.fastai/data/imdb/train/neg')]\n")])])]),a("p",[t._v("The data follows an ImageNet-style organization, in the train folder, we have two subfolders, "),a("code",[t._v("pos")]),t._v(" and "),a("code",[t._v("neg")]),t._v(" (for positive reviews and negative reviews). We can gather it by using the "),a("RouterLink",{attrs:{to:"/text.data.html#TextDataLoaders.from_folder"}},[a("code",[t._v("TextDataLoaders.from_folder")])]),t._v(' method. The only thing we need to specify is the name of the validation folder, which is "test" (and not the default "valid").')],1),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("dls "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TextDataLoaders"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_folder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("untar_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("URLs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("IMDB"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valid"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'test'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("We can then have a look at the data with the "),a("code",[t._v("show_batch")]),t._v(" method:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("dls"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("table",{staticClass:"dataframe",attrs:{border:"1"}},[a("thead",[a("tr",{staticStyle:{"text-align":"right"}},[a("th"),t._v(" "),a("th",[t._v("text")]),t._v(" "),a("th",[t._v("category")])])]),t._v(" "),a("tbody",[a("tr",[a("th",[t._v("0")]),t._v(" "),a("td",[t._v("xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero")]),t._v(" "),a("td",[t._v("pos")])]),t._v(" "),a("tr",[a("th",[t._v("1")]),t._v(" "),a("td",[t._v("xxbos xxmaj warning : xxmaj does contain spoilers . \\n\\n xxmaj open xxmaj your xxmaj eyes \\n\\n xxmaj if you have not seen this film and plan on doing so , just stop reading here and take my word for it . xxmaj you have to see this film . i have seen it four times so far and i still have n't made up my mind as to what exactly happened in the film . xxmaj that is all i am going to say because if you have not seen this film , then stop reading right now . \\n\\n xxmaj if you are still reading then i am going to pose some questions to you and maybe if anyone has any answers you can email me and let me know what you think . \\n\\n i remember my xxmaj grade 11 xxmaj english teacher quite well . xxmaj")]),t._v(" "),a("td",[t._v("pos")])]),t._v(" "),a("tr",[a("th",[t._v("2")]),t._v(" "),a("td",[t._v("xxbos i thought that xxup rotj was clearly the best out of the three xxmaj star xxmaj wars movies . i find it surprising that xxup rotj is considered the weakest installment in the xxmaj trilogy by many who have voted . xxmaj to me it seemed like xxup rotj was the best because it had the most profound plot , the most suspense , surprises , most xxunk the ending ) and definitely the most episodic movie . i personally like the xxmaj empire xxmaj strikes xxmaj back a lot also but i think it is slightly less good than than xxup rotj since it was slower - moving , was not as episodic , and i just did not feel as much suspense or emotion as i did with the third movie . \\n\\n xxmaj it also seems like to me that after reading these surprising reviews that")]),t._v(" "),a("td",[t._v("pos")])]),t._v(" "),a("tr",[a("th",[t._v("3")]),t._v(" "),a("td",[t._v("xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the sweetest and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that tries too hard , nor does it come up with the oddest possible scenarios to get the two protagonists together in the end . xxmaj in fact , all its charm is innate , contained within the characters and the setting and the plot … which is highly believable to boot . xxmaj it 's easy to think that such a love story , as beautiful as any other ever told , * could * happen to you … a feeling you do n't often get from other romantic comedies")]),t._v(" "),a("td",[t._v("pos")])]),t._v(" "),a("tr",[a("th",[t._v("4")]),t._v(" "),a("td",[t._v("xxbos xxmaj the premise of this movie has been tickling my imagination for quite some time now . xxmaj we 've all heard or read about it in some kind of con - text . xxmaj what would you do if you were all alone in the world ? xxmaj what would you do if the entire world suddenly disappeared in front of your eyes ? xxmaj in fact , the last part is actually what happens to xxmaj dave and xxmaj andrew , two room - mates living in a run - down house in the middle of a freeway system . xxmaj andrew is a nervous wreck to say the least and xxmaj dave is considered being one of the biggest losers of society . xxmaj that alone is the main reason to why these two guys get so well along , because they simply only have each")]),t._v(" "),a("td",[t._v("pos")])]),t._v(" "),a("tr",[a("th",[t._v("5")]),t._v(" "),a("td",[t._v("xxbos xxrep 3 * xxup spoilers xxrep 3 * xxrep 3 * xxup spoilers xxrep 3 * xxmaj continued … \\n\\n xxmaj from here on in the whole movie collapses in on itself . xxmaj first we meet a rogue program with the indication we 're gon na get ghosts and vampires and werewolves and the like . xxmaj we get a guy with a retarded accent talking endless garbage , two ' ghosts ' that serve no real purpose and have no character what - so - ever and a bunch of henchmen . xxmaj someone 's told me they 're vampires ( straight out of xxmaj blade 2 ) , but they 're so undefined i did n't realise . \\n\\n xxmaj the funny accented guy with a ridiculous name suffers the same problem as the xxmaj oracle , only for far longer and far far worse .")]),t._v(" "),a("td",[t._v("neg")])]),t._v(" "),a("tr",[a("th",[t._v("6")]),t._v(" "),a("td",[t._v('xxbos xxmaj i \'ve rented and watched this movie for the 1st time on xxup dvd without reading any reviews about it . xxmaj so , after 15 minutes of watching xxmaj i \'ve noticed that something is wrong with this movie ; it \'s xxup terrible ! i mean , in the trailers it looked scary and serious ! \\n\\n i think that xxmaj eli xxmaj roth ( mr . xxmaj director ) thought that if all the characters in this film were stupid , the movie would be funny … ( so stupid , it \'s funny … ? xxup wrong ! ) xxmaj he should watch and learn from better horror - comedies such xxunk xxmaj night " , " the xxmaj lost xxmaj boys " and " the xxmaj return xxmaj of the xxmaj living xxmaj dead " ! xxmaj those are funny ! \\n\\n "')]),t._v(" "),a("td",[t._v("neg")])]),t._v(" "),a("tr",[a("th",[t._v("7")]),t._v(" "),a("td",[t._v("xxbos xxup myra xxup breckinridge is one of those rare films that established its place in film history immediately . xxmaj praise for the film was absolutely nonexistent , even from the people involved in making it . xxmaj this film was loathed from day one . xxmaj while every now and then one will come across some maverick who will praise the film on philosophical grounds ( aggressive feminism or the courage to tackle the issue of xxunk ) , the film has not developed a cult following like some notorious flops do . xxmaj it 's not hailed as a misunderstood masterpiece like xxup scarface , or trotted out to be ridiculed as a camp classic like xxup showgirls . \\n\\n xxmaj undoubtedly the reason is that the film , though outrageously awful , is not lovable , or even likable . xxup myra xxup breckinridge is just")]),t._v(" "),a("td",[t._v("neg")])]),t._v(" "),a("tr",[a("th",[t._v("8")]),t._v(" "),a("td",[t._v("xxbos xxmaj after reading the previous comments , xxmaj i 'm just glad that i was n't the only person left confused , especially by the last 20 minutes . xxmaj john xxmaj carradine is shown twice walking down into a grave and pulling the lid shut after him . i anxiously awaited some kind of explanation for this odd behavior … naturally i assumed he had something to do with the evil goings - on at the house , but since he got killed off by the first rising corpse ( hereafter referred to as xxmaj zombie # 1 ) , these scenes made absolutely no sense . xxmaj please , if someone out there knows why xxmaj carradine kept climbing down into graves -- let the rest of us in on it ! ! \\n\\n xxmaj all the action is confined to the last 20 minutes so xxmaj")]),t._v(" "),a("td",[t._v("neg")])])])]),t._v(" "),a("p",[t._v("We can see that the library automatically processed all the texts to split then in "),a("em",[t._v("tokens")]),t._v(", adding some special tokens like:")]),t._v(" "),a("ul",[a("li",[a("code",[t._v("xxbos")]),t._v(" to indicate the beginning of a text")]),t._v(" "),a("li",[a("code",[t._v("xxmaj")]),t._v(" to indicate the next word was capitalized")])]),t._v(" "),a("p",[t._v("Then, we can define a "),a("RouterLink",{attrs:{to:"/learner.html#Learner"}},[a("code",[t._v("Learner")])]),t._v(" suitable for text classification in one line:")],1),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("learn "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" text_classifier_learner"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AWD_LSTM"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" drop_mult"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("accuracy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("We use the "),a("a",{attrs:{href:"https://arxiv.org/abs/1708.02182",target:"_blank",rel:"noopener noreferrer"}},[t._v("AWD LSTM"),a("OutboundLink")],1),t._v(" architecture, "),a("code",[t._v("drop_mult")]),t._v(" is a parameter that controls the magnitude of all dropouts in that model, and we use "),a("RouterLink",{attrs:{to:"/metrics.html#accuracy"}},[a("code",[t._v("accuracy")])]),t._v(" to track down how well we are doing. We can then fine-tune our pretrained model:")],1),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("learn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fine_tune"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("table",{staticClass:"dataframe",attrs:{border:"1"}},[a("thead",[a("tr",{staticStyle:{"text-align":"left"}},[a("th",[t._v("epoch")]),t._v(" "),a("th",[t._v("train_loss")]),t._v(" "),a("th",[t._v("valid_loss")]),t._v(" "),a("th",[t._v("accuracy")]),t._v(" "),a("th",[t._v("time")])])]),t._v(" "),a("tbody",[a("tr",[a("td",[t._v("0")]),t._v(" "),a("td",[t._v("0.587251")]),t._v(" "),a("td",[t._v("0.386230")]),t._v(" "),a("td",[t._v("0.828960")]),t._v(" "),a("td",[t._v("01:35")])])])]),t._v(" "),a("table",{staticClass:"dataframe",attrs:{border:"1"}},[a("thead",[a("tr",{staticStyle:{"text-align":"left"}},[a("th",[t._v("epoch")]),t._v(" "),a("th",[t._v("train_loss")]),t._v(" "),a("th",[t._v("valid_loss")]),t._v(" "),a("th",[t._v("accuracy")]),t._v(" "),a("th",[t._v("time")])])]),t._v(" "),a("tbody",[a("tr",[a("td",[t._v("0")]),t._v(" "),a("td",[t._v("0.307347")]),t._v(" "),a("td",[t._v("0.263843")]),t._v(" "),a("td",[t._v("0.892800")]),t._v(" "),a("td",[t._v("03:03")])]),t._v(" "),a("tr",[a("td",[t._v("1")]),t._v(" "),a("td",[t._v("0.215867")]),t._v(" "),a("td",[t._v("0.226208")]),t._v(" "),a("td",[t._v("0.911800")]),t._v(" "),a("td",[t._v("02:55")])]),t._v(" "),a("tr",[a("td",[t._v("2")]),t._v(" "),a("td",[t._v("0.155399")]),t._v(" "),a("td",[t._v("0.231144")]),t._v(" "),a("td",[t._v("0.913960")]),t._v(" "),a("td",[t._v("03:12")])]),t._v(" "),a("tr",[a("td",[t._v("3")]),t._v(" "),a("td",[t._v("0.129277")]),t._v(" "),a("td",[t._v("0.200941")]),t._v(" "),a("td",[t._v("0.925920")]),t._v(" "),a("td",[t._v("03:01")])])])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("learn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fine_tune"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("table",{staticClass:"dataframe",attrs:{border:"1"}},[a("thead",[a("tr",{staticStyle:{"text-align":"left"}},[a("th",[t._v("epoch")]),t._v(" "),a("th",[t._v("train_loss")]),t._v(" "),a("th",[t._v("valid_loss")]),t._v(" "),a("th",[t._v("accuracy")]),t._v(" "),a("th",[t._v("time")])])]),t._v(" "),a("tbody",[a("tr",[a("td",[t._v("0")]),t._v(" "),a("td",[t._v("0.594912")]),t._v(" "),a("td",[t._v("0.407416")]),t._v(" "),a("td",[t._v("0.823640")]),t._v(" "),a("td",[t._v("01:35")])])])]),t._v(" "),a("table",{staticClass:"dataframe",attrs:{border:"1"}},[a("thead",[a("tr",{staticStyle:{"text-align":"left"}},[a("th",[t._v("epoch")]),t._v(" "),a("th",[t._v("train_loss")]),t._v(" "),a("th",[t._v("valid_loss")]),t._v(" "),a("th",[t._v("accuracy")]),t._v(" "),a("th",[t._v("time")])])]),t._v(" "),a("tbody",[a("tr",[a("td",[t._v("0")]),t._v(" "),a("td",[t._v("0.268259")]),t._v(" "),a("td",[t._v("0.316242")]),t._v(" "),a("td",[t._v("0.876000")]),t._v(" "),a("td",[t._v("03:03")])]),t._v(" "),a("tr",[a("td",[t._v("1")]),t._v(" "),a("td",[t._v("0.184861")]),t._v(" "),a("td",[t._v("0.246242")]),t._v(" "),a("td",[t._v("0.898080")]),t._v(" "),a("td",[t._v("03:10")])]),t._v(" "),a("tr",[a("td",[t._v("2")]),t._v(" "),a("td",[t._v("0.136392")]),t._v(" "),a("td",[t._v("0.220086")]),t._v(" "),a("td",[t._v("0.918200")]),t._v(" "),a("td",[t._v("03:16")])]),t._v(" "),a("tr",[a("td",[t._v("3")]),t._v(" "),a("td",[t._v("0.106423")]),t._v(" "),a("td",[t._v("0.191092")]),t._v(" "),a("td",[t._v("0.931360")]),t._v(" "),a("td",[t._v("03:15")])])])]),t._v(" "),a("p",[t._v("Not too bad! To see how well our model is doing, we can use the "),a("code",[t._v("show_results")]),t._v(" method:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("learn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_results"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("table",{staticClass:"dataframe",attrs:{border:"1"}},[a("thead",[a("tr",{staticStyle:{"text-align":"right"}},[a("th"),t._v(" "),a("th",[t._v("text")]),t._v(" "),a("th",[t._v("category")]),t._v(" "),a("th",[t._v("category_")])])]),t._v(" "),a("tbody",[a("tr",[a("th",[t._v("0")]),t._v(" "),a("td",[t._v("xxbos xxmaj there 's a sign on xxmaj the xxmaj lost xxmaj highway that says : \\n\\n * major xxup spoilers xxup ahead * \\n\\n ( but you already knew that , did n't you ? ) \\n\\n xxmaj since there 's a great deal of people that apparently did not get the point of this movie , xxmaj i 'd like to contribute my interpretation of why the plot makes perfect sense . xxmaj as others have pointed out , one single viewing of this movie is not sufficient . xxmaj if you have the xxup dvd of xxup md , you can \" cheat \" by looking at xxmaj david xxmaj lynch 's \" top 10 xxmaj hints to xxmaj unlocking xxup md \" ( but only upon second or third viewing , please . ) ;) \\n\\n xxmaj first of all , xxmaj mulholland xxmaj drive is")]),t._v(" "),a("td",[t._v("pos")]),t._v(" "),a("td",[t._v("pos")])]),t._v(" "),a("tr",[a("th",[t._v("1")]),t._v(" "),a("td",[t._v("xxbos ( some spoilers included : ) \\n\\n xxmaj although , many commentators have called this film surreal , the term fits poorly here . xxmaj to quote from xxmaj encyclopedia xxmaj xxunk 's , surreal means : \\n\\n \" fantastic or incongruous imagery \" : xxmaj one need n't explain to the unimaginative how many ways a plucky ten - year - old boy at large and seeking his fortune in the driver 's seat of a red xxmaj mustang could be fantastic : those curious might read xxmaj james xxmaj kincaid ; but if you asked said lad how he were incongruous behind the wheel of a sports car , he 'd surely protest , \" no way ! \" xxmaj what fantasies and incongruities the film offers mostly appear within the first fifteen minutes . xxmaj thereafter we get more iterations of the same , in an")]),t._v(" "),a("td",[t._v("pos")]),t._v(" "),a("td",[t._v("neg")])]),t._v(" "),a("tr",[a("th",[t._v("2")]),t._v(" "),a("td",[t._v("xxbos xxmaj hearkening back to those \" good xxmaj old xxmaj days \" of 1971 , we can vividly recall when we were treated with a whole xxmaj season of xxmaj charles xxmaj chaplin at the xxmaj cinema . xxmaj that 's what the promotional guy called it when we saw him on somebody 's old talk show . ( we ca n't recall just whose it was ; either xxup merv xxup griffin or xxup woody xxup woodbury , one or the other ! ) xxmaj the guest talked about xxmaj sir xxmaj charles ' career and how his films had been out of circulation ever since the 1952 exclusion of the former \" little xxmaj tramp ' from xxmaj los xxmaj xxunk xxmaj xxunk on the grounds of his being an \" undesirable xxmaj alien \" . ( no xxmaj schultz , he 's xxup not from another")]),t._v(" "),a("td",[t._v("pos")]),t._v(" "),a("td",[t._v("pos")])]),t._v(" "),a("tr",[a("th",[t._v("3")]),t._v(" "),a("td",[t._v('xxbos " buffalo xxmaj bill , xxmaj hero of the xxmaj far xxmaj west " director xxmaj mario xxmaj costa \'s unsavory xxmaj spaghetti western " the xxmaj beast " with xxmaj klaus xxmaj kinski could only have been produced in xxmaj europe . xxmaj hollywood would never dared to have made a western about a sexual predator on the prowl as the protagonist of a movie . xxmaj never mind that xxmaj kinski is ideally suited to the role of \' crazy \' xxmaj johnny . xxmaj he plays an individual entirely without sympathy who is ironically dressed from head to toe in a white suit , pants , and hat . xxmaj this low - budget oater has nothing appetizing about it . xxmaj the typically breathtaking xxmaj spanish scenery around xxmaj almeria is nowhere in evidence . xxmaj instead , xxmaj costa and his director of photography')]),t._v(" "),a("td",[t._v("pos")]),t._v(" "),a("td",[t._v("pos")])]),t._v(" "),a("tr",[a("th",[t._v("4")]),t._v(" "),a("td",[t._v("xxbos xxmaj if you 've seen the trailer for this movie , you pretty much know what to expect , because what you see here is what you get . xxmaj and even if you have n't seen the previews , it wo n't take you long to pick up on what you 're in for-- specifically , a good time and plenty of xxunk from this clever satire of ` reality xxup tv ' shows and ` buddy xxmaj cop ' movies , ` showtime , ' directed by xxmaj tom xxmaj dey , starring xxmaj robert xxmaj de xxmaj niro and xxmaj eddie xxmaj murphy . \\n\\n\\t xxmaj mitch xxmaj preston ( de xxmaj niro ) is a detective with the xxup l.a.p.d . , and he 's good at what he does ; but working a case one night , things suddenly go south when another cop")]),t._v(" "),a("td",[t._v("pos")]),t._v(" "),a("td",[t._v("pos")])]),t._v(" "),a("tr",[a("th",[t._v("5")]),t._v(" "),a("td",[t._v("xxbos * xxmaj some spoilers * \\n\\n xxmaj this movie is sometimes subtitled \" life xxmaj everlasting . \" xxmaj that 's often taken as reference to the final scene , but more accurately describes how dead and buried this once - estimable series is after this sloppy and illogical send - off . \\n\\n xxmaj there 's a \" hey kids , let 's put on a show air \" about this telemovie , which can be endearing in spots . xxmaj some fans will feel like insiders as they enjoy picking out all the various cameo appearances . xxmaj co - writer , co - producer xxmaj tom xxmaj fontana and his pals pack the goings - on with friends and favorites from other shows , as well as real xxmaj baltimore personages . \\n\\n xxmaj that 's on top of the returns of virtually all the members")]),t._v(" "),a("td",[t._v("neg")]),t._v(" "),a("td",[t._v("neg")])]),t._v(" "),a("tr",[a("th",[t._v("6")]),t._v(" "),a("td",[t._v("xxbos ( caution : several spoilers ) \\n\\n xxmaj someday , somewhere , there 's going to be a post - apocalyptic movie made that does n't stink . xxmaj unfortunately , xxup the xxup postman is not that movie , though i have to give it credit for trying . \\n\\n xxmaj kevin xxmaj costner plays somebody credited only as \" the xxmaj postman . \" xxmaj he 's not actually a postman , just a wanderer with a mule in the wasteland of a western xxmaj america devastated by some unspecified catastrophe . xxmaj he trades with isolated villages by performing xxmaj shakespeare . xxmaj suddenly a pack of bandits called the xxmaj holnists , the self - declared warlords of the xxmaj west , descend upon a village that xxmaj costner 's visiting , and their evil leader xxmaj gen . xxmaj bethlehem ( will xxmaj patton")]),t._v(" "),a("td",[t._v("neg")]),t._v(" "),a("td",[t._v("neg")])]),t._v(" "),a("tr",[a("th",[t._v("7")]),t._v(" "),a("td",[t._v("xxbos xxmaj in a style reminiscent of the best of xxmaj david xxmaj lean , this romantic love story sweeps across the screen with epic proportions equal to the vast desert regions against which it is set . xxmaj it 's a film which purports that one does not choose love , but rather that it 's love that does the choosing , regardless of who , where or when ; and furthermore , that it 's a matter of the heart often contingent upon prevailing conditions and circumstances . xxmaj and thus is the situation in ` the xxmaj english xxmaj patient , ' directed by xxmaj anthony xxmaj minghella , the story of two people who discover passion and true love in the most inopportune of places and times , proving that when it is predestined , love will find a way . \\n\\n xxmaj it 's xxup")]),t._v(" "),a("td",[t._v("pos")]),t._v(" "),a("td",[t._v("pos")])]),t._v(" "),a("tr",[a("th",[t._v("8")]),t._v(" "),a("td",[t._v("xxbos xxmaj no one is going to mistake xxup the xxup squall for a good movie , but it sure is a memorable one . xxmaj once you 've taken in xxmaj myrna xxmaj loy 's performance as xxmaj nubi the hot - blooded gypsy girl you 're not likely to forget the experience . xxmaj when this film was made the exotically beautiful xxmaj miss xxmaj loy was still being cast as foreign vixens , often xxmaj asian and usually sinister . xxmaj she 's certainly an eyeful here . xxmaj it appears that her skin was darkened and her hair was curled . xxmaj in most scenes she 's barefoot and wearing little more than a skirt and a loose - fitting peasant blouse , while in one scene she wears nothing but a patterned towel . i suppose xxmaj i 'm focusing on xxmaj miss xxmaj loy")]),t._v(" "),a("td",[t._v("neg")]),t._v(" "),a("td",[t._v("neg")])])])]),t._v(" "),a("p",[t._v("And we can predict on new texts quite easily:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("learn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"I really liked that movie!"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[t._v("('pos', tensor(1), tensor([0.0092, 0.9908]))\n")])])]),a("p",[t._v('Here we can see the model has considered the review to be positive. The second part of the result is the index of "pos" in our data vocabulary and the last part is the probabilities attributed to each class (99.1% for "pos" and 0.9% for "neg").')]),t._v(" "),a("p",[t._v("Now it's your turn! Write your own mini movie review, or copy one from the Internet, and we can see what this model thinks about it.")]),t._v(" "),a("h3",{attrs:{id:"using-the-data-block-api"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#using-the-data-block-api"}},[t._v("#")]),t._v(" Using the data block API")]),t._v(" "),a("p",[t._v("We can also use the data block API to get our data in a "),a("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[a("code",[t._v("DataLoaders")])]),t._v(". This is a bit more advanced, so fell free to skip this part if you are not comfortable with learning new APIs just yet.")],1),t._v(" "),a("p",[t._v("A datablock is built by giving the fastai library a bunch of information:")]),t._v(" "),a("ul",[a("li",[t._v("the types used, through an argument called "),a("code",[t._v("blocks")]),t._v(": here we have images and categories, so we pass "),a("RouterLink",{attrs:{to:"/text.data.html#TextBlock"}},[a("code",[t._v("TextBlock")])]),t._v(" and "),a("RouterLink",{attrs:{to:"/data.block.html#CategoryBlock"}},[a("code",[t._v("CategoryBlock")])]),t._v(". To inform the library our texts are files in a folder, we use the "),a("code",[t._v("from_folder")]),t._v(" class method.")],1),t._v(" "),a("li",[t._v("how to get the raw items, here our function "),a("RouterLink",{attrs:{to:"/data.transforms.html#get_text_files"}},[a("code",[t._v("get_text_files")])]),t._v(".")],1),t._v(" "),a("li",[t._v("how to label those items, here with the parent folder.")]),t._v(" "),a("li",[t._v("how to split those items, here with the grandparent folder.")])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("imdb "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataBlock"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blocks"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TextBlock"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_folder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" CategoryBlock"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 get_items"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("get_text_files"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 get_y"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("parent_label"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 splitter"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("GrandparentSplitter"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("valid_name"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'test'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("This only gives a blueprint on how to assemble the data. To actually create it, we need to use the "),a("code",[t._v("dataloaders")]),t._v(" method:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("dls "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" imdb"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloaders"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h2",{attrs:{id:"the-ulmfit-approach"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#the-ulmfit-approach"}},[t._v("#")]),t._v(" The ULMFiT approach")]),t._v(" "),a("p",[t._v("The pretrained model we used in the previous section is called a language model. It was pretrained on Wikipedia on the task of guessing the next word, after reading all the words before. We got great results by directly fine-tuning this language model to a movie review classifier, but with one extra step, we can do even better: the Wikipedia English is slightly different from the IMDb English. So instead of jumping directly to the classifier, we could fine-tune our pretrained language model to the IMDb corpus and "),a("em",[t._v("then")]),t._v(" use that as the base for our classifier.")]),t._v(" "),a("p",[t._v("One reason, of course, is that it is helpful to understand the foundations of the models that you are using. But there is another very practical reason, which is that you get even better results if you fine tune the (sequence-based) language model prior to fine tuning the classification model. For instance, in the IMDb sentiment analysis task, the dataset includes 50,000 additional movie reviews that do not have any positive or negative labels attached in the unsup folder. We can use all of these reviews to fine tune the pretrained language model — this will result in a language model that is particularly good at predicting the next word of a movie review. In contrast, the pretrained model was trained only on Wikipedia articles.")]),t._v(" "),a("p",[t._v("The whole process is summarized by this picture:")]),t._v(" "),a("p",[a("img",{attrs:{src:"/images/ulmfit.png",alt:"ULMFit process"}})]),t._v(" "),a("h3",{attrs:{id:"fine-tuning-a-language-model-on-imdb"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fine-tuning-a-language-model-on-imdb"}},[t._v("#")]),t._v(" Fine-tuning a language model on IMDb")]),t._v(" "),a("p",[t._v("We can get our texts in a "),a("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[a("code",[t._v("DataLoaders")])]),t._v(" suitable for language modeling very easily:")],1),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("dls_lm "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TextDataLoaders"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_folder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_lm"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valid_pct"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("We need to pass something for "),a("code",[t._v("valid_pct")]),t._v(" otherwise this method will try to split the data by using the grandparent folder names. By passing "),a("code",[t._v("valid_pct=0.1")]),t._v(", we tell it to get a random 10% of those reviews for the validation set.")]),t._v(" "),a("p",[t._v("We can have a look at our data using "),a("code",[t._v("show_batch")]),t._v(". Here the task is to guess the next word, so we can see the targets have all shifted one word to the right.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("dls_lm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("max_n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("table",{staticClass:"dataframe",attrs:{border:"1"}},[a("thead",[a("tr",{staticStyle:{"text-align":"right"}},[a("th"),t._v(" "),a("th",[t._v("text")]),t._v(" "),a("th",[t._v("text_")])])]),t._v(" "),a("tbody",[a("tr",[a("th",[t._v("0")]),t._v(" "),a("td",[t._v('xxbos xxmaj about thirty minutes into the film , i thought this was one of the weakest " xxunk ever because it had the usual beginning ( a murder happening , then xxmaj columbo coming , inspecting everything and interrogating the main suspect ) squared ! xxmaj it was boring because i thought i knew everything already . \\n\\n xxmaj but then there was a surprising twist that turned this episode into')]),t._v(" "),a("td",[t._v('xxmaj about thirty minutes into the film , i thought this was one of the weakest " xxunk ever because it had the usual beginning ( a murder happening , then xxmaj columbo coming , inspecting everything and interrogating the main suspect ) squared ! xxmaj it was boring because i thought i knew everything already . \\n\\n xxmaj but then there was a surprising twist that turned this episode into a')])]),t._v(" "),a("tr",[a("th",[t._v("1")]),t._v(" "),a("td",[t._v("yeon . xxmaj these two girls were magical on the screen . i will certainly be looking into their other films . xxmaj xxunk xxmaj jeong - ah is xxunk cheerful and hauntingly evil as the stepmother . xxmaj finally , xxmaj xxunk - su xxmaj kim gives an excellent performance as the weary , broken father . \\n\\n i truly love this film . xxmaj if you have yet to see")]),t._v(" "),a("td",[t._v(". xxmaj these two girls were magical on the screen . i will certainly be looking into their other films . xxmaj xxunk xxmaj jeong - ah is xxunk cheerful and hauntingly evil as the stepmother . xxmaj finally , xxmaj xxunk - su xxmaj kim gives an excellent performance as the weary , broken father . \\n\\n i truly love this film . xxmaj if you have yet to see '")])]),t._v(" "),a("tr",[a("th",[t._v("2")]),t._v(" "),a("td",[t._v("tends to be tedious whenever there are n't any hideous monsters on display . xxmaj luckily the gutsy killings and eerie set designs ( by no less than xxmaj bill xxmaj paxton ! ) compensate for a lot ! a nine - headed expedition is send ( at hyper speed ) to the unexplored regions of space to find out what happened to a previously vanished spaceship and its crew . xxmaj")]),t._v(" "),a("td",[t._v("to be tedious whenever there are n't any hideous monsters on display . xxmaj luckily the gutsy killings and eerie set designs ( by no less than xxmaj bill xxmaj paxton ! ) compensate for a lot ! a nine - headed expedition is send ( at hyper speed ) to the unexplored regions of space to find out what happened to a previously vanished spaceship and its crew . xxmaj bad")])]),t._v(" "),a("tr",[a("th",[t._v("3")]),t._v(" "),a("td",[t._v("movie just sort of meanders around and nothing happens ( i do n't mean in terms of plot - no plot is fine , but no action ? xxmaj come on . ) xxmaj in hindsight , i should have expected this - after all , how much can really happen between 4 teens and a bear ? xxmaj so although special effects , acting , etc are more or less on")]),t._v(" "),a("td",[t._v("just sort of meanders around and nothing happens ( i do n't mean in terms of plot - no plot is fine , but no action ? xxmaj come on . ) xxmaj in hindsight , i should have expected this - after all , how much can really happen between 4 teens and a bear ? xxmaj so although special effects , acting , etc are more or less on par")])]),t._v(" "),a("tr",[a("th",[t._v("4")]),t._v(" "),a("td",[t._v("greetings again from the darkness . xxmaj writer / xxmaj director ( and xxmaj wes xxmaj anderson collaborator ) xxmaj noah xxmaj baumbach presents a semi - autobiographical therapy session where he unleashes the anguish and turmoil that has carried over from his childhood . xxmaj the result is an amazing insight into what many people go through in a desperate attempt to try and make their family work . \\n\\n xxmaj")]),t._v(" "),a("td",[t._v("again from the darkness . xxmaj writer / xxmaj director ( and xxmaj wes xxmaj anderson collaborator ) xxmaj noah xxmaj baumbach presents a semi - autobiographical therapy session where he unleashes the anguish and turmoil that has carried over from his childhood . xxmaj the result is an amazing insight into what many people go through in a desperate attempt to try and make their family work . \\n\\n xxmaj the")])])])]),t._v(" "),a("p",[t._v("Then we have a convenience method to directly grab a "),a("RouterLink",{attrs:{to:"/learner.html#Learner"}},[a("code",[t._v("Learner")])]),t._v(" from it, using the "),a("RouterLink",{attrs:{to:"/text.models.awdlstm.html#AWD_LSTM"}},[a("code",[t._v("AWD_LSTM")])]),t._v(" architecture like before. We use accuracy and perplexity as metrics (the later is the exponential of the loss) and we set a default weight decay of 0.1. "),a("code",[t._v("to_fp16")]),t._v(" puts the "),a("RouterLink",{attrs:{to:"/learner.html#Learner"}},[a("code",[t._v("Learner")])]),t._v(" in mixed precision, which is going to help speed up training on GPUs that have Tensor Cores.")],1),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("learn "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" language_model_learner"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls_lm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AWD_LSTM"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("accuracy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Perplexity"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" path"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wd"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_fp16"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("By default, a pretrained "),a("RouterLink",{attrs:{to:"/learner.html#Learner"}},[a("code",[t._v("Learner")])]),t._v(" is in a frozen state, meaning that only the head of the model will train while the body stays frozen. We show you what is behind the fine_tune method here and use a fit_one_cycle method to fit the model:")],1),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("learn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_one_cycle"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("table",{staticClass:"dataframe",attrs:{border:"1"}},[a("thead",[a("tr",{staticStyle:{"text-align":"left"}},[a("th",[t._v("epoch")]),t._v(" "),a("th",[t._v("train_loss")]),t._v(" "),a("th",[t._v("valid_loss")]),t._v(" "),a("th",[t._v("accuracy")]),t._v(" "),a("th",[t._v("perplexity")]),t._v(" "),a("th",[t._v("time")])])]),t._v(" "),a("tbody",[a("tr",[a("td",[t._v("0")]),t._v(" "),a("td",[t._v("4.120048")]),t._v(" "),a("td",[t._v("3.912788")]),t._v(" "),a("td",[t._v("0.299565")]),t._v(" "),a("td",[t._v("50.038246")]),t._v(" "),a("td",[t._v("11:39")])])])]),t._v(" "),a("p",[t._v("This model takes a while to train, so it's a good opportunity to talk about saving intermediary results.")]),t._v(" "),a("p",[t._v("You can easily save the state of your model like so:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("learn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'1epoch'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("It will create a file in "),a("code",[t._v("learn.path/models/")]),t._v(' named "1epoch.pth". If you want to load your model on another machine after creating your '),a("RouterLink",{attrs:{to:"/learner.html#Learner"}},[a("code",[t._v("Learner")])]),t._v(" the same way, or resume training later, you can load the content of this file with:")],1),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("learn "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" learn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'1epoch'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("We can them fine-tune the model after unfreezing:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("learn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unfreeze"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_one_cycle"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("table",{staticClass:"dataframe",attrs:{border:"1"}},[a("thead",[a("tr",{staticStyle:{"text-align":"left"}},[a("th",[t._v("epoch")]),t._v(" "),a("th",[t._v("train_loss")]),t._v(" "),a("th",[t._v("valid_loss")]),t._v(" "),a("th",[t._v("accuracy")]),t._v(" "),a("th",[t._v("perplexity")]),t._v(" "),a("th",[t._v("time")])])]),t._v(" "),a("tbody",[a("tr",[a("td",[t._v("0")]),t._v(" "),a("td",[t._v("3.893486")]),t._v(" "),a("td",[t._v("3.772820")]),t._v(" "),a("td",[t._v("0.317104")]),t._v(" "),a("td",[t._v("43.502548")]),t._v(" "),a("td",[t._v("12:37")])]),t._v(" "),a("tr",[a("td",[t._v("1")]),t._v(" "),a("td",[t._v("3.820479")]),t._v(" "),a("td",[t._v("3.717197")]),t._v(" "),a("td",[t._v("0.323790")]),t._v(" "),a("td",[t._v("41.148880")]),t._v(" "),a("td",[t._v("12:30")])]),t._v(" "),a("tr",[a("td",[t._v("2")]),t._v(" "),a("td",[t._v("3.735622")]),t._v(" "),a("td",[t._v("3.659760")]),t._v(" "),a("td",[t._v("0.330321")]),t._v(" "),a("td",[t._v("38.851997")]),t._v(" "),a("td",[t._v("12:09")])]),t._v(" "),a("tr",[a("td",[t._v("3")]),t._v(" "),a("td",[t._v("3.677086")]),t._v(" "),a("td",[t._v("3.624794")]),t._v(" "),a("td",[t._v("0.333960")]),t._v(" "),a("td",[t._v("37.516987")]),t._v(" "),a("td",[t._v("12:12")])]),t._v(" "),a("tr",[a("td",[t._v("4")]),t._v(" "),a("td",[t._v("3.636646")]),t._v(" "),a("td",[t._v("3.601300")]),t._v(" "),a("td",[t._v("0.337017")]),t._v(" "),a("td",[t._v("36.645859")]),t._v(" "),a("td",[t._v("12:05")])]),t._v(" "),a("tr",[a("td",[t._v("5")]),t._v(" "),a("td",[t._v("3.553636")]),t._v(" "),a("td",[t._v("3.584241")]),t._v(" "),a("td",[t._v("0.339355")]),t._v(" "),a("td",[t._v("36.026001")]),t._v(" "),a("td",[t._v("12:04")])]),t._v(" "),a("tr",[a("td",[t._v("6")]),t._v(" "),a("td",[t._v("3.507634")]),t._v(" "),a("td",[t._v("3.571892")]),t._v(" "),a("td",[t._v("0.341353")]),t._v(" "),a("td",[t._v("35.583862")]),t._v(" "),a("td",[t._v("12:08")])]),t._v(" "),a("tr",[a("td",[t._v("7")]),t._v(" "),a("td",[t._v("3.444101")]),t._v(" "),a("td",[t._v("3.565988")]),t._v(" "),a("td",[t._v("0.342194")]),t._v(" "),a("td",[t._v("35.374371")]),t._v(" "),a("td",[t._v("12:08")])]),t._v(" "),a("tr",[a("td",[t._v("8")]),t._v(" "),a("td",[t._v("3.398597")]),t._v(" "),a("td",[t._v("3.566283")]),t._v(" "),a("td",[t._v("0.342647")]),t._v(" "),a("td",[t._v("35.384815")]),t._v(" "),a("td",[t._v("12:11")])]),t._v(" "),a("tr",[a("td",[t._v("9")]),t._v(" "),a("td",[t._v("3.375563")]),t._v(" "),a("td",[t._v("3.568166")]),t._v(" "),a("td",[t._v("0.342528")]),t._v(" "),a("td",[t._v("35.451500")]),t._v(" "),a("td",[t._v("12:05")])])])]),t._v(" "),a("p",[t._v("Once this is done, we save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the "),a("em",[t._v("encoder")]),t._v(". We can save it with "),a("code",[t._v("save_encoder")]),t._v(":")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("learn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save_encoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'finetuned'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("blockquote",[a("p",[t._v("Jargon:Encoder: The model not including the task-specific final layer(s). It means much the same thing as "),a("em",[t._v("body")]),t._v(" when applied to vision CNNs, but tends to be more used for NLP and generative models.")])]),t._v(" "),a("p",[t._v("Before using this to fine-tune a classifier on the reviews, we can use our model to generate random reviews: since it's trained to guess what the next word of the sentence is, we can use it to write new reviews:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("TEXT "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"I liked this movie because"')]),t._v("\nN_WORDS "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("40")]),t._v("\nN_SENTENCES "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\npreds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("learn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TEXT"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" N_WORDS"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" temperature"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.75")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n         "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" _ "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("N_SENTENCES"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"\\n"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[t._v('i liked this movie because of its story and characters . The story line was very strong , very good for a sci - fi film . The main character , Alucard , was very well developed and brought the whole story\ni liked this movie because i like the idea of the premise of the movie , the ( very ) convenient virus ( which , when you have to kill a few people , the " evil " machine has to be used to protect\n')])])]),a("h3",{attrs:{id:"training-a-text-classifier"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#training-a-text-classifier"}},[t._v("#")]),t._v(" Training a text classifier")]),t._v(" "),a("p",[t._v("We can gather our data for text classification almost exactly like before:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("dls_clas "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TextDataLoaders"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_folder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("untar_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("URLs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("IMDB"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valid"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'test'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text_vocab"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("dls_lm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vocab"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("The main difference is that we have to use the exact same vocabulary as when we were fine-tuning our language model, or the weights learned won't make any sense. We pass that vocabulary with "),a("code",[t._v("text_vocab")]),t._v(".")]),t._v(" "),a("p",[t._v("Then we can define our text classifier like before:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("learn "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" text_classifier_learner"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AWD_LSTM"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" drop_mult"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("accuracy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("The difference is that before training it, we load the previous encoder:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("learn "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" learn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load_encoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'finetuned'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("The last step is to train with discriminative learning rates and "),a("em",[t._v("gradual unfreezing")]),t._v(". In computer vision, we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("learn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_one_cycle"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2e")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("table",{staticClass:"dataframe",attrs:{border:"1"}},[a("thead",[a("tr",{staticStyle:{"text-align":"left"}},[a("th",[t._v("epoch")]),t._v(" "),a("th",[t._v("train_loss")]),t._v(" "),a("th",[t._v("valid_loss")]),t._v(" "),a("th",[t._v("accuracy")]),t._v(" "),a("th",[t._v("time")])])]),t._v(" "),a("tbody",[a("tr",[a("td",[t._v("0")]),t._v(" "),a("td",[t._v("0.347427")]),t._v(" "),a("td",[t._v("0.184480")]),t._v(" "),a("td",[t._v("0.929320")]),t._v(" "),a("td",[t._v("00:33")])])])]),t._v(" "),a("p",[t._v("In just one epoch we get the same result as our training in the first section, not too bad! We can pass "),a("code",[t._v("-2")]),t._v(" to "),a("code",[t._v("freeze_to")]),t._v(" to freeze all except the last two parameter groups:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("learn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("freeze_to"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_one_cycle"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("slice")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.6")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("table",{staticClass:"dataframe",attrs:{border:"1"}},[a("thead",[a("tr",{staticStyle:{"text-align":"left"}},[a("th",[t._v("epoch")]),t._v(" "),a("th",[t._v("train_loss")]),t._v(" "),a("th",[t._v("valid_loss")]),t._v(" "),a("th",[t._v("accuracy")]),t._v(" "),a("th",[t._v("time")])])]),t._v(" "),a("tbody",[a("tr",[a("td",[t._v("0")]),t._v(" "),a("td",[t._v("0.247763")]),t._v(" "),a("td",[t._v("0.171683")]),t._v(" "),a("td",[t._v("0.934640")]),t._v(" "),a("td",[t._v("00:37")])])])]),t._v(" "),a("p",[t._v("Then we can unfreeze a bit more, and continue training:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("learn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("freeze_to"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_one_cycle"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("slice")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5e")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.6")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5e")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("table",{staticClass:"dataframe",attrs:{border:"1"}},[a("thead",[a("tr",{staticStyle:{"text-align":"left"}},[a("th",[t._v("epoch")]),t._v(" "),a("th",[t._v("train_loss")]),t._v(" "),a("th",[t._v("valid_loss")]),t._v(" "),a("th",[t._v("accuracy")]),t._v(" "),a("th",[t._v("time")])])]),t._v(" "),a("tbody",[a("tr",[a("td",[t._v("0")]),t._v(" "),a("td",[t._v("0.193377")]),t._v(" "),a("td",[t._v("0.156696")]),t._v(" "),a("td",[t._v("0.941200")]),t._v(" "),a("td",[t._v("00:45")])])])]),t._v(" "),a("p",[t._v("And finally, the whole model!")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("learn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unfreeze"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_one_cycle"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("slice")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.6")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("table",{staticClass:"dataframe",attrs:{border:"1"}},[a("thead",[a("tr",{staticStyle:{"text-align":"left"}},[a("th",[t._v("epoch")]),t._v(" "),a("th",[t._v("train_loss")]),t._v(" "),a("th",[t._v("valid_loss")]),t._v(" "),a("th",[t._v("accuracy")]),t._v(" "),a("th",[t._v("time")])])]),t._v(" "),a("tbody",[a("tr",[a("td",[t._v("0")]),t._v(" "),a("td",[t._v("0.172888")]),t._v(" "),a("td",[t._v("0.153770")]),t._v(" "),a("td",[t._v("0.943120")]),t._v(" "),a("td",[t._v("01:01")])]),t._v(" "),a("tr",[a("td",[t._v("1")]),t._v(" "),a("td",[t._v("0.161492")]),t._v(" "),a("td",[t._v("0.155567")]),t._v(" "),a("td",[t._v("0.942640")]),t._v(" "),a("td",[t._v("00:57")])])])])])}),[],!1,null,null,null);e.default=n.exports}}]);