(window.webpackJsonp=window.webpackJsonp||[]).push([[43],{372:function(t,s,a){"use strict";a.r(s);var n=a(42),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"text-core"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#text-core"}},[t._v("#")]),t._v(" Text core")]),t._v(" "),a("blockquote",[a("p",[t._v("Basic function to preprocess text before assembling it in a "),a("code",[t._v("DataLoaders")]),t._v(".")])]),t._v(" "),a("h2",{attrs:{id:"preprocessing-rules"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#preprocessing-rules"}},[t._v("#")]),t._v(" Preprocessing rules")]),t._v(" "),a("p",[t._v("The following are rules applied to texts before or after it's tokenized.")]),t._v(" "),a("h4",{staticClass:"doc_header",attrs:{id:"spec_add_spaces"}},[a("code",[t._v("spec_add_spaces")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L28"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("spec_add_spaces")]),t._v("("),a("strong",[a("code",[t._v("t")])]),t._v(")")])]),t._v(" "),a("p",[t._v("Add spaces around / and #")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("test_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("spec_add_spaces"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'#fastai'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("' # fastai'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("spec_add_spaces"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/fastai'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("' / fastai'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("spec_add_spaces"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\\\fastai'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("' \\\\ fastai'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h4",{staticClass:"doc_header",attrs:{id:"rm_useless_spaces"}},[a("code",[t._v("rm_useless_spaces")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L35"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("rm_useless_spaces")]),t._v("("),a("strong",[a("code",[t._v("t")])]),t._v(")")])]),t._v(" "),a("p",[t._v("Remove multiple spaces")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("test_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rm_useless_spaces"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a  b   c'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a b c'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h4",{staticClass:"doc_header",attrs:{id:"replace_rep"}},[a("code",[t._v("replace_rep")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L42"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("replace_rep")]),t._v("("),a("strong",[a("code",[t._v("t")])]),t._v(")")])]),t._v(" "),a("p",[t._v("Replace repetitions at the character level: cccc -- TK_REP 4 c")]),t._v(" "),a("p",[t._v("It starts replacing at 3 repetitions of the same character or more.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("test_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("replace_rep"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'aa'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'aa'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("replace_rep"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'aaaa'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string-interpolation"}},[a("span",{pre:!0,attrs:{class:"token string"}},[t._v("f' ")]),a("span",{pre:!0,attrs:{class:"token interpolation"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("TK_REP"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v(" 4 a '")])]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h4",{staticClass:"doc_header",attrs:{id:"replace_wrep"}},[a("code",[t._v("replace_wrep")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L53"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("replace_wrep")]),t._v("("),a("strong",[a("code",[t._v("t")])]),t._v(")")])]),t._v(" "),a("p",[t._v("Replace word repetitions: word word word word -- TK_WREP 4 word")]),t._v(" "),a("p",[t._v("It starts replacing at 3 repetitions of the same word or more.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("test_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("replace_wrep"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ah ah'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ah ah'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("replace_wrep"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ah ah ah'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string-interpolation"}},[a("span",{pre:!0,attrs:{class:"token string"}},[t._v("f' ")]),a("span",{pre:!0,attrs:{class:"token interpolation"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("TK_WREP"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v(" 3 ah '")])]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("replace_wrep"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ah ah   ah  ah'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string-interpolation"}},[a("span",{pre:!0,attrs:{class:"token string"}},[t._v("f' ")]),a("span",{pre:!0,attrs:{class:"token interpolation"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("TK_WREP"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v(" 4 ah '")])]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("replace_wrep"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ah ah ah ah '")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string-interpolation"}},[a("span",{pre:!0,attrs:{class:"token string"}},[t._v("f' ")]),a("span",{pre:!0,attrs:{class:"token interpolation"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("TK_WREP"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v(" 4 ah  '")])]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("replace_wrep"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ah ah ah ah.'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string-interpolation"}},[a("span",{pre:!0,attrs:{class:"token string"}},[t._v("f' ")]),a("span",{pre:!0,attrs:{class:"token interpolation"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("TK_WREP"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v(" 4 ah .'")])]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("replace_wrep"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ah ah ahi'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string-interpolation"}},[a("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'ah ah ahi'")])]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h4",{staticClass:"doc_header",attrs:{id:"fix_html"}},[a("code",[t._v("fix_html")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L61"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("fix_html")]),t._v("("),a("strong",[a("code",[t._v("x")])]),t._v(")")])]),t._v(" "),a("p",[t._v("Various messy things we've seen in documents")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("test_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fix_html"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'#39;bli#146;'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("\"'bli'\"")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fix_html"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Sarah amp; Duck...'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Sarah & Duck …'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fix_html"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a nbsp; #36;'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a   $'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fix_html"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\\\\" <unk>'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string-interpolation"}},[a("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'\" ")]),a("span",{pre:!0,attrs:{class:"token interpolation"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("UNK"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fix_html"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'quot;  @.@  @-@ '")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"\' .-"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fix_html"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'<br />text\\\\n'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\ntext\\n'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h4",{staticClass:"doc_header",attrs:{id:"replace_all_caps"}},[a("code",[t._v("replace_all_caps")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L72"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("replace_all_caps")]),t._v("("),a("strong",[a("code",[t._v("t")])]),t._v(")")])]),t._v(" "),a("p",[t._v("Replace tokens in ALL CAPS by their lower version and add "),a("code",[t._v("TK_UP")]),t._v(" before.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("test_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("replace_all_caps"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"I\'M SHOUTING"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string-interpolation"}},[a("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"')]),a("span",{pre:!0,attrs:{class:"token interpolation"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("TK_UP"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v(" i'm ")]),a("span",{pre:!0,attrs:{class:"token interpolation"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("TK_UP"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v(' shouting"')])]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("replace_all_caps"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"I\'m speaking normally"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"I\'m speaking normally"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("replace_all_caps"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"I am speaking normally"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"i am speaking normally"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h4",{staticClass:"doc_header",attrs:{id:"replace_maj"}},[a("code",[t._v("replace_maj")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L83"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("replace_maj")]),t._v("("),a("strong",[a("code",[t._v("t")])]),t._v(")")])]),t._v(" "),a("p",[t._v("Replace tokens in ALL CAPS by their lower version and add "),a("code",[t._v("TK_UP")]),t._v(" before.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("test_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("replace_maj"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Jeremy Howard"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string-interpolation"}},[a("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'")]),a("span",{pre:!0,attrs:{class:"token interpolation"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("TK_MAJ"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v(" jeremy ")]),a("span",{pre:!0,attrs:{class:"token interpolation"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("TK_MAJ"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v(" howard'")])]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("replace_maj"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"I don\'t think there is any maj here"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"i don\'t think there is any maj here"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h4",{staticClass:"doc_header",attrs:{id:"lowercase"}},[a("code",[t._v("lowercase")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L91"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("lowercase")]),t._v("("),a("strong",[a("code",[t._v("t")])]),t._v(", "),a("strong",[a("code",[t._v("add_bos")])]),t._v("="),a("em",[a("code",[t._v("True")])]),t._v(", "),a("strong",[a("code",[t._v("add_eos")])]),t._v("="),a("em",[a("code",[t._v("False")])]),t._v(")")])]),t._v(" "),a("p",[t._v("Converts "),a("code",[t._v("t")]),t._v(" to lowercase")]),t._v(" "),a("h4",{staticClass:"doc_header",attrs:{id:"replace_space"}},[a("code",[t._v("replace_space")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L96"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("replace_space")]),t._v("("),a("strong",[a("code",[t._v("t")])]),t._v(")")])]),t._v(" "),a("p",[t._v("Replace embedded spaces in a token with unicode line char to allow for split/join")]),t._v(" "),a("h2",{attrs:{id:"tokenizing"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tokenizing"}},[t._v("#")]),t._v(" Tokenizing")]),t._v(" "),a("p",[t._v("A tokenizer is a class that must implement "),a("code",[t._v("__call__")]),t._v(". This method receives a iterator of texts and must return a generator with their tokenized versions. Here is the most basic example:")]),t._v(" "),a("h3",{staticClass:"doc_header",attrs:{id:"BaseTokenizer"}},[a("code",[t._v("class")]),t._v(" "),a("code",[t._v("BaseTokenizer")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L107"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("BaseTokenizer")]),t._v("("),a("strong",[a("code",[t._v("split_char")])]),t._v("="),a("em",[a("code",[t._v("' '")])]),t._v(", "),a("strong",[t._v("**"),a("code",[t._v("kwargs")])]),t._v(")")])]),t._v(" "),a("p",[t._v("Basic tokenizer that just splits on spaces")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("tok "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" BaseTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tok"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This is a text"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"is"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"a"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntok "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" BaseTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tok"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This is a text"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This is a te"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"t"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h3",{staticClass:"doc_header",attrs:{id:"SpacyTokenizer"}},[a("code",[t._v("class")]),t._v(" "),a("code",[t._v("SpacyTokenizer")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L113"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("SpacyTokenizer")]),t._v("("),a("strong",[a("code",[t._v("lang")])]),t._v("="),a("em",[a("code",[t._v("'en'")])]),t._v(", "),a("strong",[a("code",[t._v("special_toks")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("buf_sz")])]),t._v("="),a("em",[a("code",[t._v("5000")])]),t._v(")")])]),t._v(" "),a("p",[t._v("Spacy tokenizer for "),a("code",[t._v("lang")])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("tok "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SpacyTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ninp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("exp "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This isn\'t the easiest text."')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"is"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"n\'t"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"the"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"easiest"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"."')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("L"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tok"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("inp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("inp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("exp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("exp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h3",{staticClass:"doc_header",attrs:{id:"TokenizeWithRules"}},[a("code",[t._v("class")]),t._v(" "),a("code",[t._v("TokenizeWithRules")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L128"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("TokenizeWithRules")]),t._v("("),a("strong",[a("code",[t._v("tok")])]),t._v(", "),a("strong",[a("code",[t._v("rules")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("post_rules")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(")")])]),t._v(" "),a("p",[t._v("A wrapper around "),a("code",[t._v("tok")]),t._v(" which applies "),a("code",[t._v("rules")]),t._v(", then tokenizes, then applies "),a("code",[t._v("post_rules")])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("f "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TokenizeWithRules"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("BaseTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("rules"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("replace_all_caps"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("f"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"THIS isn\'t a problem"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("TK_UP"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'this'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"isn\'t"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'problem'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TokenizeWithRules"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("SpacyTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("f"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This isn\'t a problem"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("BOS"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" TK_MAJ"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'this'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'is'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"n\'t"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'problem'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TokenizeWithRules"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("BaseTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("split_char"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"\'"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" rules"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("f"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This isn\'t a problem"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'This▁isn'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'t▁a▁problem'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("The main function that will be called during one of the processes handling tokenization. It will iterate through the "),a("code",[t._v("batch")]),t._v(" of texts, apply them "),a("code",[t._v("rules")]),t._v(" and tokenize them.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("texts "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"this is a text"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"this is another text"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ntok "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TokenizeWithRules"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("BaseTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" texts"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__getitem__"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tok"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'this'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'is'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'text'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'this'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'is'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'another'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'text'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h4",{staticClass:"doc_header",attrs:{id:"tokenize1"}},[a("code",[t._v("tokenize1")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L139"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("tokenize1")]),t._v("("),a("strong",[a("code",[t._v("text")])]),t._v(", "),a("strong",[a("code",[t._v("tok")])]),t._v(", "),a("strong",[a("code",[t._v("rules")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("post_rules")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(")")])]),t._v(" "),a("p",[t._v("Call "),a("RouterLink",{attrs:{to:"/text.core.html#TokenizeWithRules"}},[a("code",[t._v("TokenizeWithRules")])]),t._v(" with a single text")],1),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("test_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tokenize1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This isn\'t a problem"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" SpacyTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("BOS"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" TK_MAJ"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'this'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'is'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"n\'t"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'problem'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tokenize1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This isn\'t a problem"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tok"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("BaseTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" rules"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'This'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"isn\'t"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'problem'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h4",{staticClass:"doc_header",attrs:{id:"parallel_tokenize"}},[a("code",[t._v("parallel_tokenize")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L145"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("parallel_tokenize")]),t._v("("),a("strong",[a("code",[t._v("items")])]),t._v(", "),a("strong",[a("code",[t._v("tok")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("rules")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("n_workers")])]),t._v("="),a("em",[a("code",[t._v("4")])]),t._v(", "),a("strong",[t._v("**"),a("code",[t._v("kwargs")])]),t._v(")")])]),t._v(" "),a("p",[t._v("Calls optional "),a("code",[t._v("setup")]),t._v(" on "),a("code",[t._v("tok")]),t._v(" before launching "),a("RouterLink",{attrs:{to:"/text.core.html#TokenizeWithRules"}},[a("code",[t._v("TokenizeWithRules")])]),t._v(" using `parallel_gen")],1),t._v(" "),a("p",[t._v("Note that since this uses "),a("a",{attrs:{href:"https://fastcore.fast.ai/utils#parallel_gen",target:"_blank",rel:"noopener noreferrer"}},[a("code",[t._v("parallel_gen")]),a("OutboundLink")],1),t._v(" behind the scenes, the generator returned contains tuples of indices and results. There is no guarantee that the results are returned in order, so you should sort by the first item of the tuples (the indices) if you need them ordered.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("res  "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" parallel_tokenize"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'0 1'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'1 2'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" rules"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_workers"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nidxs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("toks "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("zip")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("L"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("res"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sorted")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("itemgetter"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("toks"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'0'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'1'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'1'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'2'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h3",{attrs:{id:"tokenize-texts-in-files"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tokenize-texts-in-files"}},[t._v("#")]),t._v(" Tokenize texts in files")]),t._v(" "),a("p",[t._v("Preprocessing function for texts in filenames. Tokenized texts will be saved in a similar fashion in a directory suffixed with "),a("code",[t._v("_tok")]),t._v(" in the parent folder of "),a("code",[t._v("path")]),t._v(" (override with "),a("code",[t._v("output_dir")]),t._v("). This directory is the return value.")]),t._v(" "),a("h4",{staticClass:"doc_header",attrs:{id:"tokenize_folder"}},[a("code",[t._v("tokenize_folder")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L178"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("tokenize_folder")]),t._v("("),a("strong",[a("code",[t._v("path")])]),t._v(", "),a("strong",[a("code",[t._v("extensions")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("folders")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("output_dir")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("skip_if_exists")])]),t._v("="),a("em",[a("code",[t._v("True")])]),t._v(", "),a("strong",[a("code",[t._v("output_names")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("n_workers")])]),t._v("="),a("em",[a("code",[t._v("4")])]),t._v(", "),a("strong",[a("code",[t._v("rules")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("tok")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("encoding")])]),t._v("="),a("em",[a("code",[t._v("'utf8'")])]),t._v(")")])]),t._v(" "),a("p",[t._v("Tokenize text files in "),a("code",[t._v("path")]),t._v(" in parallel using "),a("code",[t._v("n_workers")])]),t._v(" "),a("p",[t._v("The result will be in "),a("code",[t._v("output_dir")]),t._v(" (defaults to a folder in the same parent directory as "),a("code",[t._v("path")]),t._v(", with "),a("code",[t._v("_tok")]),t._v(" added to "),a("code",[t._v("path.name")]),t._v(") with the same structure as in "),a("code",[t._v("path")]),t._v(". Tokenized texts for a given file will be in the file having the same name in "),a("code",[t._v("output_dir")]),t._v(". Additionally, a file with a .len suffix contains the number of tokens and the count of all words is stored in "),a("code",[t._v("output_dir/counter.pkl")]),t._v(".")]),t._v(" "),a("p",[a("code",[t._v("extensions")]),t._v(" will default to "),a("code",[t._v("['.txt']")]),t._v(" and all text files in "),a("code",[t._v("path")]),t._v(" are treated unless you specify a list of folders in "),a("code",[t._v("include")]),t._v(". "),a("code",[t._v("rules")]),t._v(" (that defaults to "),a("RouterLink",{attrs:{to:"/text.core.html#defaults.text_proc_rules"}},[a("code",[t._v("defaults.text_proc_rules")])]),t._v(") are applied to each text before going in the tokenizer.")],1),t._v(" "),a("h4",{staticClass:"doc_header",attrs:{id:"tokenize_files"}},[a("code",[t._v("tokenize_files")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L187"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("tokenize_files")]),t._v("("),a("strong",[a("code",[t._v("files")])]),t._v(", "),a("strong",[a("code",[t._v("path")])]),t._v(", "),a("strong",[a("code",[t._v("output_dir")])]),t._v(", "),a("strong",[a("code",[t._v("output_names")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("n_workers")])]),t._v("="),a("em",[a("code",[t._v("4")])]),t._v(", "),a("strong",[a("code",[t._v("rules")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("tok")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("encoding")])]),t._v("="),a("em",[a("code",[t._v("'utf8'")])]),t._v(", "),a("strong",[a("code",[t._v("skip_if_exists")])]),t._v("="),a("em",[a("code",[t._v("False")])]),t._v(")")])]),t._v(" "),a("p",[t._v("Tokenize text "),a("code",[t._v("files")]),t._v(" in parallel using "),a("code",[t._v("n_workers")])]),t._v(" "),a("h3",{attrs:{id:"tokenize-texts-in-a-dataframe"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tokenize-texts-in-a-dataframe"}},[t._v("#")]),t._v(" Tokenize texts in a dataframe")]),t._v(" "),a("h4",{staticClass:"doc_header",attrs:{id:"tokenize_texts"}},[a("code",[t._v("tokenize_texts")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L203"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("tokenize_texts")]),t._v("("),a("strong",[a("code",[t._v("texts")])]),t._v(", "),a("strong",[a("code",[t._v("n_workers")])]),t._v("="),a("em",[a("code",[t._v("4")])]),t._v(", "),a("strong",[a("code",[t._v("rules")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("tok")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(")")])]),t._v(" "),a("p",[t._v("Tokenize "),a("code",[t._v("texts")]),t._v(" in parallel using "),a("code",[t._v("n_workers")])]),t._v(" "),a("h4",{staticClass:"doc_header",attrs:{id:"tokenize_df"}},[a("code",[t._v("tokenize_df")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L211"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("tokenize_df")]),t._v("("),a("strong",[a("code",[t._v("df")])]),t._v(", "),a("strong",[a("code",[t._v("text_cols")])]),t._v(", "),a("strong",[a("code",[t._v("n_workers")])]),t._v("="),a("em",[a("code",[t._v("4")])]),t._v(", "),a("strong",[a("code",[t._v("rules")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("mark_fields")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("tok")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("res_col_name")])]),t._v("="),a("em",[a("code",[t._v("'text'")])]),t._v(")")])]),t._v(" "),a("p",[t._v("Tokenize texts in "),a("code",[t._v("df[text_cols]")]),t._v(" in parallel using "),a("code",[t._v("n_workers")])]),t._v(" "),a("p",[t._v("This function returns a new dataframe with the same non-text columns, a column named text that contains the tokenized texts and a column named text_lengths that contains their respective length. It also returns a counter of all seen words to quickly build a vocabulary afterward.")]),t._v(" "),a("p",[a("code",[t._v("rules")]),t._v(" (that defaults to "),a("RouterLink",{attrs:{to:"/text.core.html#defaults.text_proc_rules"}},[a("code",[t._v("defaults.text_proc_rules")])]),t._v(") are applied to each text before going in the tokenizer. If "),a("code",[t._v("mark_fields")]),t._v(" isn't specified, it defaults to "),a("code",[t._v("False")]),t._v(" when there is a single text column, "),a("code",[t._v("True")]),t._v(" when there are several. In that case, the texts in each of those columns are joined with "),a("code",[t._v("FLD")]),t._v(" markers followed by the number of the field.")],1),t._v(" "),a("h4",{staticClass:"doc_header",attrs:{id:"tokenize_csv"}},[a("code",[t._v("tokenize_csv")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L229"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("tokenize_csv")]),t._v("("),a("strong",[a("code",[t._v("fname")])]),t._v(", "),a("strong",[a("code",[t._v("text_cols")])]),t._v(", "),a("strong",[a("code",[t._v("outname")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("n_workers")])]),t._v("="),a("em",[a("code",[t._v("4")])]),t._v(", "),a("strong",[a("code",[t._v("rules")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("mark_fields")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("tok")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("header")])]),t._v("="),a("em",[a("code",[t._v("'infer'")])]),t._v(", "),a("strong",[a("code",[t._v("chunksize")])]),t._v("="),a("em",[a("code",[t._v("50000")])]),t._v(")")])]),t._v(" "),a("p",[t._v("Tokenize texts in the "),a("code",[t._v("text_cols")]),t._v(" of the csv "),a("code",[t._v("fname")]),t._v(" in parallel using "),a("code",[t._v("n_workers")])]),t._v(" "),a("h4",{staticClass:"doc_header",attrs:{id:"load_tokenized_csv"}},[a("code",[t._v("load_tokenized_csv")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L246"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("load_tokenized_csv")]),t._v("("),a("strong",[a("code",[t._v("fname")])]),t._v(")")])]),t._v(" "),a("p",[t._v("Utility function to quickly load a tokenized csv ans the corresponding counter")]),t._v(" "),a("p",[t._v("The result will be written in a new csv file in "),a("code",[t._v("outname")]),t._v(" (defaults to the same as "),a("code",[t._v("fname")]),t._v(" with the suffix "),a("code",[t._v("_tok.csv")]),t._v(") and will have the same header as the original file, the same non-text columns, a text and a text_lengths column as described in "),a("RouterLink",{attrs:{to:"/text.core.html#tokenize_df"}},[a("code",[t._v("tokenize_df")])]),t._v(".")],1),t._v(" "),a("p",[a("code",[t._v("rules")]),t._v(" (that defaults to "),a("RouterLink",{attrs:{to:"/text.core.html#defaults.text_proc_rules"}},[a("code",[t._v("defaults.text_proc_rules")])]),t._v(") are applied to each text before going in the tokenizer. If "),a("code",[t._v("mark_fields")]),t._v(" isn't specified, it defaults to "),a("code",[t._v("False")]),t._v(" when there is a single text column, "),a("code",[t._v("True")]),t._v(" when there are several. In that case, the texts in each of those columns are joined with "),a("code",[t._v("FLD")]),t._v(" markers followed by the number of the field.")],1),t._v(" "),a("p",[t._v("The csv file is opened with "),a("code",[t._v("header")]),t._v(" and optionally with blocks of "),a("code",[t._v("chunksize")]),t._v(" at a time. If this argument is passed, each chunk is processed independently and saved in the output file to save memory usage.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("_prepare_texts")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tmp_d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Prepare texts in a folder struct in tmp_d, a csv file and returns a dataframe"')]),t._v("\n    path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tmp_d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'tmp'")]),t._v("\n    path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mkdir"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" d "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'b'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'c'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" \n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mkdir"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token string-interpolation"}},[a("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'text")]),a("span",{pre:!0,attrs:{class:"token interpolation"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v(".txt'")])]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" f"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" f"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("write"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string-interpolation"}},[a("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"This is an example of text ')]),a("span",{pre:!0,attrs:{class:"token interpolation"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v(" ")]),a("span",{pre:!0,attrs:{class:"token interpolation"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    texts "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string-interpolation"}},[a("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"This is an example of text ')]),a("span",{pre:!0,attrs:{class:"token interpolation"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v(" ")]),a("span",{pre:!0,attrs:{class:"token interpolation"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" d "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'b'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'c'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    df "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'text'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" texts"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'label'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" columns"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'text'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'label'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    csv_fname "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tmp_d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'input.csv'")]),t._v("\n    df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_csv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("csv_fname"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("csv_fname\n")])])]),a("h3",{staticClass:"doc_header",attrs:{id:"Tokenizer"}},[a("code",[t._v("class")]),t._v(" "),a("code",[t._v("Tokenizer")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L255"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("Tokenizer")]),t._v("("),a("strong",[a("code",[t._v("tok")])]),t._v(", "),a("strong",[a("code",[t._v("rules")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("counter")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("lengths")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("mode")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("sep")])]),t._v("="),a("em",[a("code",[t._v("' '")])]),t._v(") :: "),a("a",{attrs:{href:"https://fastcore.fast.ai/transform#Transform",target:"_blank",rel:"noopener noreferrer"}},[a("code",[t._v("Transform")]),a("OutboundLink")],1)])]),t._v(" "),a("p",[t._v("Provides a consistent "),a("a",{attrs:{href:"https://fastcore.fast.ai/transform#Transform",target:"_blank",rel:"noopener noreferrer"}},[a("code",[t._v("Transform")]),a("OutboundLink")],1),t._v(" interface to tokenizers operating on "),a("code",[t._v("DataFrame")]),t._v("s and folders")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" tempfile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("TemporaryDirectory"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" tmp_d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("csv_fname "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" _prepare_texts"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tmp_d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    items "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_text_files"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    splits "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" RandomSplitter"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("items"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    dsets "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("items"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("Tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_folder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" splits"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("splits"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dsets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    dsets "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("Tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'text'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" splits"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("splits"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dsets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[t._v("((#10) ['xxbos','xxmaj','this','is','an','example','of','text','c','2'],)\n\n\n\n\n\n\n('xxbos', 'xxmaj', 'this', 'is', 'an', 'example', 'of', 'text', 'c', '0')\n")])])]),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("tst "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" test_set"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dsets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'This is a test'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'this is another test'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tst"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'xxbos'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'xxmaj'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'this'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'is'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'test'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n              "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'xxbos'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'this'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'is'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'another'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'test'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h2",{attrs:{id:"sentencepiece"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sentencepiece"}},[t._v("#")]),t._v(" Sentencepiece")]),t._v(" "),a("h3",{staticClass:"doc_header",attrs:{id:"SentencePieceTokenizer"}},[a("code",[t._v("class")]),t._v(" "),a("code",[t._v("SentencePieceTokenizer")]),a("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/text/core.py#L315"}},[t._v("[source]")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("SentencePieceTokenizer")]),t._v("("),a("strong",[a("code",[t._v("lang")])]),t._v("="),a("em",[a("code",[t._v("'en'")])]),t._v(", "),a("strong",[a("code",[t._v("special_toks")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("sp_model")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("vocab_sz")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("max_vocab_sz")])]),t._v("="),a("em",[a("code",[t._v("30000")])]),t._v(", "),a("strong",[a("code",[t._v("model_type")])]),t._v("="),a("em",[a("code",[t._v("'unigram'")])]),t._v(", "),a("strong",[a("code",[t._v("char_coverage")])]),t._v("="),a("em",[a("code",[t._v("None")])]),t._v(", "),a("strong",[a("code",[t._v("cache_dir")])]),t._v("="),a("em",[a("code",[t._v("'tmp'")])]),t._v(")")])]),t._v(" "),a("p",[t._v("SentencePiece tokenizer for "),a("code",[t._v("lang")])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("texts "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string-interpolation"}},[a("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"This is an example of text ')]),a("span",{pre:!0,attrs:{class:"token interpolation"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ndf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'text'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" texts"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'label'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" columns"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'text'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'label'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("cnt "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenize_df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text_cols"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'text'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tok"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("SentencePieceTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vocab_sz"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("34")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_workers"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" tempfile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("TemporaryDirectory"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" tmp_d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("csv_fname "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" _prepare_texts"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tmp_d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    items "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_text_files"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    splits "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" RandomSplitter"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("items"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    tok "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SentencePieceTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("special_toks"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    dsets "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("items"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("Tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_folder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tok"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tok"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" splits"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("splits"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dsets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    dsets "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("Tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'text'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tok"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tok"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" splits"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("splits"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dsets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[t._v("((#33) ['▁xx','b','o','s','▁xx','m','a','j','▁t','h'...],)\n\n\n\n\n\n\n(#33) ['▁xx','b','o','s','▁xx','m','a','j','▁t','h'...]\n")])])])])}),[],!1,null,null,null);s.default=e.exports}}]);