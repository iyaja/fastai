(window.webpackJsonp=window.webpackJsonp||[]).push([[65],{368:function(t,a,s){"use strict";s.r(a);var e=s(42),n=Object(e.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"computer-vision"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#computer-vision"}},[t._v("#")]),t._v(" Computer vision")]),t._v(" "),s("blockquote",[s("p",[t._v("Using the fastai library in computer vision.")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" fastai"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vision"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("all")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n")])])]),s("p",[t._v("This tutorial highlights on how to quickly build a "),s("RouterLink",{attrs:{to:"/learner.html#Learner"}},[s("code",[t._v("Learner")])]),t._v(" and fine tune a pretrained model on most computer vision tasks.")],1),t._v(" "),s("h2",{attrs:{id:"single-label-classification"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#single-label-classification"}},[t._v("#")]),t._v(" Single-label classification")]),t._v(" "),s("p",[t._v("For this task, we will use the "),s("a",{attrs:{href:"https://www.robots.ox.ac.uk/~vgg/data/pets/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Oxford-IIIT Pet Dataset"),s("OutboundLink")],1),t._v(" that contains images of cats and dogs of 37 different breeds. We will first show how to build a simple cat-vs-dog classifier, then a little bit more advanced model that can classify all breeds.")]),t._v(" "),s("p",[t._v("The dataset can be downloaded and decompressed with this line of code:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("path "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("PETS"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("It will only do this download once, and return the location of the decompressed archive. We can check what is inside with the "),s("code",[t._v(".ls()")]),t._v(" method.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(#3) [Path('/home/ashwin/.fastai/data/oxford-iiit-pet/images'),Path('/home/ashwin/.fastai/data/oxford-iiit-pet/models'),Path('/home/ashwin/.fastai/data/oxford-iiit-pet/annotations')]\n")])])]),s("p",[t._v("We will ignore the annotations folder for now, and focus on the images one. "),s("RouterLink",{attrs:{to:"/data.transforms.html#get_image_files"}},[s("code",[t._v("get_image_files")])]),t._v(" is a fastai function that helps us grab all the image files (recursively) in one folder.")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("files "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"images"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("7390\n")])])]),s("h3",{attrs:{id:"cats-vs-dogs"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#cats-vs-dogs"}},[t._v("#")]),t._v(" Cats vs dogs")]),t._v(" "),s("p",[t._v("To label our data for the cats vs dogs problem, we need to know which filenames are of dog pictures and which ones are of cat pictures. There is an easy way to distinguish: the name of the file begins with a capital for cats, and a lowercased letter for dogs:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(Path('/home/ashwin/.fastai/data/oxford-iiit-pet/images/yorkshire_terrier_102.jpg'),\n Path('/home/ashwin/.fastai/data/oxford-iiit-pet/images/great_pyrenees_102.jpg'))\n")])])]),s("p",[t._v("We can then define an easy label function:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("label_func")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("isupper"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("To get our data ready for a model, we need to put it in a "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[t._v("DataLoaders")])]),t._v(" object. Here we have a function that labels using the file names, so we will use "),s("RouterLink",{attrs:{to:"/vision.data.html#ImageDataLoaders.from_name_func"}},[s("code",[t._v("ImageDataLoaders.from_name_func")])]),t._v(". There are other factory methods of "),s("RouterLink",{attrs:{to:"/vision.data.html#ImageDataLoaders"}},[s("code",[t._v("ImageDataLoaders")])]),t._v(" that could be more suitable for your problem, so make sure to check them all in "),s("RouterLink",{attrs:{to:"/vision.data.html"}},[s("code",[t._v("vision.data")])]),t._v(".")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ImageDataLoaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_name_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" item_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("Resize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("We have passed to this function the directory we're working in, the "),s("code",[t._v("files")]),t._v(" we grabbed, our "),s("code",[t._v("label_func")]),t._v(" and one last piece as "),s("code",[t._v("item_tfms")]),t._v(": this is a "),s("a",{attrs:{href:"https://fastcore.fast.ai/transform#Transform",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Transform")]),s("OutboundLink")],1),t._v(" applied on all items of our dataset that will resize each imge to 224 by 224, by using a random crop on the largest dimension to make it a square, then resizing to 224 by 224. If we didn't pass this, we would get an error later as it would be impossible to batch the items together.")]),t._v(" "),s("p",[t._v("We can then check if everything looks okay with the "),s("code",[t._v("show_batch")]),t._v(" method ("),s("code",[t._v("True")]),t._v(" is for cat, "),s("code",[t._v("False")]),t._v(" is for dog):")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_18_0.png",alt:"png"}})]),t._v(" "),s("p",[t._v("Then we can create a "),s("RouterLink",{attrs:{to:"/learner.html#Learner"}},[s("code",[t._v("Learner")])]),t._v(", which is a fastai object that combines the data and a model for training, and uses transfer learning to fine tune a pretrained model in just two lines of code:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cnn_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" resnet34"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("error_rate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fine_tune"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("error_rate")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.138543")]),t._v(" "),s("td",[t._v("0.023240")]),t._v(" "),s("td",[t._v("0.008119")]),t._v(" "),s("td",[t._v("00:18")])])])]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("error_rate")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.055319")]),t._v(" "),s("td",[t._v("0.017367")]),t._v(" "),s("td",[t._v("0.004736")]),t._v(" "),s("td",[t._v("00:24")])])])]),t._v(" "),s("p",[t._v("The first line downloaded a model called ResNet34, pretrained on "),s("a",{attrs:{href:"http://www.image-net.org/",target:"_blank",rel:"noopener noreferrer"}},[t._v("ImageNet"),s("OutboundLink")],1),t._v(", and adapted it to our specific problem. It then fine tuned that model and in a relatively short time, we get a model with an error rate of 0.3%... amazing!")]),t._v(" "),s("p",[t._v("If you want to make a prediction on a new image, you can use "),s("code",[t._v("learn.predict")]),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("('False', tensor(0), tensor([9.9970e-01, 2.9784e-04]))\n")])])]),s("p",[t._v("The predict method returns three things: the decoded prediction (here "),s("code",[t._v("False")]),t._v(" for dog), the index of the predicted class and the tensor of probabilities that our image is one of a dog (here the model is quite confident!) This method accepts a filename, a PIL image or a tensor directly in this case.")]),t._v(" "),s("p",[t._v("We can also have a look at some predictions with the "),s("code",[t._v("show_results")]),t._v(" method:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_results"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_24_1.png",alt:"png"}})]),t._v(" "),s("p",[t._v("Check out the other applications like text or tabular, or the other problems covered in this tutorial, and you will see they all share a consistent API for gathering the data and look at it, create a "),s("RouterLink",{attrs:{to:"/learner.html#Learner"}},[s("code",[t._v("Learner")])]),t._v(", train the model and look at some predictions.")],1),t._v(" "),s("h3",{attrs:{id:"classifying-breeds"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#classifying-breeds"}},[t._v("#")]),t._v(" Classifying breeds")]),t._v(" "),s("p",[t._v("To label our data with the breed name, we will use a regular expression to extract it from the filename. Looking back at a filename, we have:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("'yorkshire_terrier_187.jpg'\n")])])]),s("p",[t._v("so the class is everything before the last "),s("code",[t._v("_")]),t._v(" followed by some digits. A regular expression that will catch the name is thus:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pat "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'^(.*)_\\d+.jpg'")]),t._v("\n")])])]),s("p",[t._v("Since it's pretty common to use regular expressions to label the data (often, labels are hidden in the file names), there is a factory method to do just that:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ImageDataLoaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_name_re"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pat"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" item_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("Resize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Like before, we can then use "),s("code",[t._v("show_batch")]),t._v(" to have a look at our data:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_34_0.png",alt:"png"}})]),t._v(" "),s("p",[t._v("Since classifying the exact breed of cats or dogs amongst 37 different breeds is a harder problem, we will slightly change the definition of our "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[t._v("DataLoaders")])]),t._v(" to use data augmentation:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ImageDataLoaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_name_re"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pat"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" item_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("Resize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("460")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                    batch_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("aug_transforms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("This time we resized to a larger size before batching, and we add "),s("code",[t._v("batch_tfms")]),t._v(". "),s("RouterLink",{attrs:{to:"/vision.augment.html#aug_transforms"}},[s("code",[t._v("aug_transforms")])]),t._v(" is a function that provides a collection of data augmentation transforms with defaults we found worked very well on most datasets (you can customize each one by passing the right arguments).")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_38_0.png",alt:"png"}})]),t._v(" "),s("p",[t._v("We can then create our "),s("RouterLink",{attrs:{to:"/learner.html#Learner"}},[s("code",[t._v("Learner")])]),t._v(" exactly as before and train our model.")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cnn_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" resnet34"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("error_rate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("We used the default learning rate before, but we might want to find the best one possible. For this, we can use the learning rate finder:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lr_find"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(0.010000000149011612, 0.00363078061491251)\n")])])]),s("p",[s("img",{attrs:{src:"output_42_2.png",alt:"png"}})]),t._v(" "),s("p",[t._v("It plots the graph of the learning rate finder and gives us two suggestions (minimum divided by 10 and steepest gradient). Let's use "),s("code",[t._v("3e-3")]),t._v(" here. We will also do a bit more epochs:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fine_tune"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("error_rate")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("1.282734")]),t._v(" "),s("td",[t._v("0.274779")]),t._v(" "),s("td",[t._v("0.085250")]),t._v(" "),s("td",[t._v("00:14")])])])]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("error_rate")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.464303")]),t._v(" "),s("td",[t._v("0.334058")]),t._v(" "),s("td",[t._v("0.102165")]),t._v(" "),s("td",[t._v("00:17")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.433266")]),t._v(" "),s("td",[t._v("0.319324")]),t._v(" "),s("td",[t._v("0.094723")]),t._v(" "),s("td",[t._v("00:17")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("0.259503")]),t._v(" "),s("td",[t._v("0.202348")]),t._v(" "),s("td",[t._v("0.066306")]),t._v(" "),s("td",[t._v("00:17")])]),t._v(" "),s("tr",[s("td",[t._v("3")]),t._v(" "),s("td",[t._v("0.145520")]),t._v(" "),s("td",[t._v("0.183488")]),t._v(" "),s("td",[t._v("0.060217")]),t._v(" "),s("td",[t._v("00:17")])])])]),t._v(" "),s("p",[t._v("Again, we can have a look at some predictions with "),s("code",[t._v("show_results")]),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_results"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_46_1.png",alt:"png"}})]),t._v(" "),s("p",[t._v("Another thing that is useful is an interpretation object, it can show us where the model made the worse predictions:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("interp "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Interpretation"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("interp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("plot_top_losses"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("9")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" figsize"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_49_0.png",alt:"png"}})]),t._v(" "),s("h3",{attrs:{id:"with-the-data-block-api"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#with-the-data-block-api"}},[t._v("#")]),t._v(" With the data block API")]),t._v(" "),s("p",[t._v("We can also use the data block API to get our data in a "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[t._v("DataLoaders")])]),t._v(". This is a bit more advanced, so fell free to skip this part if you are not comfortable with learning new API's just yet.")],1),t._v(" "),s("p",[t._v("A datablock is built by giving the fastai library a bunch of informations:")]),t._v(" "),s("ul",[s("li",[t._v("the types used, through an argument called "),s("code",[t._v("blocks")]),t._v(": here we have images and categories, so we pass "),s("RouterLink",{attrs:{to:"/vision.data.html#ImageBlock"}},[s("code",[t._v("ImageBlock")])]),t._v(" and "),s("RouterLink",{attrs:{to:"/data.block.html#CategoryBlock"}},[s("code",[t._v("CategoryBlock")])]),t._v(".")],1),t._v(" "),s("li",[t._v("how to get the raw items, here our function "),s("RouterLink",{attrs:{to:"/data.transforms.html#get_image_files"}},[s("code",[t._v("get_image_files")])]),t._v(".")],1),t._v(" "),s("li",[t._v("how to label those items, here with the same regular expression as before.")]),t._v(" "),s("li",[t._v("how to split those items, here with a random splitter.")]),t._v(" "),s("li",[t._v("the "),s("code",[t._v("item_tfms")]),t._v(" and "),s("code",[t._v("batch_tfms")]),t._v(" like before.")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pets "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blocks"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" CategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                 get_items"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                 splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("RandomSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 get_y"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("using_attr"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("RegexLabeller"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'(.+)_\\d+.jpg$'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'name'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 item_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("Resize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("460")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 batch_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("aug_transforms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("The pets object by itself is empty: it only containes the functions that will help us gather the data. We have to call "),s("code",[t._v("dataloaders")]),t._v(" method to get a "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[t._v("DataLoaders")])]),t._v(". We pass it the source of the data:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("PETS"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"images"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Then we can look at some of our pictures with "),s("code",[t._v("dls.show_batch()")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("max_n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("9")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_56_0.png",alt:"png"}})]),t._v(" "),s("h2",{attrs:{id:"multi-label-classification"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#multi-label-classification"}},[t._v("#")]),t._v(" Multi-label classification")]),t._v(" "),s("p",[t._v("For this task, we will use the "),s("a",{attrs:{href:"http://host.robots.ox.ac.uk/pascal/VOC/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Pascal Dataset"),s("OutboundLink")],1),t._v(" that contains images with different kinds of objects/persons. It's orginally a dataset for object detection, meaning the task is not only to detect if there is an instance of one class of an image, but to also draw a bounding box around it. Here we will just try to predict all the classes in one given image.")]),t._v(" "),s("p",[t._v("Multi-label classification defers from before in the sense each image does not belong to one category. An image could have a person "),s("em",[t._v("and")]),t._v(" a horse inside it for instance. Or have none of the categories we study.")]),t._v(" "),s("p",[t._v("As before, we can download the dataset pretty easily:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("path "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("PASCAL_2007"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(#9) [Path('/home/ashwin/.fastai/data/pascal_2007/valid.json'),Path('/home/ashwin/.fastai/data/pascal_2007/segmentation'),Path('/home/ashwin/.fastai/data/pascal_2007/train.csv'),Path('/home/ashwin/.fastai/data/pascal_2007/test.csv'),Path('/home/ashwin/.fastai/data/pascal_2007/models'),Path('/home/ashwin/.fastai/data/pascal_2007/test'),Path('/home/ashwin/.fastai/data/pascal_2007/train.json'),Path('/home/ashwin/.fastai/data/pascal_2007/train'),Path('/home/ashwin/.fastai/data/pascal_2007/test.json')]\n")])])]),s("p",[t._v("The information about the labels of each image is in the file named "),s("code",[t._v("train.csv")]),t._v(". We load it using pandas:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("df "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train.csv'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("fname")]),t._v(" "),s("th",[t._v("labels")]),t._v(" "),s("th",[t._v("is_valid")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("000005.jpg")]),t._v(" "),s("td",[t._v("chair")]),t._v(" "),s("td",[t._v("True")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("000007.jpg")]),t._v(" "),s("td",[t._v("car")]),t._v(" "),s("td",[t._v("True")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("000009.jpg")]),t._v(" "),s("td",[t._v("horse person")]),t._v(" "),s("td",[t._v("True")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("000012.jpg")]),t._v(" "),s("td",[t._v("car")]),t._v(" "),s("td",[t._v("False")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("000016.jpg")]),t._v(" "),s("td",[t._v("bicycle")]),t._v(" "),s("td",[t._v("True")])])])])]),t._v(" "),s("h3",{attrs:{id:"using-the-high-level-api"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#using-the-high-level-api"}},[t._v("#")]),t._v(" Using the high-level API")]),t._v(" "),s("p",[t._v("That's pretty straightforward: for each filename, we get the different labels (separated by space) and the last column tells if it's in the validation set or not. To get this in "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[t._v("DataLoaders")])]),t._v(" quickly, we have a factory method, "),s("code",[t._v("from_df")]),t._v(". We can specify the underlying path where all the images are, an additional folder to add between the base path and the filenames (here "),s("code",[t._v("train")]),t._v("), the "),s("code",[t._v("valid_col")]),t._v(" to consider for the validation set (if we don't specify this, we take a random subset), a "),s("code",[t._v("label_delim")]),t._v(" to split the labels and, as before, "),s("code",[t._v("item_tfms")]),t._v(" and "),s("code",[t._v("batch_tfms")]),t._v(".")],1),t._v(" "),s("p",[t._v("Note that we don't have to specify the "),s("code",[t._v("fn_col")]),t._v(" and the "),s("code",[t._v("label_col")]),t._v(" because they default to the first and second column respectively.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ImageDataLoaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" folder"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valid_col"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'is_valid'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label_delim"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("' '")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                               item_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("Resize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("460")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" batch_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("aug_transforms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("As before, we can then have a look at the data with the "),s("code",[t._v("show_batch")]),t._v(" method.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_66_0.png",alt:"png"}})]),t._v(" "),s("p",[t._v("Training a model is as easy as before: the same functions can be applied and the fastai library will automatically detect that we are in a multi-label problem, thus picking the right loss function. The only difference is in the metric we pass: "),s("RouterLink",{attrs:{to:"/metrics.html#error_rate"}},[s("code",[t._v("error_rate")])]),t._v(" will not work for a multi-label problem, but we can use "),s("code",[t._v("accuracy_thresh")]),t._v(".")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cnn_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" resnet50"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("partial"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("accuracy_multi"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" thresh"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("As before, we can use "),s("code",[t._v("learn.lr_find")]),t._v(" to pick a good learning rate:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lr_find"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(0.025118863582611083, 0.033113110810518265)\n")])])]),s("p",[s("img",{attrs:{src:"output_70_2.png",alt:"png"}})]),t._v(" "),s("p",[t._v("We can pick the suggested learning rate and fine-tune our pretrained model:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fine_tune"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy_multi")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.429346")]),t._v(" "),s("td",[t._v("0.128341")]),t._v(" "),s("td",[t._v("0.958785")]),t._v(" "),s("td",[t._v("00:15")])])])]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy_multi")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.161289")]),t._v(" "),s("td",[t._v("0.466793")]),t._v(" "),s("td",[t._v("0.925856")]),t._v(" "),s("td",[t._v("00:17")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.167091")]),t._v(" "),s("td",[t._v("0.269290")]),t._v(" "),s("td",[t._v("0.945757")]),t._v(" "),s("td",[t._v("00:16")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("0.145694")]),t._v(" "),s("td",[t._v("0.121434")]),t._v(" "),s("td",[t._v("0.956355")]),t._v(" "),s("td",[t._v("00:17")])]),t._v(" "),s("tr",[s("td",[t._v("3")]),t._v(" "),s("td",[t._v("0.118979")]),t._v(" "),s("td",[t._v("0.105355")]),t._v(" "),s("td",[t._v("0.962570")]),t._v(" "),s("td",[t._v("00:17")])])])]),t._v(" "),s("p",[t._v("Like before, we can easily have a look at the results:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_results"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_74_1.png",alt:"png"}})]),t._v(" "),s("p",[t._v("Or get the predictions on a given image:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train/000005.jpg'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("((#1) ['chair'],\n tensor([False, False, False, False, False, False, False, False,  True, False,\n         False, False, False, False, False, False, False, False, False, False]),\n tensor([6.0259e-05, 6.2471e-04, 2.9542e-04, 3.7423e-04, 3.9715e-02, 1.9572e-03,\n         7.3608e-04, 1.7575e-02, 9.2661e-01, 6.8919e-04, 4.9777e-01, 1.2133e-02,\n         1.1415e-03, 1.6763e-03, 1.2275e-01, 7.3525e-02, 5.6224e-04, 2.1502e-01,\n         1.7034e-03, 1.4736e-01]))\n")])])]),s("p",[t._v("As for the single classification predictions, we get three things. The last one is the prediction of the model on each class (going from 0 to 1). The second to last cooresponds to a one-hot encoded targets (you get "),s("code",[t._v("True")]),t._v(" for all predicted classes, the ones that get a probability > 0.5) and the first is the decoded, readable version.")]),t._v(" "),s("p",[t._v("And like before, we can check where the model did its worse:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("interp "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Interpretation"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ninterp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("plot_top_losses"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("9")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("target")]),t._v(" "),s("th",[t._v("predicted")]),t._v(" "),s("th",[t._v("probabilities")]),t._v(" "),s("th",[t._v("loss")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("chair;diningtable;person;pottedplant")]),t._v(" "),s("td",[t._v("person")]),t._v(" "),s("td",[t._v("tensor([1.3859e-07, 2.0057e-05, 1.3330e-05, 4.4968e-05, 2.4639e-05, 3.7735e-06,\\n        8.4346e-04, 2.1878e-04, 1.1218e-03, 4.2916e-08, 1.4975e-03, 2.5902e-03,\\n        2.4724e-05, 1.0664e-05, 9.9991e-01, 3.4751e-03, 1.5933e-05, 5.5496e-04,\\n        3.3906e-05, 3.2141e-02])")]),t._v(" "),s("td",[t._v("0.9498050808906555")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("bottle;person")]),t._v(" "),s("td",[t._v("person")]),t._v(" "),s("td",[t._v("tensor([1.7861e-12, 2.4931e-09, 1.8502e-09, 1.2212e-09, 1.9618e-08, 1.3690e-11,\\n        3.2873e-06, 5.8404e-08, 2.5140e-07, 2.7120e-14, 7.0966e-07, 6.9432e-04,\\n        1.1541e-09, 1.0451e-09, 1.0000e+00, 6.0462e-07, 3.4618e-08, 4.7445e-06,\\n        4.1124e-09, 9.9111e-04])")]),t._v(" "),s("td",[t._v("0.8874256014823914")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("bus;car;person")]),t._v(" "),s("td",[t._v("person")]),t._v(" "),s("td",[t._v("tensor([4.9122e-08, 5.5848e-06, 2.1049e-06, 8.9707e-06, 5.6819e-06, 1.3298e-05,\\n        1.4959e-03, 5.9346e-05, 5.3715e-05, 2.1142e-09, 2.5905e-05, 6.1096e-03,\\n        5.5608e-06, 3.3048e-06, 9.9968e-01, 5.6836e-05, 6.0220e-06, 2.0087e-04,\\n        2.4676e-05, 3.3325e-03])")]),t._v(" "),s("td",[t._v("0.8871580362319946")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("bottle;person")]),t._v(" "),s("td",[t._v("person")]),t._v(" "),s("td",[t._v("tensor([5.2528e-13, 1.7562e-09, 3.2824e-10, 1.1594e-09, 2.1161e-08, 1.3134e-11,\\n        2.4606e-06, 3.8796e-08, 8.5993e-07, 3.3807e-15, 1.6779e-06, 1.5134e-04,\\n        1.5394e-10, 5.9556e-10, 1.0000e+00, 3.3567e-06, 9.9627e-09, 8.0715e-06,\\n        2.5843e-09, 4.6214e-03])")]),t._v(" "),s("td",[t._v("0.8837955594062805")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("bottle;person")]),t._v(" "),s("td",[t._v("person")]),t._v(" "),s("td",[t._v("tensor([1.8547e-12, 2.8884e-09, 1.6598e-09, 2.4114e-09, 2.3059e-08, 6.5701e-11,\\n        3.9855e-06, 4.8739e-08, 8.4072e-07, 1.9686e-14, 2.4176e-06, 3.3824e-04,\\n        2.4855e-09, 1.2725e-09, 1.0000e+00, 1.8027e-06, 2.5171e-08, 6.1435e-06,\\n        1.4883e-08, 3.2184e-03])")]),t._v(" "),s("td",[t._v("0.8794390559196472")])]),t._v(" "),s("tr",[s("th",[t._v("5")]),t._v(" "),s("td",[t._v("bottle;person")]),t._v(" "),s("td",[t._v("person")]),t._v(" "),s("td",[t._v("tensor([4.1999e-11, 1.2264e-08, 2.6133e-08, 2.0119e-08, 1.3637e-07, 6.2015e-10,\\n        1.6232e-05, 1.7808e-07, 6.3830e-07, 1.1085e-12, 9.6746e-07, 4.1021e-04,\\n        1.1263e-08, 1.4854e-08, 9.9999e-01, 2.1052e-06, 1.7178e-07, 8.3955e-06,\\n        7.8824e-08, 1.1336e-03])")]),t._v(" "),s("td",[t._v("0.7904742360115051")])]),t._v(" "),s("tr",[s("th",[t._v("6")]),t._v(" "),s("td",[t._v("chair;diningtable;person")]),t._v(" "),s("td",[t._v("person")]),t._v(" "),s("td",[t._v("tensor([3.2536e-08, 5.8821e-06, 4.8512e-06, 1.5012e-05, 4.5857e-04, 1.0428e-06,\\n        2.9736e-04, 4.4146e-05, 4.0318e-04, 3.7364e-09, 4.8184e-04, 1.9579e-03,\\n        8.3174e-07, 3.4988e-06, 9.9994e-01, 1.4466e-03, 7.1675e-06, 4.5994e-04,\\n        9.3516e-06, 2.8538e-02])")]),t._v(" "),s("td",[t._v("0.774388313293457")])]),t._v(" "),s("tr",[s("th",[t._v("7")]),t._v(" "),s("td",[t._v("bus;person")]),t._v(" "),s("td",[t._v("person")]),t._v(" "),s("td",[t._v("tensor([3.3093e-08, 3.1500e-06, 5.7413e-06, 3.6701e-06, 2.1493e-05, 3.8484e-07,\\n        2.8466e-04, 2.2827e-05, 9.6937e-05, 1.6873e-09, 6.1497e-05, 5.1155e-03,\\n        1.3798e-06, 2.3416e-06, 9.9984e-01, 8.3490e-05, 6.5483e-06, 3.6999e-04,\\n        5.8180e-06, 8.0087e-03])")]),t._v(" "),s("td",[t._v("0.7392368316650391")])]),t._v(" "),s("tr",[s("th",[t._v("8")]),t._v(" "),s("td",[t._v("bus;car")]),t._v(" "),s("td",[t._v("train")]),t._v(" "),s("td",[t._v("tensor([1.6565e-03, 4.2173e-04, 3.1245e-04, 1.5796e-03, 2.6684e-04, 9.8883e-03,\\n        1.2766e-02, 1.8388e-04, 4.5448e-03, 6.0582e-04, 1.0346e-03, 5.2797e-04,\\n        1.3014e-03, 1.6428e-03, 5.4353e-02, 5.3857e-04, 8.4819e-04, 3.2369e-04,\\n        9.8810e-01, 6.4698e-04])")]),t._v(" "),s("td",[t._v("0.674037516117096")])])])]),t._v(" "),s("p",[s("img",{attrs:{src:"output_79_1.png",alt:"png"}})]),t._v(" "),s("h3",{attrs:{id:"with-the-data-block-api-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#with-the-data-block-api-2"}},[t._v("#")]),t._v(" With the data block API")]),t._v(" "),s("p",[t._v("We can also use the data block API to get our data in a "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[t._v("DataLoaders")])]),t._v(". Like we said before, feel free to skip this part if you are not comfortable with learning new APIs just yet.")],1),t._v(" "),s("p",[t._v("Remember how the data is structured in our dataframe:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("fname")]),t._v(" "),s("th",[t._v("labels")]),t._v(" "),s("th",[t._v("is_valid")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("000005.jpg")]),t._v(" "),s("td",[t._v("chair")]),t._v(" "),s("td",[t._v("True")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("000007.jpg")]),t._v(" "),s("td",[t._v("car")]),t._v(" "),s("td",[t._v("True")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("000009.jpg")]),t._v(" "),s("td",[t._v("horse person")]),t._v(" "),s("td",[t._v("True")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("000012.jpg")]),t._v(" "),s("td",[t._v("car")]),t._v(" "),s("td",[t._v("False")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("000016.jpg")]),t._v(" "),s("td",[t._v("bicycle")]),t._v(" "),s("td",[t._v("True")])])])])]),t._v(" "),s("p",[t._v("In this case we build the data block by providing:")]),t._v(" "),s("ul",[s("li",[t._v("the types used: "),s("RouterLink",{attrs:{to:"/vision.data.html#ImageBlock"}},[s("code",[t._v("ImageBlock")])]),t._v(" and "),s("RouterLink",{attrs:{to:"/data.block.html#MultiCategoryBlock"}},[s("code",[t._v("MultiCategoryBlock")])]),t._v(".")],1),t._v(" "),s("li",[t._v("how to get the input items from our dataframe: here we read the column "),s("code",[t._v("fname")]),t._v(" and need to add path/train/ at the beginning to get proper filenames.")]),t._v(" "),s("li",[t._v("how to get the targets from our dataframe: here we read the column "),s("code",[t._v("labels")]),t._v(" and need to split by space.")]),t._v(" "),s("li",[t._v("how to split the items, here by using the column "),s("code",[t._v("is_valid")]),t._v(".")]),t._v(" "),s("li",[t._v("the "),s("code",[t._v("item_tfms")]),t._v(" and "),s("code",[t._v("batch_tfms")]),t._v(" like before.")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pascal "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blocks"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" MultiCategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ColSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'is_valid'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   get_x"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ColReader"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'fname'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pref"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" os"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sep"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   get_y"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ColReader"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'labels'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label_delim"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("' '")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   item_tfms "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Resize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("460")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   batch_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("aug_transforms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("This block is slightly different than before: we don't need to pass a function to gather all our items as the dataframe we will give already has them all. However, we do need to preprocess the row of that dataframe to get out inputs, which is why we pass a "),s("code",[t._v("get_x")]),t._v(". It defaults to the fastai function "),s("code",[t._v("noop")]),t._v(", which is why we didn't need to pass it along before.")]),t._v(" "),s("p",[t._v("Like before, "),s("code",[t._v("pascal")]),t._v(" is just a blueprint. We need to pass it the source of our data to be able to get "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[t._v("DataLoaders")])]),t._v(":")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pascal"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Then we can look at some of our pictures with "),s("code",[t._v("dls.show_batch()")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("max_n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("9")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_88_0.png",alt:"png"}})]),t._v(" "),s("h2",{attrs:{id:"segmentation"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#segmentation"}},[t._v("#")]),t._v(" Segmentation")]),t._v(" "),s("p",[t._v("Segmentation is a problem where we have to predict a category for each pixel of the image. For this task, we will use the "),s("a",{attrs:{href:"http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Camvid dataset"),s("OutboundLink")],1),t._v(', a dataset of screenshots from cameras in cars. Each pixel of the image has a label such as "road", "car" or "pedestrian".')]),t._v(" "),s("p",[t._v("As usual, we can download the data with our "),s("RouterLink",{attrs:{to:"/data.external.html#untar_data"}},[s("code",[t._v("untar_data")])]),t._v(" function.")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("path "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("CAMVID_TINY"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(#5) [Path('/home/sgugger/.fastai/data/camvid_tiny/codes.txt'),Path('/home/sgugger/.fastai/data/camvid_tiny/labels'),Path('/home/sgugger/.fastai/data/camvid_tiny/models'),Path('/home/sgugger/.fastai/data/camvid_tiny/export.pkl'),Path('/home/sgugger/.fastai/data/camvid_tiny/images')]\n")])])]),s("p",[t._v("The "),s("code",[t._v("images")]),t._v(" folder contains the images, and the corresponding segmentation masks of labels are in the "),s("code",[t._v("labels")]),t._v(" folder. The "),s("code",[t._v("codes")]),t._v(" file contains the corresponding integer to class (the masks have an int value for each pixel).")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("codes "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loadtxt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'codes.txt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dtype"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ncodes\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("array(['Animal', 'Archway', 'Bicyclist', 'Bridge', 'Building', 'Car',\n       'CartLuggagePram', 'Child', 'Column_Pole', 'Fence', 'LaneMkgsDriv',\n       'LaneMkgsNonDriv', 'Misc_Text', 'MotorcycleScooter', 'OtherMoving',\n       'ParkingBlock', 'Pedestrian', 'Road', 'RoadShoulder', 'Sidewalk',\n       'SignSymbol', 'Sky', 'SUVPickupTruck', 'TrafficCone',\n       'TrafficLight', 'Train', 'Tree', 'Truck_Bus', 'Tunnel',\n       'VegetationMisc', 'Void', 'Wall'], dtype='<U17')\n")])])]),s("h3",{attrs:{id:"using-the-high-level-api-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#using-the-high-level-api-2"}},[t._v("#")]),t._v(" Using the high-level API")]),t._v(" "),s("p",[t._v("As before, the "),s("RouterLink",{attrs:{to:"/data.transforms.html#get_image_files"}},[s("code",[t._v("get_image_files")])]),t._v(" function helps us grab all the image filenames:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("fnames "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"images"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfnames"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("Path('/home/sgugger/.fastai/data/camvid_tiny/images/0016E5_05310.png')\n")])])]),s("p",[t._v("Let's have a look in the labels folder:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"labels"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("Path('/home/sgugger/.fastai/data/camvid_tiny/labels/0016E5_00840_P.png')\n")])])]),s("p",[t._v("It seems the segmentation masks have the same base names as the images but with an extra "),s("code",[t._v("_P")]),t._v(", so we can define a label function:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("label_func")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"labels"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("fn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stem"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("_P")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("fn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("suffix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),t._v("\n")])])]),s("p",[t._v("We can then gather our data using "),s("RouterLink",{attrs:{to:"/vision.data.html#SegmentationDataLoaders"}},[s("code",[t._v("SegmentationDataLoaders")])]),t._v(":")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SegmentationDataLoaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_label_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" bs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fnames "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" fnames"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label_func "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" label_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" codes "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" codes\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("We do not need to pass "),s("code",[t._v("item_tfms")]),t._v(" to resize our images here because they already are all of the same size.")]),t._v(" "),s("p",[t._v("As usual, we can have a look at our data with the "),s("code",[t._v("show_batch")]),t._v(" method. In this instance, the fastai library is superimposing the masks with one specific color per pixel:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("max_n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_104_0.png",alt:"png"}})]),t._v(" "),s("p",[t._v("A traditional CNN won't work for segmentation, we have to use a special kind of model called a UNet, so we use "),s("RouterLink",{attrs:{to:"/vision.learner.html#unet_learner"}},[s("code",[t._v("unet_learner")])]),t._v(" to define our "),s("RouterLink",{attrs:{to:"/learner.html#Learner"}},[s("code",[t._v("Learner")])]),t._v(":")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" unet_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" resnet34"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fine_tune"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("3.227327")]),t._v(" "),s("td",[t._v("2.570531")]),t._v(" "),s("td",[t._v("00:05")])])])]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("1.845436")]),t._v(" "),s("td",[t._v("1.761499")]),t._v(" "),s("td",[t._v("00:02")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("1.595670")]),t._v(" "),s("td",[t._v("1.565473")]),t._v(" "),s("td",[t._v("00:02")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("1.409528")]),t._v(" "),s("td",[t._v("1.209965")]),t._v(" "),s("td",[t._v("00:02")])]),t._v(" "),s("tr",[s("td",[t._v("3")]),t._v(" "),s("td",[t._v("1.283140")]),t._v(" "),s("td",[t._v("1.103817")]),t._v(" "),s("td",[t._v("00:02")])]),t._v(" "),s("tr",[s("td",[t._v("4")]),t._v(" "),s("td",[t._v("1.147777")]),t._v(" "),s("td",[t._v("0.933692")]),t._v(" "),s("td",[t._v("00:02")])]),t._v(" "),s("tr",[s("td",[t._v("5")]),t._v(" "),s("td",[t._v("1.030369")]),t._v(" "),s("td",[t._v("0.924723")]),t._v(" "),s("td",[t._v("00:02")])]),t._v(" "),s("tr",[s("td",[t._v("6")]),t._v(" "),s("td",[t._v("0.932455")]),t._v(" "),s("td",[t._v("0.882634")]),t._v(" "),s("td",[t._v("00:02")])]),t._v(" "),s("tr",[s("td",[t._v("7")]),t._v(" "),s("td",[t._v("0.855784")]),t._v(" "),s("td",[t._v("0.881755")]),t._v(" "),s("td",[t._v("00:02")])])])]),t._v(" "),s("p",[t._v("And as before, we can get some idea of the predicted results with "),s("code",[t._v("show_results")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_results"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("max_n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" figsize"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_108_1.png",alt:"png"}})]),t._v(" "),s("h3",{attrs:{id:"with-the-data-block-api-3"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#with-the-data-block-api-3"}},[t._v("#")]),t._v(" With the data block API")]),t._v(" "),s("p",[t._v("We can also use the data block API to get our data in a "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[t._v("DataLoaders")])]),t._v(". Like it's been said before, feel free to skip this part if you are not comfortable with learning new APIs just yet.")],1),t._v(" "),s("p",[t._v("In this case we build the data block by providing:")]),t._v(" "),s("ul",[s("li",[t._v("the types used: "),s("RouterLink",{attrs:{to:"/vision.data.html#ImageBlock"}},[s("code",[t._v("ImageBlock")])]),t._v(" and "),s("RouterLink",{attrs:{to:"/vision.data.html#MaskBlock"}},[s("code",[t._v("MaskBlock")])]),t._v(". We provide the "),s("code",[t._v("codes")]),t._v(" to "),s("RouterLink",{attrs:{to:"/vision.data.html#MaskBlock"}},[s("code",[t._v("MaskBlock")])]),t._v(" as there is no way to guess them from the data.")],1),t._v(" "),s("li",[t._v("how to gather our items, here by using "),s("RouterLink",{attrs:{to:"/data.transforms.html#get_image_files"}},[s("code",[t._v("get_image_files")])]),t._v(".")],1),t._v(" "),s("li",[t._v("how to get the targets from our items: by using "),s("code",[t._v("label_func")]),t._v(".")]),t._v(" "),s("li",[t._v("how to split the items, here randomly.")]),t._v(" "),s("li",[s("code",[t._v("batch_tfms")]),t._v(" for data augmentation.")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("camvid "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blocks"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" MaskBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("codes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   get_items "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   get_y "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" label_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("RandomSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   batch_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("aug_transforms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("120")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("160")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" camvid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"images"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" bs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("max_n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_113_0.png",alt:"png"}})]),t._v(" "),s("h2",{attrs:{id:"points"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#points"}},[t._v("#")]),t._v(" Points")]),t._v(" "),s("p",[t._v("This section uses the data block API, so if you skipped it before, we recommend you skip this section as well.")]),t._v(" "),s("p",[t._v("We will now look at a task where we want to predict points in a picture. For this, we will use the "),s("a",{attrs:{href:"https://data.vision.ee.ethz.ch/cvl/gfanelli/head_pose/head_forest.html#db",target:"_blank",rel:"noopener noreferrer"}},[t._v("Biwi Kinect Head Pose Dataset"),s("OutboundLink")],1),t._v(". First thing first, let's begin by downloading the dataset as usual.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("path "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("BIWI_HEAD_POSE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Let's see what we've got!")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(#50) [Path('/home/sgugger/.fastai/data/biwi_head_pose/01.obj'),Path('/home/sgugger/.fastai/data/biwi_head_pose/18.obj'),Path('/home/sgugger/.fastai/data/biwi_head_pose/04'),Path('/home/sgugger/.fastai/data/biwi_head_pose/10.obj'),Path('/home/sgugger/.fastai/data/biwi_head_pose/24'),Path('/home/sgugger/.fastai/data/biwi_head_pose/14.obj'),Path('/home/sgugger/.fastai/data/biwi_head_pose/20.obj'),Path('/home/sgugger/.fastai/data/biwi_head_pose/11.obj'),Path('/home/sgugger/.fastai/data/biwi_head_pose/02.obj'),Path('/home/sgugger/.fastai/data/biwi_head_pose/07')...]\n")])])]),s("p",[t._v("There are 24 directories numbered from 01 to 24 (they correspond to the different persons photographed) and a corresponding .obj file (we won't need them here). We'll take a look inside one of these directories:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'01'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(#1000) [Path('01/frame_00087_pose.txt'),Path('01/frame_00079_pose.txt'),Path('01/frame_00114_pose.txt'),Path('01/frame_00084_rgb.jpg'),Path('01/frame_00433_pose.txt'),Path('01/frame_00323_rgb.jpg'),Path('01/frame_00428_rgb.jpg'),Path('01/frame_00373_pose.txt'),Path('01/frame_00188_rgb.jpg'),Path('01/frame_00354_rgb.jpg')...]\n")])])]),s("p",[t._v("Inside the subdirectories, we have different frames, each of them come with an image ("),s("code",[t._v("\\_rgb.jpg")]),t._v(") and a pose file ("),s("code",[t._v("\\_pose.txt")]),t._v("). We can easily get all the image files recursively with "),s("RouterLink",{attrs:{to:"/data.transforms.html#get_image_files"}},[s("code",[t._v("get_image_files")])]),t._v(", then write a function that converts an image filename to its associated pose file.")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("img_files "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("img2pose")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" Path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token format-spec"}},[t._v("-7]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("pose.txt'")])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nimg2pose"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("Path('04/frame_00084_pose.txt')\n")])])]),s("p",[t._v("We can have a look at our first image:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("im "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" PILImage"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("create"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nim"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(480, 640)\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("im"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_thumb"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("160")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_125_0.png",alt:"png"}})]),t._v(" "),s("p",[t._v("The Biwi dataset web site explains the format of the pose text file associated with each image, which shows the location of the center of the head. The details of this aren't important for our purposes, so we'll just show the function we use to extract the head center point:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("cal "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("genfromtxt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'01'")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'rgb.cal'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" skip_footer"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_ctr")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    ctr "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("genfromtxt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img2pose"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" skip_header"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    c1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ctr"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" cal"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("ctr"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" cal"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    c2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ctr"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" cal"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("ctr"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" cal"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" tensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("c1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("This function returns the coordinates as a tensor of two items:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("get_ctr"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("tensor([372.4046, 245.8602])\n")])])]),s("p",[t._v("We can pass this function to "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock"}},[s("code",[t._v("DataBlock")])]),t._v(" as "),s("code",[t._v("get_y")]),t._v(", since it is responsible for labeling each item. We'll resize the images to half their input size, just to speed up training a bit.")],1),t._v(" "),s("p",[t._v("One important point to note is that we should not just use a random splitter. The reason for this is that the same person appears in multiple images in this dataset  but we want to ensure that our model can generalise to people that it hasn't seen yet. Each folder in the dataset contains the images for one person. Therefore, we can create a splitter function which returns true for just one person, resulting in a validation set containing just that person's images.")]),t._v(" "),s("p",[t._v("The only other difference to previous data block examples is that the second block is a "),s("RouterLink",{attrs:{to:"/vision.data.html#PointBlock"}},[s("code",[t._v("PointBlock")])]),t._v(". This is necessary so that fastai knows that the labels represent coordinates; that way, it knows that when doing data augmentation, it should do the same augmentation to these coordinates as it does to the images.")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("biwi "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    blocks"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" PointBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    get_items"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    get_y"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("get_ctr"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("FuncSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parent"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'13'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    batch_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("aug_transforms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("240")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("320")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                Normalize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_stats"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("imagenet_stats"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" biwi"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("max_n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("9")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" figsize"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_132_0.png",alt:"png"}})]),t._v(" "),s("p",[t._v("Now that we have assembled our data, we can use the rest of the fastai API as usual. "),s("RouterLink",{attrs:{to:"/vision.learner.html#cnn_learner"}},[s("code",[t._v("cnn_learner")])]),t._v(" works perfectly in this case, and the library will infer the proper loss function from the data:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cnn_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" resnet18"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_range"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lr_find"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(0.005754399299621582, 3.6307804407442745e-07)\n")])])]),s("p",[s("img",{attrs:{src:"output_135_2.png",alt:"png"}})]),t._v(" "),s("p",[t._v("Then we can train our model:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fine_tune"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.057434")]),t._v(" "),s("td",[t._v("0.002171")]),t._v(" "),s("td",[t._v("00:31")])])])]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.005320")]),t._v(" "),s("td",[t._v("0.005426")]),t._v(" "),s("td",[t._v("00:39")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.003624")]),t._v(" "),s("td",[t._v("0.000698")]),t._v(" "),s("td",[t._v("00:39")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("0.002163")]),t._v(" "),s("td",[t._v("0.000099")]),t._v(" "),s("td",[t._v("00:39")])]),t._v(" "),s("tr",[s("td",[t._v("3")]),t._v(" "),s("td",[t._v("0.001325")]),t._v(" "),s("td",[t._v("0.000233")]),t._v(" "),s("td",[t._v("00:39")])])])]),t._v(" "),s("p",[t._v("The loss is the mean squared error, so that means we make on average an error of")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("math"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sqrt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.0001")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("0.01\n")])])]),s("p",[t._v("percent when predicting our points! And we can look at those results as usual:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_results"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_141_1.png",alt:"png"}})])])}),[],!1,null,null,null);a.default=n.exports}}]);