(window.webpackJsonp=window.webpackJsonp||[]).push([[19],{409:function(t,a,s){"use strict";s.r(a);var e=s(42),n=Object(e.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"training-callbacks"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#training-callbacks"}},[t._v("#")]),t._v(" Training callbacks")]),t._v(" "),s("blockquote",[s("p",[t._v("Various callbacks to customize training behavior")])]),t._v(" "),s("h2",{staticClass:"doc_header",attrs:{id:"ShortEpochCallback"}},[s("code",[t._v("class")]),t._v(" "),s("code",[t._v("ShortEpochCallback")]),s("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/callback/training.py#L12"}},[t._v("[source]")])]),t._v(" "),s("blockquote",[s("p",[s("code",[t._v("ShortEpochCallback")]),t._v("("),s("strong",[s("code",[t._v("pct")])]),t._v("="),s("em",[s("code",[t._v("0.01")])]),t._v(", "),s("strong",[s("code",[t._v("short_valid")])]),t._v("="),s("em",[s("code",[t._v("True")])]),t._v(") :: "),s("RouterLink",{attrs:{to:"/callback.core.html#Callback"}},[s("code",[t._v("Callback")])])],1)]),t._v(" "),s("p",[t._v("Fit just "),s("code",[t._v("pct")]),t._v(" of an epoch, then stop")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" synth_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cbs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ShortEpochCallback"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("00:00")])])])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" synth_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cbs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ShortEpochCallback"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("short_valid"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("12.395771")]),t._v(" "),s("td",[t._v("00:00")])])])]),t._v(" "),s("h2",{staticClass:"doc_header",attrs:{id:"GradientAccumulation"}},[s("code",[t._v("class")]),t._v(" "),s("code",[t._v("GradientAccumulation")]),s("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/callback/training.py#L22"}},[t._v("[source]")])]),t._v(" "),s("blockquote",[s("p",[s("code",[t._v("GradientAccumulation")]),t._v("("),s("strong",[s("code",[t._v("n_acc")])]),t._v("="),s("em",[s("code",[t._v("32")])]),t._v(") :: "),s("RouterLink",{attrs:{to:"/callback.core.html#Callback"}},[s("code",[t._v("Callback")])])],1)]),t._v(" "),s("p",[t._v("Accumulate gradients before updating weights")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" synth_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nlearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lr"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.01")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cbs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("GradientAccumulation"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n_acc"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ensure train_loss decreased")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("recorder"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("recorder"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\nlearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lr"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.01")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cbs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("GradientAccumulation"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n_acc"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ensure valid_loss didn't change (same weights)")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("recorder"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("recorder"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("10.566907")]),t._v(" "),s("td",[t._v("3.633753")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("5.525984")]),t._v(" "),s("td",[t._v("0.397483")]),t._v(" "),s("td",[t._v("00:00")])])])]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.476599")]),t._v(" "),s("td",[t._v("0.397483")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.478213")]),t._v(" "),s("td",[t._v("0.397483")]),t._v(" "),s("td",[t._v("00:00")])])])]),t._v(" "),s("h2",{attrs:{id:"bnfreeze"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#bnfreeze"}},[t._v("#")]),t._v(" BnFreeze")]),t._v(" "),s("h4",{staticClass:"doc_header",attrs:{id:"set_bn_eval"}},[s("code",[t._v("set_bn_eval")]),s("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/callback/training.py#L40"}},[t._v("[source]")])]),t._v(" "),s("blockquote",[s("p",[s("code",[t._v("set_bn_eval")]),t._v("("),s("strong",[s("code",[t._v("m")])]),t._v(":"),s("RouterLink",{attrs:{to:"/torch_core.html#Module"}},[s("code",[t._v("Module")])]),t._v(", "),s("strong",[s("code",[t._v("use_eval")])]),t._v("="),s("em",[s("code",[t._v("True")])]),t._v(")")],1)]),t._v(" "),s("p",[t._v("Set bn layers in eval mode for all recursive children of "),s("code",[t._v("m")]),t._v(".")]),t._v(" "),s("h2",{staticClass:"doc_header",attrs:{id:"BnFreeze"}},[s("code",[t._v("class")]),t._v(" "),s("code",[t._v("BnFreeze")]),s("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/callback/training.py#L48"}},[t._v("[source]")])]),t._v(" "),s("blockquote",[s("p",[s("code",[t._v("BnFreeze")]),t._v("("),s("strong",[s("code",[t._v("before_fit")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("before_epoch")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("before_train")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("before_batch")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("after_pred")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("after_loss")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("before_backward")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("after_backward")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("after_step")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("after_cancel_batch")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("after_batch")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("after_cancel_train")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("after_train")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("before_validate")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("after_cancel_validate")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("after_validate")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("after_cancel_epoch")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("after_epoch")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("after_cancel_fit")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("after_fit")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(") :: "),s("RouterLink",{attrs:{to:"/callback.core.html#Callback"}},[s("code",[t._v("Callback")])])],1)]),t._v(" "),s("p",[t._v("Freeze moving average statistics in all non-trainable batchnorm layers.")]),t._v(" "),s("p",[s("RouterLink",{attrs:{to:"/callback.training.html#BnFreeze"}},[s("code",[t._v("BnFreeze")])]),t._v(" is useful when you'd like to train two separate models that have a common feature extractor / body. The only part of the model that's different is the head that you attach for transfer learning. "),s("br")],1),t._v(" "),s("p",[s("RouterLink",{attrs:{to:"/learner.html#Learner.freeze()"}},[s("code",[t._v("Learner.freeze()")])]),t._v(" doesn't suffice here as the "),s("RouterLink",{attrs:{to:"/layers.html#BatchNorm"}},[s("code",[t._v("BatchNorm")])]),t._v(" layers are trainable by default, and running mean and std of batches are tracked. For feature extractors to fully match, you need to set "),s("code",[t._v("train_bn=False")]),t._v(" and these stats need to be frozen as well, which is precisely the function of "),s("RouterLink",{attrs:{to:"/callback.training.html#BnFreeze"}},[s("code",[t._v("BnFreeze")])]),t._v(".")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" fastai"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vision"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("all")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("path "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MNIST_TINY"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndls  "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ImageDataLoaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_folder"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valid_pct"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("We first demonstrate the mismatch of the running stats when using only "),s("code",[t._v("train_bn=False")]),t._v(", by creating a "),s("RouterLink",{attrs:{to:"/learner.html#Learner"}},[s("code",[t._v("Learner")])]),t._v("...:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cnn_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("deepcopy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" resnet18"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pretrained"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train_bn"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("...and grab the first "),s("RouterLink",{attrs:{to:"/layers.html#BatchNorm"}},[s("code",[t._v("BatchNorm")])]),t._v(" layer, and store its running mean:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("m "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" learn1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("running_mean"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("clone"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("You can see that now that running mean has changed:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lr"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.02")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_ne"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("learn1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("running_mean"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" m"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("1.058304")]),t._v(" "),s("td",[t._v("0.713414")]),t._v(" "),s("td",[t._v("00:02")])])])]),t._v(" "),s("p",[t._v("When we use the "),s("RouterLink",{attrs:{to:"/callback.training.html#BnFreeze"}},[s("code",[t._v("BnFreeze")])]),t._v(" callback, the running statistics will not be changed during training. This is often important for getting good results from transfer learning.")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cnn_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("deepcopy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" resnet18"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pretrained"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train_bn"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cbs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("BnFreeze"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nm "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" learn1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("running_mean"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("clone"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlearn1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lr"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.02")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_eq"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("learn1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("running_mean"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" m"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.540841")]),t._v(" "),s("td",[t._v("0.432421")]),t._v(" "),s("td",[t._v("00:02")])])])])])}),[],!1,null,null,null);a.default=n.exports}}]);