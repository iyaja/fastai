(window.webpackJsonp=window.webpackJsonp||[]).push([[55],{384:function(a,t,s){"use strict";s.r(t);var e=s(42),n=Object(e.a)({},(function(){var a=this,t=a.$createElement,s=a._self._c||t;return s("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[s("h1",{attrs:{id:"data-block-tutorial"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#data-block-tutorial"}},[a._v("#")]),a._v(" Data block tutorial")]),a._v(" "),s("blockquote",[s("p",[a._v("Using the data block across all applications")])]),a._v(" "),s("p",[a._v("In this tutorial, we'll see how to use the data block API on a variety of tasks and how to debug data blocks. The data block API takes its name from the way it's designed: every bit needed to build the "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[a._v("DataLoaders")])]),a._v(" object (type of inputs, targets, how to label, split...) is encapsulated in a block, and you can mix and match those blocks")],1),a._v(" "),s("h2",{attrs:{id:"building-a-datablock-from-scratch"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#building-a-datablock-from-scratch"}},[a._v("#")]),a._v(" Building a "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock"}},[s("code",[a._v("DataBlock")])]),a._v(" from scratch")],1),a._v(" "),s("p",[a._v("The rest of this tutorial will give many examples, but let's first build a "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock"}},[s("code",[a._v("DataBlock")])]),a._v(" from scratch on the dogs versus cats problem we saw in the "),s("a",{attrs:{href:"http://docs.fast.ai/tutorial.vision",target:"_blank",rel:"noopener noreferrer"}},[a._v("vision tutorial"),s("OutboundLink")],1),a._v(". First we import everything needed in vision.")],1),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("from")]),a._v(" fastai"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[a._v("all")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("from")]),a._v(" fastai"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("vision"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[a._v("all")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v("\n")])])]),s("p",[a._v("The first step is to download and decompress our data (if it's not already done) and get its location:")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("path "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("PETS"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[a._v('And as we saw, all the filenames are in the "images" folder. The '),s("RouterLink",{attrs:{to:"/data.transforms.html#get_image_files"}},[s("code",[a._v("get_image_files")])]),a._v(" function helps get all the images in subfolders:")],1),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("fnames "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"images"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[a._v("Let's begin with an empty "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock"}},[s("code",[a._v("DataBlock")])]),a._v(".")],1),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dblock "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[a._v("By itself, a "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock"}},[s("code",[a._v("DataBlock")])]),a._v(" is just a blue print on how to assemble your data. It does not do anything until you pass it a source. You can choose to then convert that source into a "),s("RouterLink",{attrs:{to:"/data.core.html#Datasets"}},[s("code",[a._v("Datasets")])]),a._v(" or a "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[a._v("DataLoaders")])]),a._v(" by using the "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock.datasets"}},[s("code",[a._v("DataBlock.datasets")])]),a._v(" or "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock.dataloaders"}},[s("code",[a._v("DataBlock.dataloaders")])]),a._v(" method. Since we haven't done anything to get our data ready for batches, the "),s("code",[a._v("dataloaders")]),a._v(" method will fail here, but we can have a look at how it gets converted in "),s("RouterLink",{attrs:{to:"/data.core.html#Datasets"}},[s("code",[a._v("Datasets")])]),a._v(". This is where we pass the source of our data, here all our filenames:")],1),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dsets "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" dblock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("datasets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("fnames"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndsets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[a._v("(Path('/home/jhoward/.fastai/data/oxford-iiit-pet/images/Maine_Coon_91.jpg'),\n Path('/home/jhoward/.fastai/data/oxford-iiit-pet/images/Maine_Coon_91.jpg'))\n")])])]),s("p",[a._v("By default, the data block API assumes we have an input and a target, which is why we see our filename repeated twice.")]),a._v(" "),s("p",[a._v("The first thing we can do is use a "),s("code",[a._v("get_items")]),a._v(" function to actually assemble our items inside the data block:")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dblock "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("get_items "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[a._v("The difference is that you then pass as a source the folder with the images and not all the filenames:")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dsets "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" dblock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("datasets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"images"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndsets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[a._v("(Path('/home/jhoward/.fastai/data/oxford-iiit-pet/images/Persian_76.jpg'),\n Path('/home/jhoward/.fastai/data/oxford-iiit-pet/images/Persian_76.jpg'))\n")])])]),s("p",[a._v('Our inputs are ready to be processed as images (since images can be built from filenames), but our target is not. Since we are in a cat versus dog problem, we need to convert that filename to "cat" vs "dog" (or '),s("code",[a._v("True")]),a._v(" vs "),s("code",[a._v("False")]),a._v("). Let's build a function for this:")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("def")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[a._v("label_func")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("fname"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("return")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"cat"')]),a._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("if")]),a._v(" fname"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("isupper"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("else")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"dog"')]),a._v("\n")])])]),s("p",[a._v("We can then tell our data block to use it to label our target by passing it as "),s("code",[a._v("get_y")]),a._v(":")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dblock "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("get_items "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   get_y     "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" label_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n\ndsets "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" dblock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("datasets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"images"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndsets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[a._v("(Path('/home/jhoward/.fastai/data/oxford-iiit-pet/images/pug_160.jpg'), 'dog')\n")])])]),s("p",[a._v("Now that our inputs and targets are ready, we can specify types to tell the data block API that our inputs are images and our targets are categories. Types are represented by blocks in the data block API, here we use "),s("RouterLink",{attrs:{to:"/vision.data.html#ImageBlock"}},[s("code",[a._v("ImageBlock")])]),a._v(" and "),s("RouterLink",{attrs:{to:"/data.block.html#CategoryBlock"}},[s("code",[a._v("CategoryBlock")])]),a._v(":")],1),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dblock "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("blocks    "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" CategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   get_items "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   get_y     "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" label_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n\ndsets "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" dblock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("datasets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"images"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndsets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[a._v("(PILImage mode=RGB size=500x375, TensorCategory(1))\n")])])]),s("p",[a._v("We can see how the "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock"}},[s("code",[a._v("DataBlock")])]),a._v(' automatically added the transforms necessary to open the image, or how it changed the name "cat" to an index (with a special tensor type). To do this, it created a mapping from categories to index called "vocab" that we can access this way:')],1),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dsets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("vocab\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[a._v("(#2) ['cat','dog']\n")])])]),s("p",[a._v("Note that you can mix and match any block for input and targets, which is why the API is named data block API. You can also have more than two blocks (if you have multiple inputs and/or targets), you would just need to pass "),s("code",[a._v("n_inp")]),a._v(" to the "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock"}},[s("code",[a._v("DataBlock")])]),a._v(" to tell the library how many inputs there are (the rest would be targets) and pass a list of functions to "),s("code",[a._v("get_x")]),a._v(" and/or "),s("code",[a._v("get_y")]),a._v(" (to explain how to process each item to be ready for his type). See the object detection below for such an example.")],1),a._v(" "),s("p",[a._v("The next step is to control how our validation set is created. We do this by passing a "),s("code",[a._v("splitter")]),a._v(" to "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock"}},[s("code",[a._v("DataBlock")])]),a._v(". For instance, here is how to do a random split.")],1),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dblock "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("blocks    "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" CategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   get_items "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   get_y     "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" label_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   splitter  "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" RandomSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n\ndsets "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" dblock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("datasets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"images"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndsets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[a._v("(PILImage mode=RGB size=500x335, TensorCategory(0))\n")])])]),s("p",[a._v("The last step is to specify item transforms and batch transforms (the same way we do it in "),s("RouterLink",{attrs:{to:"/vision.data.html#ImageDataLoaders"}},[s("code",[a._v("ImageDataLoaders")])]),a._v(" factory methods):")],1),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dblock "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("blocks    "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" CategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   get_items "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   get_y     "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" label_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   splitter  "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" RandomSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   item_tfms "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" Resize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("224")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[a._v("With that resize, we are now able to batch items together and can finally call "),s("code",[a._v("dataloaders")]),a._v(" to convert our "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock"}},[s("code",[a._v("DataBlock")])]),a._v(" to a "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[a._v("DataLoaders")])]),a._v(" object:")],1),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" dblock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"images"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_30_0.png",alt:"png"}})]),a._v(" "),s("p",[a._v("The way we usually build the data block in one go is by answering a list of questions:")]),a._v(" "),s("ul",[s("li",[a._v("what is the types of your inputs/targets? Here images and categories")]),a._v(" "),s("li",[a._v("where is your data? Here in filenames in subfolders")]),a._v(" "),s("li",[a._v("does something need to be applied to inputs? Here no")]),a._v(" "),s("li",[a._v("does something need to be applied to the target? Here the "),s("code",[a._v("label_func")]),a._v(" function")]),a._v(" "),s("li",[a._v("how to split the data? Here randomly")]),a._v(" "),s("li",[a._v("do we need to apply something on formed items? Here a resize")]),a._v(" "),s("li",[a._v("do we need to apply something on formed batches? Here no")])]),a._v(" "),s("p",[a._v("This gives us this design:")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dblock "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("blocks    "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" CategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   get_items "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   get_y     "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" label_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   splitter  "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" RandomSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   item_tfms "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" Resize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("224")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[a._v("For two questions that got a no, the corresponding arguments we would pass if the anwser was different would be "),s("code",[a._v("get_x")]),a._v(" and "),s("code",[a._v("batch_tfms")]),a._v(".")]),a._v(" "),s("h2",{attrs:{id:"image-classification"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#image-classification"}},[a._v("#")]),a._v(" Image classification")]),a._v(" "),s("p",[a._v("Let's begin with examples of image classification problems. There are two kinds of image classification problems: problems with single-label (each image has one given label) or multi-label (each image can have multiple or no labels at all). We will cover those two kinds here.")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("from")]),a._v(" fastai"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("vision"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[a._v("all")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v("\n")])])]),s("h3",{attrs:{id:"mnist-single-label"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#mnist-single-label"}},[a._v("#")]),a._v(" MNIST (single label)")]),a._v(" "),s("p",[s("a",{attrs:{href:"http://yann.lecun.com/exdb/mnist/",target:"_blank",rel:"noopener noreferrer"}},[a._v("MNIST"),s("OutboundLink")],1),a._v(" is a dataset of hand-written digits from 0 to 9. We can very easily load it in the data block API by answering the following questions:")]),a._v(" "),s("ul",[s("li",[a._v("what are the types of our inputs and targets? Black and white images and labels.")]),a._v(" "),s("li",[a._v("where is the data? In subfolders.")]),a._v(" "),s("li",[a._v("how do we know if a sample is in the training or the validation set? By looking at the grandparent folder.")]),a._v(" "),s("li",[a._v("how do we know the label of an image? By looking at the parent folder.")])]),a._v(" "),s("p",[a._v("In terms of the API, those answers translate like this:")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("mnist "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("blocks"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("cls"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("PILImageBW"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" CategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" \n                  get_items"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" \n                  splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("GrandparentSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                  get_y"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("parent_label"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[a._v("Our types become blocks: one for images (using the black and white "),s("RouterLink",{attrs:{to:"/vision.core.html#PILImageBW"}},[s("code",[a._v("PILImageBW")])]),a._v(" class) and one for categories. Searching subfolder for all image filenames is done by the "),s("RouterLink",{attrs:{to:"/data.transforms.html#get_image_files"}},[s("code",[a._v("get_image_files")])]),a._v(" function. The split training/validation is done by using a "),s("RouterLink",{attrs:{to:"/data.transforms.html#GrandparentSplitter"}},[s("code",[a._v("GrandparentSplitter")])]),a._v(". And the function to get our targets (often called "),s("code",[a._v("y")]),a._v(") is "),s("RouterLink",{attrs:{to:"/data.transforms.html#parent_label"}},[s("code",[a._v("parent_label")])]),a._v(".")],1),a._v(" "),s("p",[a._v("To get an idea of the objects the fastai library provides for reading, labelling or splitting, check the "),s("RouterLink",{attrs:{to:"/data.transforms.html"}},[s("code",[a._v("data.transforms")])]),a._v(" module.")],1),a._v(" "),s("p",[a._v("In itself, a data block is just a blueprint. It does not do anything and does not check for errors. You have to feed it the source of the data to actually gather something. This is done with the "),s("code",[a._v(".dataloaders")]),a._v(" method:")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" mnist"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("MNIST_TINY"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("max_n"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("9")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" figsize"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_41_0.png",alt:"png"}})]),a._v(" "),s("p",[a._v("If something went wrong in the previous step, or if you're just curious about what happened under the hood, use the "),s("code",[a._v("summary")]),a._v(" method. It will go verbosely step by step, and you will see at which point the process failed.")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("mnist"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("summary"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("MNIST_TINY"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[a._v("Setting-up type transforms pipelines\nCollecting items from /home/jhoward/.fastai/data/mnist_tiny\nFound 1428 items\n2 datasets of sizes 709,699\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: parent_label -> Categorize\n\nBuilding one sample\n  Pipeline: PILBase.create\n    starting from\n      /home/jhoward/.fastai/data/mnist_tiny/train/7/723.png\n    applying PILBase.create gives\n      PILImageBW mode=L size=28x28\n  Pipeline: parent_label -> Categorize\n    starting from\n      /home/jhoward/.fastai/data/mnist_tiny/train/7/723.png\n    applying parent_label gives\n      7\n    applying Categorize gives\n      TensorCategory(1)\n\nFinal sample: (PILImageBW mode=L size=28x28, TensorCategory(1))\n\n\nSetting up after_item: Pipeline: ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: IntToFloatTensor\n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: ToTensor\n    starting from\n      (PILImageBW mode=L size=28x28, TensorCategory(1))\n    applying ToTensor gives\n      (TensorImageBW of size 1x28x28, TensorCategory(1))\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\n\nApplying batch_tfms to the batch built\n  Pipeline: IntToFloatTensor\n    starting from\n      (TensorImageBW of size 4x1x28x28, TensorCategory([1, 1, 1, 1], device='cuda:5'))\n    applying IntToFloatTensor gives\n      (TensorImageBW of size 4x1x28x28, TensorCategory([1, 1, 1, 1], device='cuda:5'))\n")])])]),s("p",[a._v("Let's go over another example!")]),a._v(" "),s("h3",{attrs:{id:"pets-single-label"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#pets-single-label"}},[a._v("#")]),a._v(" Pets (single label)")]),a._v(" "),s("p",[a._v("The "),s("a",{attrs:{href:"https://www.robots.ox.ac.uk/~vgg/data/pets/",target:"_blank",rel:"noopener noreferrer"}},[a._v("Oxford IIIT Pets dataset"),s("OutboundLink")],1),a._v(" is a dataset of pictures of dogs and cats, with 37 different breeds. A slight (but very) important difference with MNIST is that images are now not all of the same size. In MNIST they were all 28 by 28 pixels, but here they have different aspect ratios or dimensions. Therefore, we will need to add something to make them all the same size to be able to assemble them together in a batch. We will also see how to add data augmentation.")]),a._v(" "),s("p",[a._v("So let's go over the same questions as before and add two more:")]),a._v(" "),s("ul",[s("li",[a._v("what are the types of our inputs and targets? Images and labels.")]),a._v(" "),s("li",[a._v("where is the data? In subfolders.")]),a._v(" "),s("li",[a._v("how do we know if a sample is in the training or the validation set? We'll take a random split.")]),a._v(" "),s("li",[a._v("how do we know the label of an image? By looking at the parent folder.")]),a._v(" "),s("li",[a._v("do we want to apply a function to a given sample? Yes, we need to resize everything to a given size.")]),a._v(" "),s("li",[a._v("do we want to apply a function to a batch after it's created? Yes, we want data augmentation.")])]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("pets "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("blocks"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" CategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" \n                 get_items"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" \n                 splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("RandomSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                 get_y"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("Pipeline"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("attrgetter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"name"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" RegexLabeller"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("pat "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("r'^(.*)_\\d+.jpg$'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                 item_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("Resize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("128")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                 batch_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("aug_transforms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[a._v("And like for MNIST, we can see how the answers to those questions directly translate in the API. Our types become blocks: one for images and one for categories. Searching subfolder for all image filenames is done by the "),s("RouterLink",{attrs:{to:"/data.transforms.html#get_image_files"}},[s("code",[a._v("get_image_files")])]),a._v(" function. The split training/validation is done by using a "),s("RouterLink",{attrs:{to:"/data.transforms.html#RandomSplitter"}},[s("code",[a._v("RandomSplitter")])]),a._v(". The function to get our targets (often called "),s("code",[a._v("y")]),a._v(") is a composition of two transforms: we get the name attribute of our "),s("code",[a._v("Path")]),a._v(" filenames, then apply a regular expression to get the class. To compose those two transforms into one, we use a "),s("a",{attrs:{href:"https://fastcore.fast.ai/transform#Pipeline",target:"_blank",rel:"noopener noreferrer"}},[s("code",[a._v("Pipeline")]),s("OutboundLink")],1),a._v(".")],1),a._v(" "),s("p",[a._v("Finally, We apply a resize at the item level and "),s("code",[a._v("aug_transforms()")]),a._v(" at the batch level.")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" pets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("PETS"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"images"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("max_n"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("9")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_49_0.png",alt:"png"}})]),a._v(" "),s("p",[a._v("Now let's see how we can use the same API for a multi-label problem.")]),a._v(" "),s("h3",{attrs:{id:"pascal-multi-label"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#pascal-multi-label"}},[a._v("#")]),a._v(" Pascal (multi-label)")]),a._v(" "),s("p",[a._v("The "),s("a",{attrs:{href:"http://host.robots.ox.ac.uk/pascal/VOC/",target:"_blank",rel:"noopener noreferrer"}},[a._v("Pascal dataset"),s("OutboundLink")],1),a._v(" is originally an object detection dataset (we have to predict where some objects are in pictures). But it contains lots of pictures with various objects in them, so it gives a great example for a multi-label problem. Let's download it and have a look at the data:")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("pascal_source "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("PASCAL_2007"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndf "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("read_csv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("pascal_source"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"train.csv"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[a._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),a._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),a._v(" "),s("th",[a._v("fname")]),a._v(" "),s("th",[a._v("labels")]),a._v(" "),s("th",[a._v("is_valid")])])]),a._v(" "),s("tbody",[s("tr",[s("th",[a._v("0")]),a._v(" "),s("td",[a._v("000005.jpg")]),a._v(" "),s("td",[a._v("chair")]),a._v(" "),s("td",[a._v("True")])]),a._v(" "),s("tr",[s("th",[a._v("1")]),a._v(" "),s("td",[a._v("000007.jpg")]),a._v(" "),s("td",[a._v("car")]),a._v(" "),s("td",[a._v("True")])]),a._v(" "),s("tr",[s("th",[a._v("2")]),a._v(" "),s("td",[a._v("000009.jpg")]),a._v(" "),s("td",[a._v("horse person")]),a._v(" "),s("td",[a._v("True")])]),a._v(" "),s("tr",[s("th",[a._v("3")]),a._v(" "),s("td",[a._v("000012.jpg")]),a._v(" "),s("td",[a._v("car")]),a._v(" "),s("td",[a._v("False")])]),a._v(" "),s("tr",[s("th",[a._v("4")]),a._v(" "),s("td",[a._v("000016.jpg")]),a._v(" "),s("td",[a._v("bicycle")]),a._v(" "),s("td",[a._v("True")])])])])]),a._v(" "),s("p",[a._v("So it looks like we have one column with filenames, one column with the labels (separated by space) and one column that tells us if the filename should go in the validation set or not.")]),a._v(" "),s("p",[a._v("There are multiple ways to put this in a "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock"}},[s("code",[a._v("DataBlock")])]),a._v(", let's go over them, but first, let's answer our usual questionnaire:")],1),a._v(" "),s("ul",[s("li",[a._v("what are the types of our inputs and targets? Images and multiple labels.")]),a._v(" "),s("li",[a._v("where is the data? In a dataframe.")]),a._v(" "),s("li",[a._v("how do we know if a sample is in the training or the validation set? A column of our dataframe.")]),a._v(" "),s("li",[a._v("how do we get an image? By looking at the column fname.")]),a._v(" "),s("li",[a._v("how do we know the label of an image? By looking at the column labels.")]),a._v(" "),s("li",[a._v("do we want to apply a function to a given sample? Yes, we need to resize everything to a given size.")]),a._v(" "),s("li",[a._v("do we want to apply a function to a batch after it's created? Yes, we want data augmentation.")])]),a._v(" "),s("p",[a._v("Notice how there is one more question compared to before: we wont have to use a "),s("code",[a._v("get_items")]),a._v(" function here because we already have all our data in one place. But we will need to do something to the raw dataframe to get our inputs, read the first column and add the proper folder before the filename. This is what we pass as "),s("code",[a._v("get_x")]),a._v(".")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("pascal "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("blocks"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" MultiCategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("ColSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   get_x"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("ColReader"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" pref"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("pascal_source"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"train"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   get_y"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("ColReader"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" label_delim"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("' '")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   item_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("Resize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("224")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   batch_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("aug_transforms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[a._v("Again, we can see how the answers to the questions directly translate in the API. Our types become blocks: one for images and one for multi-categories. The split is done by a "),s("RouterLink",{attrs:{to:"/data.transforms.html#ColSplitter"}},[s("code",[a._v("ColSplitter")])]),a._v(" (it defaults to the column named "),s("code",[a._v("is_valid")]),a._v("). The function to get our inputs (often called "),s("code",[a._v("x")]),a._v(") is a "),s("RouterLink",{attrs:{to:"/data.transforms.html#ColReader"}},[s("code",[a._v("ColReader")])]),a._v(" on the first column with a prefix, the function to get our targets (often called "),s("code",[a._v("y")]),a._v(") is "),s("RouterLink",{attrs:{to:"/data.transforms.html#ColReader"}},[s("code",[a._v("ColReader")])]),a._v(" on the second column, with a space delimiter. We apply a resize at the item level and "),s("code",[a._v("aug_transforms()")]),a._v(" at the batch level.")],1),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" pascal"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_58_0.png",alt:"png"}})]),a._v(" "),s("p",[a._v("Another way to do this is by directly using functions for "),s("code",[a._v("get_x")]),a._v(" and "),s("code",[a._v("get_y")]),a._v(":")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("pascal "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("blocks"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" MultiCategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("ColSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   get_x"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("lambda")]),a._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v("pascal_source"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"train"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("/")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[a._v("f'")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("{")]),a._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'")])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   get_y"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("lambda")]),a._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("' '")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   item_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("Resize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("224")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   batch_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("aug_transforms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n\ndls "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" pascal"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_60_0.png",alt:"png"}})]),a._v(" "),s("p",[a._v("Alternatively, we can use the names of the columns as attributes (since rows of a dataframe are pandas series).")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("pascal "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("blocks"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" MultiCategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("ColSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   get_x"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("lambda")]),a._v(" o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[a._v("f'")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("{")]),a._v("pascal_source"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("/train/'")])]),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("+")]),a._v("o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("fname"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   get_y"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("lambda")]),a._v(" o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v("o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   item_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("Resize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("224")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   batch_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("aug_transforms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n\ndls "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" pascal"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_62_0.png",alt:"png"}})]),a._v(" "),s("p",[a._v("The most efficient way (to avoid iterating over the rows of the dataframe, which can take a long time) is to use the "),s("code",[a._v("from_columns")]),a._v(" method. It will use "),s("code",[a._v("get_items")]),a._v(" to convert the columns into numpy arrays. The drawback is that since we lose the dataframe after extracting the relevant columns, we can't use a "),s("RouterLink",{attrs:{to:"/data.transforms.html#ColSplitter"}},[s("code",[a._v("ColSplitter")])]),a._v(" anymore. Here we used an "),s("RouterLink",{attrs:{to:"/data.transforms.html#IndexSplitter"}},[s("code",[a._v("IndexSplitter")])]),a._v(" after manually extracting the index of the validation set from the dataframe:")],1),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("def")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[a._v("_pascal_items")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("return")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("\n    "),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[a._v("f'")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("{")]),a._v("pascal_source"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("/train/'")])]),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("+")]),a._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("fname"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[a._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\nvalid_idx "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'is_valid'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("values\n\npascal "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("from_columns"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("blocks"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" MultiCategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   get_items"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("_pascal_items"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("IndexSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("valid_idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   item_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("Resize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("224")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                   batch_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("aug_transforms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" pascal"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_65_0.png",alt:"png"}})]),a._v(" "),s("h2",{attrs:{id:"image-localization"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#image-localization"}},[a._v("#")]),a._v(" Image localization")]),a._v(" "),s("p",[a._v("There are various problems that fall in the image localization category: image segmentation (which is a task where you have to predict the class of each pixel of an image), coordinate predictions (predict one or several key points on an image) and object detection (draw a box around objects to detect).")]),a._v(" "),s("p",[a._v("Let's see an example of each of those and how to use the data block API in each case.")]),a._v(" "),s("h3",{attrs:{id:"segmentation"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#segmentation"}},[a._v("#")]),a._v(" Segmentation")]),a._v(" "),s("p",[a._v("We will use a small subset of the "),s("a",{attrs:{href:"http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/",target:"_blank",rel:"noopener noreferrer"}},[a._v("CamVid dataset"),s("OutboundLink")],1),a._v(" for our example.")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("path "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("CAMVID_TINY"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[a._v("Let's go over our usual questionnaire:")]),a._v(" "),s("ul",[s("li",[a._v("what are the types of our inputs and targets? Images and segmentation masks.")]),a._v(" "),s("li",[a._v("where is the data? In subfolders.")]),a._v(" "),s("li",[a._v("how do we know if a sample is in the training or the validation set? We'll take a random split.")]),a._v(" "),s("li",[a._v('how do we know the label of an image? By looking at a corresponding file in the "labels" folder.')]),a._v(" "),s("li",[a._v("do we want to apply a function to a batch after it's created? Yes, we want data augmentation.")])]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("camvid "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("blocks"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" MaskBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("codes "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("loadtxt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'codes.txt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" dtype"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[a._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n    get_items"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n    splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("RandomSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n    get_y"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("lambda")]),a._v(" o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v(" path"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'labels'")]),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("/")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[a._v("f'")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("{")]),a._v("o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("stem"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("_P")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("{")]),a._v("o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("suffix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'")])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n    batch_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("aug_transforms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[a._v("The "),s("RouterLink",{attrs:{to:"/vision.data.html#MaskBlock"}},[s("code",[a._v("MaskBlock")])]),a._v(" is generated with the "),s("code",[a._v("codes")]),a._v(" that give the correpondence between pixel value of the masks and the object they correspond to (like car, road, pedestrian...). The rest should look pretty familiar by now.")],1),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" camvid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"images"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_74_0.png",alt:"png"}})]),a._v(" "),s("h3",{attrs:{id:"points"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#points"}},[a._v("#")]),a._v(" Points")]),a._v(" "),s("p",[a._v("For this example we will use a small sample of the "),s("a",{attrs:{href:"https://www.kaggle.com/kmader/biwi-kinect-head-pose-database",target:"_blank",rel:"noopener noreferrer"}},[a._v("BiWi kinect head pose dataset"),s("OutboundLink")],1),a._v(". It contains pictures of people and the task is to predict where the center of their head is. We have saved this small dataet with a dictionary filename to center:")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("biwi_source "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("BIWI_SAMPLE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\nfn2ctr "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("biwi_source"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'centers.pkl'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("load"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[a._v("Then we can go over our usual questions:")]),a._v(" "),s("ul",[s("li",[a._v("what are the types of our inputs and targets? Images and points.")]),a._v(" "),s("li",[a._v("where is the data? In subfolders.")]),a._v(" "),s("li",[a._v("how do we know if a sample is in the training or the validation set? We'll take a random split.")]),a._v(" "),s("li",[a._v("how do we know the label of an image? By using the "),s("code",[a._v("fn2ctr")]),a._v(" dictionary.")]),a._v(" "),s("li",[a._v("do we want to apply a function to a batch after it's created? Yes, we want data augmentation.")])]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("biwi "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("blocks"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" PointBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                 get_items"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                 splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("RandomSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                 get_y"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("lambda")]),a._v(" o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v("fn2ctr"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("flip"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                 batch_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("aug_transforms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[a._v("And we can use it to create a "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[a._v("DataLoaders")])]),a._v(":")],1),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" biwi"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("biwi_source"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("max_n"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("9")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_81_0.png",alt:"png"}})]),a._v(" "),s("h3",{attrs:{id:"bounding-boxes"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#bounding-boxes"}},[a._v("#")]),a._v(" Bounding boxes")]),a._v(" "),s("p",[a._v("For this task, we will use a small subset of the "),s("a",{attrs:{href:"http://cocodataset.org/#home",target:"_blank",rel:"noopener noreferrer"}},[a._v("COCO dataset"),s("OutboundLink")],1),a._v(". It contains pictures with day-to-day objects and the goal is to predict where the objects are by drawing a rectangle around them.")]),a._v(" "),s("p",[a._v("The fastai library comes with a function called "),s("RouterLink",{attrs:{to:"/vision.core.html#get_annotations"}},[s("code",[a._v("get_annotations")])]),a._v(" that will interpret the content of "),s("code",[a._v("train.json")]),a._v(" and give us a dictionary filename to (bounding boxes, labels).")],1),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("coco_source "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("COCO_TINY"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\nimages"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" lbl_bbox "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" get_annotations"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("coco_source"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'train.json'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\nimg2bbox "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[a._v("dict")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[a._v("zip")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("images"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" lbl_bbox"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[a._v("Then we can go over our usual questions:")]),a._v(" "),s("ul",[s("li",[a._v("what are the types of our inputs and targets? Images and bounding boxes.")]),a._v(" "),s("li",[a._v("where is the data? In subfolders.")]),a._v(" "),s("li",[a._v("how do we know if a sample is in the training or the validation set? We'll take a random split.")]),a._v(" "),s("li",[a._v("how do we know the label of an image? By using the "),s("code",[a._v("img2bbox")]),a._v(" dictionary.")]),a._v(" "),s("li",[a._v("do we want to apply a function to a given sample? Yes, we need to resize everything to a given size.")]),a._v(" "),s("li",[a._v("do we want to apply a function to a batch after it's created? Yes, we want data augmentation.")])]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("coco "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("blocks"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" BBoxBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" BBoxLblBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                 get_items"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                 splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("RandomSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                 get_y"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("lambda")]),a._v(" o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v(" img2bbox"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("lambda")]),a._v(" o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v(" img2bbox"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" \n                 item_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("Resize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("128")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                 batch_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("aug_transforms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                 n_inp"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[a._v("Note that we provide three types, because we have two targets: the bounding boxes and the labels. That's why we pass "),s("code",[a._v("n_inp=1")]),a._v(" at the end, to tell the library where the inputs stop and the targets begin.")]),a._v(" "),s("p",[a._v("This is also why we pass a list to "),s("code",[a._v("get_y")]),a._v(": since we have two targets, we must tell the library how to label for each of them (you can use "),s("code",[a._v("noop")]),a._v(" if you don't want to do anything for one).")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" coco"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("coco_source"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("max_n"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("9")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_88_0.png",alt:"png"}})]),a._v(" "),s("h2",{attrs:{id:"text"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#text"}},[a._v("#")]),a._v(" Text")]),a._v(" "),s("p",[a._v("We will show two examples: language modeling and text classification. Note that with the data block API, you can adapt the example before for multi-label to a problem where the inputs are texts.")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("from")]),a._v(" fastai"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[a._v("all")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v("\n")])])]),s("h3",{attrs:{id:"language-model"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#language-model"}},[a._v("#")]),a._v(" Language model")]),a._v(" "),s("p",[a._v("We will use a dataset compose of movie reviews from IMDb. As usual, we can download it in one line of code with "),s("RouterLink",{attrs:{to:"/data.external.html#untar_data"}},[s("code",[a._v("untar_data")])]),a._v(".")],1),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("path "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("IMDB_SAMPLE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndf "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("read_csv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'texts.csv'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[a._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),a._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),a._v(" "),s("th",[a._v("label")]),a._v(" "),s("th",[a._v("text")]),a._v(" "),s("th",[a._v("is_valid")])])]),a._v(" "),s("tbody",[s("tr",[s("th",[a._v("0")]),a._v(" "),s("td",[a._v("negative")]),a._v(" "),s("td",[a._v("Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!")]),a._v(" "),s("td",[a._v("False")])]),a._v(" "),s("tr",[s("th",[a._v("1")]),a._v(" "),s("td",[a._v("positive")]),a._v(" "),s("td",[a._v("This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...")]),a._v(" "),s("td",[a._v("False")])]),a._v(" "),s("tr",[s("th",[a._v("2")]),a._v(" "),s("td",[a._v("negative")]),a._v(" "),s("td",[a._v("Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...")]),a._v(" "),s("td",[a._v("False")])]),a._v(" "),s("tr",[s("th",[a._v("3")]),a._v(" "),s("td",[a._v("positive")]),a._v(" "),s("td",[a._v("Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...")]),a._v(" "),s("td",[a._v("False")])]),a._v(" "),s("tr",[s("th",[a._v("4")]),a._v(" "),s("td",[a._v("negative")]),a._v(" "),s("td",[a._v("This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...")]),a._v(" "),s("td",[a._v("False")])])])])]),a._v(" "),s("p",[a._v("We can see it's composed of (pretty long!) reviews labeled positive or negative. Let's go over our usual questions:")]),a._v(" "),s("ul",[s("li",[a._v("what are the types of our inputs and targets? Texts and we don't really have targets, since the targets is derived from the inputs.")]),a._v(" "),s("li",[a._v("where is the data? In a dataframe.")]),a._v(" "),s("li",[a._v("how do we know if a sample is in the training or the validation set? We have an "),s("code",[a._v("is_valid")]),a._v(" column.")]),a._v(" "),s("li",[a._v("how do we get our inputs? In the "),s("code",[a._v("text")]),a._v(" column.")])]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("imdb_lm "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("blocks"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("TextBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("from_df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'text'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" is_lm"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[a._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                    get_x"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("ColReader"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'text'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                    splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("ColSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[a._v("Since there are no targets here, we only have one block to specify. "),s("RouterLink",{attrs:{to:"/text.data.html#TextBlock"}},[s("code",[a._v("TextBlock")])]),a._v("s are a bit special compared to other "),s("RouterLink",{attrs:{to:"/data.block.html#TransformBlock"}},[s("code",[a._v("TransformBlock")])]),a._v("s: to be able to efficiently tokenize all texts during setup, you need to use the class methods "),s("code",[a._v("from_folder")]),a._v(" or "),s("code",[a._v("from_df")]),a._v(".")],1),a._v(" "),s("p",[a._v("We can then get our data into "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[a._v("DataLoaders")])]),a._v(" by passing the dataframe to the "),s("code",[a._v("dataloaders")]),a._v(" method:")],1),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" imdb_lm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" bs"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("64")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" seq_len"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("72")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("max_n"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),a._v(" "),s("th",[a._v("text")]),a._v(" "),s("th",[a._v("text_")])])]),a._v(" "),s("tbody",[s("tr",[s("th",[a._v("0")]),a._v(" "),s("td",[a._v("xxbos xxmaj if this is someone 's \" favorite \" movie , they need some serious help . xxmaj there is nothing funny or clever about this xxunk . i have n't seen the original movie this is the remake of ( some 1950s film ) , but it simply has to be better than this newer xxunk . \\n\\n a major gets kicked out of the military for being a xxunk")]),a._v(" "),s("td",[a._v("xxmaj if this is someone 's \" favorite \" movie , they need some serious help . xxmaj there is nothing funny or clever about this xxunk . i have n't seen the original movie this is the remake of ( some 1950s film ) , but it simply has to be better than this newer xxunk . \\n\\n a major gets kicked out of the military for being a xxunk element")])]),a._v(" "),s("tr",[s("th",[a._v("1")]),a._v(" "),s("td",[a._v('( in all fields ) , desperate to grab onto any " loser " attention he can for himself . xxmaj he is to be xxunk . xxbos xxmaj arnold once again in the 80 \'s demonstrated that he was the king of action and one liners in this futuristic film about a violent game show that no xxunk survives . xxmaj but as the tag line says xxmaj arnold has yet')]),a._v(" "),s("td",[a._v('in all fields ) , desperate to grab onto any " loser " attention he can for himself . xxmaj he is to be xxunk . xxbos xxmaj arnold once again in the 80 \'s demonstrated that he was the king of action and one liners in this futuristic film about a violent game show that no xxunk survives . xxmaj but as the tag line says xxmaj arnold has yet to')])]),a._v(" "),s("tr",[s("th",[a._v("2")]),a._v(" "),s("td",[a._v("xxmaj xxunk , meets up with xxmaj om xxmaj xxunk ( from whom he ran away some 30 years ago and xxunk to again ) and all xxmaj om xxmaj xxunk finds to say is to xxunk of his friendship with xxmaj xxunk xxrep 3 ! xxmaj what a load of crap . xxmaj seriously . xxmaj not to mention the b xxrep 3 a d soundtrack . xxmaj whatever happened to")]),a._v(" "),s("td",[a._v("xxunk , meets up with xxmaj om xxmaj xxunk ( from whom he ran away some 30 years ago and xxunk to again ) and all xxmaj om xxmaj xxunk finds to say is to xxunk of his friendship with xxmaj xxunk xxrep 3 ! xxmaj what a load of crap . xxmaj seriously . xxmaj not to mention the b xxrep 3 a d soundtrack . xxmaj whatever happened to xxmaj")])]),a._v(" "),s("tr",[s("th",[a._v("3")]),a._v(" "),s("td",[a._v("on more as she brings him to her cabin . \\n\\n xxmaj what little romance , sex , or for that matter , anything at all this film has besides bitter xxunk is hardly enough to justify the price of a rental unless you are one of those who love dramas where nothing interesting happens at all . xxmaj yes , the ending is very nicely done , but it is xxunk")]),a._v(" "),s("td",[a._v("more as she brings him to her cabin . \\n\\n xxmaj what little romance , sex , or for that matter , anything at all this film has besides bitter xxunk is hardly enough to justify the price of a rental unless you are one of those who love dramas where nothing interesting happens at all . xxmaj yes , the ending is very nicely done , but it is xxunk reward")])]),a._v(" "),s("tr",[s("th",[a._v("4")]),a._v(" "),s("td",[a._v("of the night before kicking in . \\n\\n xxmaj this is another of those films where there 's no ' plot ' to follow , as such , just a real life feel of these hopeless lives carrying on from one day to the next . xxmaj it 's been acclaimed by many ( including the xxmaj xxunk ! ) but it really was just too grim and bleak for me .")]),a._v(" "),s("td",[a._v("the night before kicking in . \\n\\n xxmaj this is another of those films where there 's no ' plot ' to follow , as such , just a real life feel of these hopeless lives carrying on from one day to the next . xxmaj it 's been acclaimed by many ( including the xxmaj xxunk ! ) but it really was just too grim and bleak for me . i")])]),a._v(" "),s("tr",[s("th",[a._v("5")]),a._v(" "),s("td",[a._v("cry xxmaj freedom \" is a must - see movie for it 's portrayal and story of xxmaj steve xxmaj biko . xxmaj it 's also a xxunk and devastating portrayal of a beautiful land divided and in the xxunk grips of racial xxunk and violence . xxbos xxmaj from a plot and movement standpoint , this movie was terrible . i found myself looking at the clock in theater hoping it")]),a._v(" "),s("td",[a._v("xxmaj freedom \" is a must - see movie for it 's portrayal and story of xxmaj steve xxmaj biko . xxmaj it 's also a xxunk and devastating portrayal of a beautiful land divided and in the xxunk grips of racial xxunk and violence . xxbos xxmaj from a plot and movement standpoint , this movie was terrible . i found myself looking at the clock in theater hoping it would")])])])]),a._v(" "),s("h3",{attrs:{id:"text-classification"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#text-classification"}},[a._v("#")]),a._v(" Text classification")]),a._v(" "),s("p",[a._v("For the text classification, let's go over our usual questions:")]),a._v(" "),s("ul",[s("li",[a._v("what are the types of our inputs and targets? Texts and categories.")]),a._v(" "),s("li",[a._v("where is the data? In a dataframe.")]),a._v(" "),s("li",[a._v("how do we know if a sample is in the training or the validation set? We have an "),s("code",[a._v("is_valid")]),a._v(" column.")]),a._v(" "),s("li",[a._v("how do we get our inputs? In the "),s("code",[a._v("text")]),a._v(" column.")]),a._v(" "),s("li",[a._v("how do we get our targets? In the "),s("code",[a._v("label")]),a._v(" clolumn.")])]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("imdb_clas "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("blocks"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("TextBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("from_df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'text'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" seq_len"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("72")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" vocab"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("vocab"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" CategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                      get_x"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("ColReader"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'text'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                      get_y"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("ColReader"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'label'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n                      splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("ColSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[a._v("Like in the previous example, we use a class method to build a "),s("RouterLink",{attrs:{to:"/text.data.html#TextBlock"}},[s("code",[a._v("TextBlock")])]),a._v(". We can pass it the vocabulary of our language model (very useful for the ULMFit approach). We also show the "),s("code",[a._v("seq_len")]),a._v(" argument (which defaults to 72) just because you need to make sure to use the same here and also in your "),s("RouterLink",{attrs:{to:"/text.learner.html#text_classifier_learner"}},[s("code",[a._v("text_classifier_learner")])]),a._v(".")],1),a._v(" "),s("p",[a._v("{% include warning.html content='You need to make sure to use the same "),s("code",[a._v("seq_len")]),a._v(" in "),s("RouterLink",{attrs:{to:"/text.data.html#TextBlock"}},[s("code",[a._v("TextBlock")])]),a._v(" and the "),s("RouterLink",{attrs:{to:"/learner.html#Learner"}},[s("code",[a._v("Learner")])]),a._v(" you will define later on.' %}")],1),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" imdb_clas"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" bs"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("64")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),a._v(" "),s("th",[a._v("text")]),a._v(" "),s("th",[a._v("category")])])]),a._v(" "),s("tbody",[s("tr",[s("th",[a._v("0")]),a._v(" "),s("td",[a._v("xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review \\n\\n xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , xxunk bowl of xxunk . xxmaj it 's warm and gooey , but you 're not sure if it feels right . xxmaj try as i might , no matter how warm and gooey xxmaj raising xxmaj victor xxmaj vargas became i was always aware that something did n't quite feel right . xxmaj victor xxmaj vargas suffers from a certain xxunk on the director 's part . xxmaj apparently , the director thought that the ethnic backdrop of a xxmaj latino family on the lower east side , and an xxunk storyline would make the film critic proof . xxmaj he was right , but it did n't fool me . xxmaj raising xxmaj victor xxmaj vargas is")]),a._v(" "),s("td",[a._v("negative")])]),a._v(" "),s("tr",[s("th",[a._v("1")]),a._v(" "),s("td",[a._v("xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxbos xxup the xxup shop xxup")]),a._v(" "),s("td",[a._v("positive")])]),a._v(" "),s("tr",[s("th",[a._v("2")]),a._v(" "),s("td",[a._v("xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad")]),a._v(" "),s("td",[a._v("negative")])]),a._v(" "),s("tr",[s("th",[a._v("3")]),a._v(" "),s("td",[a._v("xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad")]),a._v(" "),s("td",[a._v("positive")])]),a._v(" "),s("tr",[s("th",[a._v("4")]),a._v(" "),s("td",[a._v("xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad")]),a._v(" "),s("td",[a._v("negative")])]),a._v(" "),s("tr",[s("th",[a._v("5")]),a._v(" "),s("td",[a._v("xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad")]),a._v(" "),s("td",[a._v("positive")])]),a._v(" "),s("tr",[s("th",[a._v("6")]),a._v(" "),s("td",[a._v("xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad")]),a._v(" "),s("td",[a._v("negative")])]),a._v(" "),s("tr",[s("th",[a._v("7")]),a._v(" "),s("td",[a._v("xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad")]),a._v(" "),s("td",[a._v("positive")])]),a._v(" "),s("tr",[s("th",[a._v("8")]),a._v(" "),s("td",[a._v("xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad")]),a._v(" "),s("td",[a._v("negative")])])])]),a._v(" "),s("h2",{attrs:{id:"tabular-data"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#tabular-data"}},[a._v("#")]),a._v(" Tabular data")]),a._v(" "),s("p",[a._v("Tabular data doesn't really use the data block API as it's relying on another API with "),s("RouterLink",{attrs:{to:"/tabular.core.html#TabularPandas"}},[s("code",[a._v("TabularPandas")])]),a._v(" for efficient preprocessing and batching (there will be some less efficient API that plays nicely with the data block API added in the near future). You can still use different blocks for the targets.")],1),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("from")]),a._v(" fastai"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("tabular"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("core "),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v("\n")])])]),s("p",[a._v("For our example, we will look at a subset of the "),s("a",{attrs:{href:"https://archive.ics.uci.edu/ml/datasets/adult",target:"_blank",rel:"noopener noreferrer"}},[a._v("adult dataset"),s("OutboundLink")],1),a._v(" which contains some census data and where the task is to predict if someone makes more than 50k or not.")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("adult_source "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("ADULT_SAMPLE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndf "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("read_csv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("adult_source"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'adult.csv'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[a._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),a._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),a._v(" "),s("th",[a._v("age")]),a._v(" "),s("th",[a._v("workclass")]),a._v(" "),s("th",[a._v("fnlwgt")]),a._v(" "),s("th",[a._v("education")]),a._v(" "),s("th",[a._v("education-num")]),a._v(" "),s("th",[a._v("marital-status")]),a._v(" "),s("th",[a._v("occupation")]),a._v(" "),s("th",[a._v("relationship")]),a._v(" "),s("th",[a._v("race")]),a._v(" "),s("th",[a._v("sex")]),a._v(" "),s("th",[a._v("capital-gain")]),a._v(" "),s("th",[a._v("capital-loss")]),a._v(" "),s("th",[a._v("hours-per-week")]),a._v(" "),s("th",[a._v("native-country")]),a._v(" "),s("th",[a._v("salary")])])]),a._v(" "),s("tbody",[s("tr",[s("th",[a._v("0")]),a._v(" "),s("td",[a._v("49")]),a._v(" "),s("td",[a._v("Private")]),a._v(" "),s("td",[a._v("101320")]),a._v(" "),s("td",[a._v("Assoc-acdm")]),a._v(" "),s("td",[a._v("12.0")]),a._v(" "),s("td",[a._v("Married-civ-spouse")]),a._v(" "),s("td",[a._v("NaN")]),a._v(" "),s("td",[a._v("Wife")]),a._v(" "),s("td",[a._v("White")]),a._v(" "),s("td",[a._v("Female")]),a._v(" "),s("td",[a._v("0")]),a._v(" "),s("td",[a._v("1902")]),a._v(" "),s("td",[a._v("40")]),a._v(" "),s("td",[a._v("United-States")]),a._v(" "),s("td",[a._v(">=50k")])]),a._v(" "),s("tr",[s("th",[a._v("1")]),a._v(" "),s("td",[a._v("44")]),a._v(" "),s("td",[a._v("Private")]),a._v(" "),s("td",[a._v("236746")]),a._v(" "),s("td",[a._v("Masters")]),a._v(" "),s("td",[a._v("14.0")]),a._v(" "),s("td",[a._v("Divorced")]),a._v(" "),s("td",[a._v("Exec-managerial")]),a._v(" "),s("td",[a._v("Not-in-family")]),a._v(" "),s("td",[a._v("White")]),a._v(" "),s("td",[a._v("Male")]),a._v(" "),s("td",[a._v("10520")]),a._v(" "),s("td",[a._v("0")]),a._v(" "),s("td",[a._v("45")]),a._v(" "),s("td",[a._v("United-States")]),a._v(" "),s("td",[a._v(">=50k")])]),a._v(" "),s("tr",[s("th",[a._v("2")]),a._v(" "),s("td",[a._v("38")]),a._v(" "),s("td",[a._v("Private")]),a._v(" "),s("td",[a._v("96185")]),a._v(" "),s("td",[a._v("HS-grad")]),a._v(" "),s("td",[a._v("NaN")]),a._v(" "),s("td",[a._v("Divorced")]),a._v(" "),s("td",[a._v("NaN")]),a._v(" "),s("td",[a._v("Unmarried")]),a._v(" "),s("td",[a._v("Black")]),a._v(" "),s("td",[a._v("Female")]),a._v(" "),s("td",[a._v("0")]),a._v(" "),s("td",[a._v("0")]),a._v(" "),s("td",[a._v("32")]),a._v(" "),s("td",[a._v("United-States")]),a._v(" "),s("td",[a._v("<50k")])]),a._v(" "),s("tr",[s("th",[a._v("3")]),a._v(" "),s("td",[a._v("38")]),a._v(" "),s("td",[a._v("Self-emp-inc")]),a._v(" "),s("td",[a._v("112847")]),a._v(" "),s("td",[a._v("Prof-school")]),a._v(" "),s("td",[a._v("15.0")]),a._v(" "),s("td",[a._v("Married-civ-spouse")]),a._v(" "),s("td",[a._v("Prof-specialty")]),a._v(" "),s("td",[a._v("Husband")]),a._v(" "),s("td",[a._v("Asian-Pac-Islander")]),a._v(" "),s("td",[a._v("Male")]),a._v(" "),s("td",[a._v("0")]),a._v(" "),s("td",[a._v("0")]),a._v(" "),s("td",[a._v("40")]),a._v(" "),s("td",[a._v("United-States")]),a._v(" "),s("td",[a._v(">=50k")])]),a._v(" "),s("tr",[s("th",[a._v("4")]),a._v(" "),s("td",[a._v("42")]),a._v(" "),s("td",[a._v("Self-emp-not-inc")]),a._v(" "),s("td",[a._v("82297")]),a._v(" "),s("td",[a._v("7th-8th")]),a._v(" "),s("td",[a._v("NaN")]),a._v(" "),s("td",[a._v("Married-civ-spouse")]),a._v(" "),s("td",[a._v("Other-service")]),a._v(" "),s("td",[a._v("Wife")]),a._v(" "),s("td",[a._v("Black")]),a._v(" "),s("td",[a._v("Female")]),a._v(" "),s("td",[a._v("0")]),a._v(" "),s("td",[a._v("0")]),a._v(" "),s("td",[a._v("50")]),a._v(" "),s("td",[a._v("United-States")]),a._v(" "),s("td",[a._v("<50k")])])])])]),a._v(" "),s("p",[a._v("In a tabular problem, we need to split the columns between the ones that represent continuous variables (like the age) and the ones that represent categorical variables (like the education):")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("cat_names "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'workclass'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'education'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'marital-status'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'occupation'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'relationship'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'race'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("\ncont_names "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'age'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'fnlwgt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'education-num'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("\n")])])]),s("p",[a._v("Standard preprocessing in fastai, use those pre-processors:")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("procs "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("Categorify"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" FillMissing"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" Normalize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("\n")])])]),s("p",[s("RouterLink",{attrs:{to:"/tabular.core.html#Categorify"}},[s("code",[a._v("Categorify")])]),a._v(" will change the categorical columns into indices, "),s("RouterLink",{attrs:{to:"/tabular.core.html#FillMissing"}},[s("code",[a._v("FillMissing")])]),a._v(" will fill the missing values in the continuous columns (if any) and add an na categorical column (if necessary). "),s("RouterLink",{attrs:{to:"/data.transforms.html#Normalize"}},[s("code",[a._v("Normalize")])]),a._v(" will normalize the continous columns (substract the mean and divide by the standard deviation).")],1),a._v(" "),s("p",[a._v("We can still use any splitter to create the splits as we'd like them:")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("splits "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" RandomSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("range_of"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[a._v("And then everything goes in a "),s("RouterLink",{attrs:{to:"/tabular.core.html#TabularPandas"}},[s("code",[a._v("TabularPandas")])]),a._v(" object:")],1),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("to "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" TabularPandas"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" procs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" cat_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" cont_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" y_names"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"salary"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" splits"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("splits"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" y_block"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("CategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p",[a._v("We put "),s("code",[a._v("y_block=CategoryBlock")]),a._v(" just to show you how to customize the block for the targets, but it's usually inferred from the data, so you don't need to pass it, normally.")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\ndls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),a._v(" "),s("th",[a._v("workclass")]),a._v(" "),s("th",[a._v("education")]),a._v(" "),s("th",[a._v("marital-status")]),a._v(" "),s("th",[a._v("occupation")]),a._v(" "),s("th",[a._v("relationship")]),a._v(" "),s("th",[a._v("race")]),a._v(" "),s("th",[a._v("education-num_na")]),a._v(" "),s("th",[a._v("age")]),a._v(" "),s("th",[a._v("fnlwgt")]),a._v(" "),s("th",[a._v("education-num")]),a._v(" "),s("th",[a._v("salary")])])]),a._v(" "),s("tbody",[s("tr",[s("th",[a._v("0")]),a._v(" "),s("td",[a._v("Federal-gov")]),a._v(" "),s("td",[a._v("HS-grad")]),a._v(" "),s("td",[a._v("Never-married")]),a._v(" "),s("td",[a._v("Adm-clerical")]),a._v(" "),s("td",[a._v("Own-child")]),a._v(" "),s("td",[a._v("White")]),a._v(" "),s("td",[a._v("False")]),a._v(" "),s("td",[a._v("21.000000")]),a._v(" "),s("td",[a._v("99199.000460")]),a._v(" "),s("td",[a._v("9.0")]),a._v(" "),s("td",[a._v("<50k")])]),a._v(" "),s("tr",[s("th",[a._v("1")]),a._v(" "),s("td",[a._v("?")]),a._v(" "),s("td",[a._v("Some-college")]),a._v(" "),s("td",[a._v("Never-married")]),a._v(" "),s("td",[a._v("?")]),a._v(" "),s("td",[a._v("Own-child")]),a._v(" "),s("td",[a._v("White")]),a._v(" "),s("td",[a._v("False")]),a._v(" "),s("td",[a._v("21.000000")]),a._v(" "),s("td",[a._v("116933.997502")]),a._v(" "),s("td",[a._v("10.0")]),a._v(" "),s("td",[a._v("<50k")])]),a._v(" "),s("tr",[s("th",[a._v("2")]),a._v(" "),s("td",[a._v("Self-emp-not-inc")]),a._v(" "),s("td",[a._v("9th")]),a._v(" "),s("td",[a._v("Married-civ-spouse")]),a._v(" "),s("td",[a._v("Exec-managerial")]),a._v(" "),s("td",[a._v("Other-relative")]),a._v(" "),s("td",[a._v("White")]),a._v(" "),s("td",[a._v("False")]),a._v(" "),s("td",[a._v("56.000001")]),a._v(" "),s("td",[a._v("201317.999844")]),a._v(" "),s("td",[a._v("5.0")]),a._v(" "),s("td",[a._v("<50k")])]),a._v(" "),s("tr",[s("th",[a._v("3")]),a._v(" "),s("td",[a._v("Private")]),a._v(" "),s("td",[a._v("Assoc-voc")]),a._v(" "),s("td",[a._v("Divorced")]),a._v(" "),s("td",[a._v("Prof-specialty")]),a._v(" "),s("td",[a._v("Not-in-family")]),a._v(" "),s("td",[a._v("White")]),a._v(" "),s("td",[a._v("False")]),a._v(" "),s("td",[a._v("36.000000")]),a._v(" "),s("td",[a._v("211021.999814")]),a._v(" "),s("td",[a._v("11.0")]),a._v(" "),s("td",[a._v(">=50k")])]),a._v(" "),s("tr",[s("th",[a._v("4")]),a._v(" "),s("td",[a._v("Self-emp-not-inc")]),a._v(" "),s("td",[a._v("HS-grad")]),a._v(" "),s("td",[a._v("Widowed")]),a._v(" "),s("td",[a._v("Exec-managerial")]),a._v(" "),s("td",[a._v("Not-in-family")]),a._v(" "),s("td",[a._v("White")]),a._v(" "),s("td",[a._v("False")]),a._v(" "),s("td",[a._v("58.000000")]),a._v(" "),s("td",[a._v("204021.000322")]),a._v(" "),s("td",[a._v("9.0")]),a._v(" "),s("td",[a._v("<50k")])]),a._v(" "),s("tr",[s("th",[a._v("5")]),a._v(" "),s("td",[a._v("?")]),a._v(" "),s("td",[a._v("11th")]),a._v(" "),s("td",[a._v("Never-married")]),a._v(" "),s("td",[a._v("#na#")]),a._v(" "),s("td",[a._v("Other-relative")]),a._v(" "),s("td",[a._v("White")]),a._v(" "),s("td",[a._v("True")]),a._v(" "),s("td",[a._v("20.000001")]),a._v(" "),s("td",[a._v("216562.998729")]),a._v(" "),s("td",[a._v("10.0")]),a._v(" "),s("td",[a._v("<50k")])]),a._v(" "),s("tr",[s("th",[a._v("6")]),a._v(" "),s("td",[a._v("State-gov")]),a._v(" "),s("td",[a._v("Doctorate")]),a._v(" "),s("td",[a._v("Married-civ-spouse")]),a._v(" "),s("td",[a._v("Prof-specialty")]),a._v(" "),s("td",[a._v("Husband")]),a._v(" "),s("td",[a._v("White")]),a._v(" "),s("td",[a._v("False")]),a._v(" "),s("td",[a._v("59.000001")]),a._v(" "),s("td",[a._v("192258.000072")]),a._v(" "),s("td",[a._v("16.0")]),a._v(" "),s("td",[a._v(">=50k")])]),a._v(" "),s("tr",[s("th",[a._v("7")]),a._v(" "),s("td",[a._v("?")]),a._v(" "),s("td",[a._v("HS-grad")]),a._v(" "),s("td",[a._v("Never-married")]),a._v(" "),s("td",[a._v("?")]),a._v(" "),s("td",[a._v("Not-in-family")]),a._v(" "),s("td",[a._v("Other")]),a._v(" "),s("td",[a._v("False")]),a._v(" "),s("td",[a._v("20.000001")]),a._v(" "),s("td",[a._v("369678.000710")]),a._v(" "),s("td",[a._v("9.0")]),a._v(" "),s("td",[a._v("<50k")])]),a._v(" "),s("tr",[s("th",[a._v("8")]),a._v(" "),s("td",[a._v("Private")]),a._v(" "),s("td",[a._v("HS-grad")]),a._v(" "),s("td",[a._v("Never-married")]),a._v(" "),s("td",[a._v("Adm-clerical")]),a._v(" "),s("td",[a._v("Unmarried")]),a._v(" "),s("td",[a._v("White")]),a._v(" "),s("td",[a._v("False")]),a._v(" "),s("td",[a._v("43.000000")]),a._v(" "),s("td",[a._v("178976.000199")]),a._v(" "),s("td",[a._v("9.0")]),a._v(" "),s("td",[a._v("<50k")])]),a._v(" "),s("tr",[s("th",[a._v("9")]),a._v(" "),s("td",[a._v("Local-gov")]),a._v(" "),s("td",[a._v("Masters")]),a._v(" "),s("td",[a._v("Never-married")]),a._v(" "),s("td",[a._v("Prof-specialty")]),a._v(" "),s("td",[a._v("Not-in-family")]),a._v(" "),s("td",[a._v("White")]),a._v(" "),s("td",[a._v("False")]),a._v(" "),s("td",[a._v("38.000000")]),a._v(" "),s("td",[a._v("40955.001812")]),a._v(" "),s("td",[a._v("14.0")]),a._v(" "),s("td",[a._v("<50k")])])])])])}),[],!1,null,null,null);t.default=n.exports}}]);