(window.webpackJsonp=window.webpackJsonp||[]).push([[61],{371:function(t,a,v){"use strict";v.r(a);var _=v(42),s=Object(_.a)({},(function(){var t=this,a=t.$createElement,v=t._self._c||a;return v("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[v("h1",{attrs:{id:"tabular-training"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#tabular-training"}},[t._v("#")]),t._v(" Tabular training")]),t._v(" "),v("blockquote",[v("p",[t._v("How to use the tabular application in fastai")])]),t._v(" "),v("p",[t._v("To illustrate the tabular application, we will use the example of the "),v("a",{attrs:{href:"https://archive.ics.uci.edu/ml/datasets/Adult",target:"_blank",rel:"noopener noreferrer"}},[t._v("Adult dataset"),v("OutboundLink")],1),t._v(" where we have to predict if a person is earning more or less than $50k per year using some general data.")]),t._v(" "),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[v("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" fastai"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tabular"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),v("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("all")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n")])])]),v("p",[t._v("We can download a sample of this dataset with the usual "),v("RouterLink",{attrs:{to:"/data.external.html#untar_data"}},[v("code",[t._v("untar_data")])]),t._v(" command:")],1),t._v(" "),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[t._v("path "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" untar_data"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("URLs"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ADULT_SAMPLE"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npath"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ls"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),v("div",{staticClass:"language- extra-class"},[v("pre",[v("code",[t._v("(#3) [Path('/root/.fastai/data/adult_sample/adult.csv'),Path('/root/.fastai/data/adult_sample/models'),Path('/root/.fastai/data/adult_sample/export.pkl')]\n")])])]),v("p",[t._v("Then we can have a look at how the data is structured:")]),t._v(" "),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[t._v("df "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'adult.csv'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndf"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),v("div",[v("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),v("table",{staticClass:"dataframe",attrs:{border:"1"}},[v("thead",[v("tr",{staticStyle:{"text-align":"right"}},[v("th"),t._v(" "),v("th",[t._v("age")]),t._v(" "),v("th",[t._v("workclass")]),t._v(" "),v("th",[t._v("fnlwgt")]),t._v(" "),v("th",[t._v("education")]),t._v(" "),v("th",[t._v("education-num")]),t._v(" "),v("th",[t._v("marital-status")]),t._v(" "),v("th",[t._v("occupation")]),t._v(" "),v("th",[t._v("relationship")]),t._v(" "),v("th",[t._v("race")]),t._v(" "),v("th",[t._v("sex")]),t._v(" "),v("th",[t._v("capital-gain")]),t._v(" "),v("th",[t._v("capital-loss")]),t._v(" "),v("th",[t._v("hours-per-week")]),t._v(" "),v("th",[t._v("native-country")]),t._v(" "),v("th",[t._v("salary")])])]),t._v(" "),v("tbody",[v("tr",[v("th",[t._v("0")]),t._v(" "),v("td",[t._v("49")]),t._v(" "),v("td",[t._v("Private")]),t._v(" "),v("td",[t._v("101320")]),t._v(" "),v("td",[t._v("Assoc-acdm")]),t._v(" "),v("td",[t._v("12.0")]),t._v(" "),v("td",[t._v("Married-civ-spouse")]),t._v(" "),v("td",[t._v("NaN")]),t._v(" "),v("td",[t._v("Wife")]),t._v(" "),v("td",[t._v("White")]),t._v(" "),v("td",[t._v("Female")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("1902")]),t._v(" "),v("td",[t._v("40")]),t._v(" "),v("td",[t._v("United-States")]),t._v(" "),v("td",[t._v(">=50k")])]),t._v(" "),v("tr",[v("th",[t._v("1")]),t._v(" "),v("td",[t._v("44")]),t._v(" "),v("td",[t._v("Private")]),t._v(" "),v("td",[t._v("236746")]),t._v(" "),v("td",[t._v("Masters")]),t._v(" "),v("td",[t._v("14.0")]),t._v(" "),v("td",[t._v("Divorced")]),t._v(" "),v("td",[t._v("Exec-managerial")]),t._v(" "),v("td",[t._v("Not-in-family")]),t._v(" "),v("td",[t._v("White")]),t._v(" "),v("td",[t._v("Male")]),t._v(" "),v("td",[t._v("10520")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("45")]),t._v(" "),v("td",[t._v("United-States")]),t._v(" "),v("td",[t._v(">=50k")])]),t._v(" "),v("tr",[v("th",[t._v("2")]),t._v(" "),v("td",[t._v("38")]),t._v(" "),v("td",[t._v("Private")]),t._v(" "),v("td",[t._v("96185")]),t._v(" "),v("td",[t._v("HS-grad")]),t._v(" "),v("td",[t._v("NaN")]),t._v(" "),v("td",[t._v("Divorced")]),t._v(" "),v("td",[t._v("NaN")]),t._v(" "),v("td",[t._v("Unmarried")]),t._v(" "),v("td",[t._v("Black")]),t._v(" "),v("td",[t._v("Female")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("32")]),t._v(" "),v("td",[t._v("United-States")]),t._v(" "),v("td",[t._v("<50k")])]),t._v(" "),v("tr",[v("th",[t._v("3")]),t._v(" "),v("td",[t._v("38")]),t._v(" "),v("td",[t._v("Self-emp-inc")]),t._v(" "),v("td",[t._v("112847")]),t._v(" "),v("td",[t._v("Prof-school")]),t._v(" "),v("td",[t._v("15.0")]),t._v(" "),v("td",[t._v("Married-civ-spouse")]),t._v(" "),v("td",[t._v("Prof-specialty")]),t._v(" "),v("td",[t._v("Husband")]),t._v(" "),v("td",[t._v("Asian-Pac-Islander")]),t._v(" "),v("td",[t._v("Male")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("40")]),t._v(" "),v("td",[t._v("United-States")]),t._v(" "),v("td",[t._v(">=50k")])]),t._v(" "),v("tr",[v("th",[t._v("4")]),t._v(" "),v("td",[t._v("42")]),t._v(" "),v("td",[t._v("Self-emp-not-inc")]),t._v(" "),v("td",[t._v("82297")]),t._v(" "),v("td",[t._v("7th-8th")]),t._v(" "),v("td",[t._v("NaN")]),t._v(" "),v("td",[t._v("Married-civ-spouse")]),t._v(" "),v("td",[t._v("Other-service")]),t._v(" "),v("td",[t._v("Wife")]),t._v(" "),v("td",[t._v("Black")]),t._v(" "),v("td",[t._v("Female")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("50")]),t._v(" "),v("td",[t._v("United-States")]),t._v(" "),v("td",[t._v("<50k")])])])])]),t._v(" "),v("p",[t._v("Some of the columns are continuous (like age) and we will treat them as float numbers we can feed our model directly. Others are categorical (like workclass or education) and we will convert them to a unique index that we will feed to embedding layers. We can specify our categorical and continuous column names, as well as the name of the dependent variable in "),v("RouterLink",{attrs:{to:"/tabular.data.html#TabularDataLoaders"}},[v("code",[t._v("TabularDataLoaders")])]),t._v(" factory methods:")],1),t._v(" "),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[t._v("dls "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TabularDataLoaders"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_csv"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'adult.csv'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" path"),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("path"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_names"),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),v("span",{pre:!0,attrs:{class:"token string"}},[t._v('"salary"')]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    cat_names "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'workclass'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'education'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'marital-status'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'occupation'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relationship'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'race'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    cont_names "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'age'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'fnlwgt'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'education-num'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    procs "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("Categorify"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FillMissing"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Normalize"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),v("p",[t._v("The last part is the list of pre-processors we apply to our data:")]),t._v(" "),v("ul",[v("li",[v("RouterLink",{attrs:{to:"/tabular.core.html#Categorify"}},[v("code",[t._v("Categorify")])]),t._v(" is going to take every categorical variable and make a map from integer to unique categories, then replace the values by the corresponding index.")],1),t._v(" "),v("li",[v("RouterLink",{attrs:{to:"/tabular.core.html#FillMissing"}},[v("code",[t._v("FillMissing")])]),t._v(" will fill the missing values in the continuous variables by the median of existing values (you can choose a specific value if you prefer)")],1),t._v(" "),v("li",[v("RouterLink",{attrs:{to:"/data.transforms.html#Normalize"}},[v("code",[t._v("Normalize")])]),t._v(" will normalize the continuous variables (substract the mean and divide by the std)")],1)]),t._v(" "),v("p",[t._v("To further expose what's going on below the surface, let's rewrite this utilizing "),v("code",[t._v("fastai")]),t._v("'s "),v("RouterLink",{attrs:{to:"/tabular.core.html#TabularPandas"}},[v("code",[t._v("TabularPandas")])]),t._v(" class. We will need to make one adjustment, which is defining how we want to split our data. By default the factory method above used a random 80/20 split, so we will do the same:")],1),t._v(" "),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[t._v("splits "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" RandomSplitter"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("valid_pct"),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),v("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("range_of"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[t._v("to "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TabularPandas"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" procs"),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("Categorify"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FillMissing"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("Normalize"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   cat_names "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'workclass'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'education'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'marital-status'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'occupation'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relationship'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'race'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   cont_names "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'age'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'fnlwgt'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'education-num'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   y_names"),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'salary'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   splits"),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("splits"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),v("p",[t._v("Before finally building our "),v("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[v("code",[t._v("DataLoaders")])]),t._v(" again:")],1),t._v(" "),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[t._v("dls "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloaders"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bs"),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),v("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),v("blockquote",[v("p",[t._v("Later we will explore why using "),v("RouterLink",{attrs:{to:"/tabular.core.html#TabularPandas"}},[v("code",[t._v("TabularPandas")])]),t._v(" to preprocess will be valuable.")],1)]),t._v(" "),v("p",[t._v("The "),v("code",[t._v("show_batch")]),t._v(" method works like for every other application:")]),t._v(" "),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[t._v("dls"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),v("table",{staticClass:"dataframe",attrs:{border:"1"}},[v("thead",[v("tr",{staticStyle:{"text-align":"right"}},[v("th"),t._v(" "),v("th",[t._v("workclass")]),t._v(" "),v("th",[t._v("education")]),t._v(" "),v("th",[t._v("marital-status")]),t._v(" "),v("th",[t._v("occupation")]),t._v(" "),v("th",[t._v("relationship")]),t._v(" "),v("th",[t._v("race")]),t._v(" "),v("th",[t._v("education-num_na")]),t._v(" "),v("th",[t._v("age")]),t._v(" "),v("th",[t._v("fnlwgt")]),t._v(" "),v("th",[t._v("education-num")]),t._v(" "),v("th",[t._v("salary")])])]),t._v(" "),v("tbody",[v("tr",[v("th",[t._v("0")]),t._v(" "),v("td",[t._v("State-gov")]),t._v(" "),v("td",[t._v("Bachelors")]),t._v(" "),v("td",[t._v("Married-civ-spouse")]),t._v(" "),v("td",[t._v("Prof-specialty")]),t._v(" "),v("td",[t._v("Wife")]),t._v(" "),v("td",[t._v("White")]),t._v(" "),v("td",[t._v("False")]),t._v(" "),v("td",[t._v("41.000000")]),t._v(" "),v("td",[t._v("75409.001182")]),t._v(" "),v("td",[t._v("13.0")]),t._v(" "),v("td",[t._v(">=50k")])]),t._v(" "),v("tr",[v("th",[t._v("1")]),t._v(" "),v("td",[t._v("Private")]),t._v(" "),v("td",[t._v("Some-college")]),t._v(" "),v("td",[t._v("Never-married")]),t._v(" "),v("td",[t._v("Craft-repair")]),t._v(" "),v("td",[t._v("Not-in-family")]),t._v(" "),v("td",[t._v("White")]),t._v(" "),v("td",[t._v("False")]),t._v(" "),v("td",[t._v("24.000000")]),t._v(" "),v("td",[t._v("38455.005013")]),t._v(" "),v("td",[t._v("10.0")]),t._v(" "),v("td",[t._v("<50k")])]),t._v(" "),v("tr",[v("th",[t._v("2")]),t._v(" "),v("td",[t._v("Private")]),t._v(" "),v("td",[t._v("Assoc-acdm")]),t._v(" "),v("td",[t._v("Married-civ-spouse")]),t._v(" "),v("td",[t._v("Prof-specialty")]),t._v(" "),v("td",[t._v("Husband")]),t._v(" "),v("td",[t._v("White")]),t._v(" "),v("td",[t._v("False")]),t._v(" "),v("td",[t._v("48.000000")]),t._v(" "),v("td",[t._v("101299.003093")]),t._v(" "),v("td",[t._v("12.0")]),t._v(" "),v("td",[t._v("<50k")])]),t._v(" "),v("tr",[v("th",[t._v("3")]),t._v(" "),v("td",[t._v("Private")]),t._v(" "),v("td",[t._v("HS-grad")]),t._v(" "),v("td",[t._v("Never-married")]),t._v(" "),v("td",[t._v("Other-service")]),t._v(" "),v("td",[t._v("Other-relative")]),t._v(" "),v("td",[t._v("Black")]),t._v(" "),v("td",[t._v("False")]),t._v(" "),v("td",[t._v("42.000000")]),t._v(" "),v("td",[t._v("227465.999281")]),t._v(" "),v("td",[t._v("9.0")]),t._v(" "),v("td",[t._v("<50k")])]),t._v(" "),v("tr",[v("th",[t._v("4")]),t._v(" "),v("td",[t._v("State-gov")]),t._v(" "),v("td",[t._v("Some-college")]),t._v(" "),v("td",[t._v("Never-married")]),t._v(" "),v("td",[t._v("Prof-specialty")]),t._v(" "),v("td",[t._v("Not-in-family")]),t._v(" "),v("td",[t._v("White")]),t._v(" "),v("td",[t._v("False")]),t._v(" "),v("td",[t._v("20.999999")]),t._v(" "),v("td",[t._v("258489.997130")]),t._v(" "),v("td",[t._v("10.0")]),t._v(" "),v("td",[t._v("<50k")])]),t._v(" "),v("tr",[v("th",[t._v("5")]),t._v(" "),v("td",[t._v("Local-gov")]),t._v(" "),v("td",[t._v("12th")]),t._v(" "),v("td",[t._v("Married-civ-spouse")]),t._v(" "),v("td",[t._v("Tech-support")]),t._v(" "),v("td",[t._v("Husband")]),t._v(" "),v("td",[t._v("White")]),t._v(" "),v("td",[t._v("False")]),t._v(" "),v("td",[t._v("39.000000")]),t._v(" "),v("td",[t._v("207853.000067")]),t._v(" "),v("td",[t._v("8.0")]),t._v(" "),v("td",[t._v("<50k")])]),t._v(" "),v("tr",[v("th",[t._v("6")]),t._v(" "),v("td",[t._v("Private")]),t._v(" "),v("td",[t._v("Assoc-voc")]),t._v(" "),v("td",[t._v("Married-civ-spouse")]),t._v(" "),v("td",[t._v("Sales")]),t._v(" "),v("td",[t._v("Husband")]),t._v(" "),v("td",[t._v("White")]),t._v(" "),v("td",[t._v("False")]),t._v(" "),v("td",[t._v("36.000000")]),t._v(" "),v("td",[t._v("238414.998930")]),t._v(" "),v("td",[t._v("11.0")]),t._v(" "),v("td",[t._v(">=50k")])]),t._v(" "),v("tr",[v("th",[t._v("7")]),t._v(" "),v("td",[t._v("Private")]),t._v(" "),v("td",[t._v("HS-grad")]),t._v(" "),v("td",[t._v("Never-married")]),t._v(" "),v("td",[t._v("Craft-repair")]),t._v(" "),v("td",[t._v("Not-in-family")]),t._v(" "),v("td",[t._v("White")]),t._v(" "),v("td",[t._v("False")]),t._v(" "),v("td",[t._v("19.000000")]),t._v(" "),v("td",[t._v("445727.998937")]),t._v(" "),v("td",[t._v("9.0")]),t._v(" "),v("td",[t._v("<50k")])]),t._v(" "),v("tr",[v("th",[t._v("8")]),t._v(" "),v("td",[t._v("Local-gov")]),t._v(" "),v("td",[t._v("Bachelors")]),t._v(" "),v("td",[t._v("Married-civ-spouse")]),t._v(" "),v("td",[t._v("#na#")]),t._v(" "),v("td",[t._v("Husband")]),t._v(" "),v("td",[t._v("White")]),t._v(" "),v("td",[t._v("True")]),t._v(" "),v("td",[t._v("59.000000")]),t._v(" "),v("td",[t._v("196013.000174")]),t._v(" "),v("td",[t._v("10.0")]),t._v(" "),v("td",[t._v(">=50k")])]),t._v(" "),v("tr",[v("th",[t._v("9")]),t._v(" "),v("td",[t._v("Private")]),t._v(" "),v("td",[t._v("HS-grad")]),t._v(" "),v("td",[t._v("Married-civ-spouse")]),t._v(" "),v("td",[t._v("Prof-specialty")]),t._v(" "),v("td",[t._v("Wife")]),t._v(" "),v("td",[t._v("Black")]),t._v(" "),v("td",[t._v("False")]),t._v(" "),v("td",[t._v("39.000000")]),t._v(" "),v("td",[t._v("147500.000403")]),t._v(" "),v("td",[t._v("9.0")]),t._v(" "),v("td",[t._v("<50k")])])])]),t._v(" "),v("p",[t._v("We can define a model using the "),v("RouterLink",{attrs:{to:"/tabular.learner.html#tabular_learner"}},[v("code",[t._v("tabular_learner")])]),t._v(" method. When we define our model, "),v("code",[t._v("fastai")]),t._v(" will try to infer the loss function based on our "),v("code",[t._v("y_names")]),t._v(" earlier.")],1),t._v(" "),v("p",[v("strong",[t._v("Note")]),t._v(": Sometimes with tabular data, your "),v("code",[t._v("y")]),t._v("'s may be encoded (such as 0 and 1). In such a case you should explicitly pass "),v("code",[t._v("y_block = CategoryBlock")]),t._v(" in your constructor so "),v("code",[t._v("fastai")]),t._v(" won't presume you are doing regression.")]),t._v(" "),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[t._v("learn "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tabular_learner"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("accuracy"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),v("p",[t._v("And we can train that model with the "),v("code",[t._v("fit_one_cycle")]),t._v(" method (the "),v("code",[t._v("fine_tune")]),t._v(" method won't be useful here since we don't have a pretrained model).")]),t._v(" "),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[t._v("learn"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_one_cycle"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),v("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),v("table",{staticClass:"dataframe",attrs:{border:"1"}},[v("thead",[v("tr",{staticStyle:{"text-align":"left"}},[v("th",[t._v("epoch")]),t._v(" "),v("th",[t._v("train_loss")]),t._v(" "),v("th",[t._v("valid_loss")]),t._v(" "),v("th",[t._v("accuracy")]),t._v(" "),v("th",[t._v("time")])])]),t._v(" "),v("tbody",[v("tr",[v("td",[t._v("0")]),t._v(" "),v("td",[t._v("0.369360")]),t._v(" "),v("td",[t._v("0.348096")]),t._v(" "),v("td",[t._v("0.840756")]),t._v(" "),v("td",[t._v("00:05")])])])]),t._v(" "),v("p",[t._v("We can then have a look at some predictions:")]),t._v(" "),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[t._v("learn"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_results"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),v("table",{staticClass:"dataframe",attrs:{border:"1"}},[v("thead",[v("tr",{staticStyle:{"text-align":"right"}},[v("th"),t._v(" "),v("th",[t._v("workclass")]),t._v(" "),v("th",[t._v("education")]),t._v(" "),v("th",[t._v("marital-status")]),t._v(" "),v("th",[t._v("occupation")]),t._v(" "),v("th",[t._v("relationship")]),t._v(" "),v("th",[t._v("race")]),t._v(" "),v("th",[t._v("education-num_na")]),t._v(" "),v("th",[t._v("age")]),t._v(" "),v("th",[t._v("fnlwgt")]),t._v(" "),v("th",[t._v("education-num")]),t._v(" "),v("th",[t._v("salary")]),t._v(" "),v("th",[t._v("salary_pred")])])]),t._v(" "),v("tbody",[v("tr",[v("th",[t._v("0")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("12.0")]),t._v(" "),v("td",[t._v("3.0")]),t._v(" "),v("td",[t._v("8.0")]),t._v(" "),v("td",[t._v("1.0")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("1.0")]),t._v(" "),v("td",[t._v("0.324868")]),t._v(" "),v("td",[t._v("-1.138177")]),t._v(" "),v("td",[t._v("-0.424022")]),t._v(" "),v("td",[t._v("0.0")]),t._v(" "),v("td",[t._v("0.0")])]),t._v(" "),v("tr",[v("th",[t._v("1")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("10.0")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("2.0")]),t._v(" "),v("td",[t._v("2.0")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("1.0")]),t._v(" "),v("td",[t._v("-0.482055")]),t._v(" "),v("td",[t._v("-1.351911")]),t._v(" "),v("td",[t._v("1.148438")]),t._v(" "),v("td",[t._v("0.0")]),t._v(" "),v("td",[t._v("0.0")])]),t._v(" "),v("tr",[v("th",[t._v("2")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("12.0")]),t._v(" "),v("td",[t._v("6.0")]),t._v(" "),v("td",[t._v("12.0")]),t._v(" "),v("td",[t._v("3.0")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("1.0")]),t._v(" "),v("td",[t._v("-0.775482")]),t._v(" "),v("td",[t._v("0.138709")]),t._v(" "),v("td",[t._v("-0.424022")]),t._v(" "),v("td",[t._v("0.0")]),t._v(" "),v("td",[t._v("0.0")])]),t._v(" "),v("tr",[v("th",[t._v("3")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("16.0")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("2.0")]),t._v(" "),v("td",[t._v("4.0")]),t._v(" "),v("td",[t._v("4.0")]),t._v(" "),v("td",[t._v("1.0")]),t._v(" "),v("td",[t._v("-1.362335")]),t._v(" "),v("td",[t._v("-0.227515")]),t._v(" "),v("td",[t._v("-0.030907")]),t._v(" "),v("td",[t._v("0.0")]),t._v(" "),v("td",[t._v("0.0")])]),t._v(" "),v("tr",[v("th",[t._v("4")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("2.0")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("0.0")]),t._v(" "),v("td",[t._v("4.0")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("1.0")]),t._v(" "),v("td",[t._v("-1.509048")]),t._v(" "),v("td",[t._v("-0.191191")]),t._v(" "),v("td",[t._v("-1.210252")]),t._v(" "),v("td",[t._v("0.0")]),t._v(" "),v("td",[t._v("0.0")])]),t._v(" "),v("tr",[v("th",[t._v("5")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("16.0")]),t._v(" "),v("td",[t._v("3.0")]),t._v(" "),v("td",[t._v("13.0")]),t._v(" "),v("td",[t._v("1.0")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("1.0")]),t._v(" "),v("td",[t._v("1.498575")]),t._v(" "),v("td",[t._v("-0.051096")]),t._v(" "),v("td",[t._v("-0.030907")]),t._v(" "),v("td",[t._v("1.0")]),t._v(" "),v("td",[t._v("1.0")])]),t._v(" "),v("tr",[v("th",[t._v("6")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("12.0")]),t._v(" "),v("td",[t._v("3.0")]),t._v(" "),v("td",[t._v("15.0")]),t._v(" "),v("td",[t._v("1.0")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("1.0")]),t._v(" "),v("td",[t._v("-0.555412")]),t._v(" "),v("td",[t._v("0.039167")]),t._v(" "),v("td",[t._v("-0.424022")]),t._v(" "),v("td",[t._v("0.0")]),t._v(" "),v("td",[t._v("0.0")])]),t._v(" "),v("tr",[v("th",[t._v("7")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("1.0")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("6.0")]),t._v(" "),v("td",[t._v("4.0")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("1.0")]),t._v(" "),v("td",[t._v("-1.582405")]),t._v(" "),v("td",[t._v("-1.396391")]),t._v(" "),v("td",[t._v("-1.603367")]),t._v(" "),v("td",[t._v("0.0")]),t._v(" "),v("td",[t._v("0.0")])]),t._v(" "),v("tr",[v("th",[t._v("8")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("3.0")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("13.0")]),t._v(" "),v("td",[t._v("2.0")]),t._v(" "),v("td",[t._v("5.0")]),t._v(" "),v("td",[t._v("1.0")]),t._v(" "),v("td",[t._v("-1.362335")]),t._v(" "),v("td",[t._v("0.158354")]),t._v(" "),v("td",[t._v("-0.817137")]),t._v(" "),v("td",[t._v("0.0")]),t._v(" "),v("td",[t._v("0.0")])])])]),t._v(" "),v("p",[t._v("Or use the predict method on a row:")]),t._v(" "),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[t._v("row"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" clas"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" probs "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" learn"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),v("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[t._v("row"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),v("table",{staticClass:"dataframe",attrs:{border:"1"}},[v("thead",[v("tr",{staticStyle:{"text-align":"right"}},[v("th"),t._v(" "),v("th",[t._v("workclass")]),t._v(" "),v("th",[t._v("education")]),t._v(" "),v("th",[t._v("marital-status")]),t._v(" "),v("th",[t._v("occupation")]),t._v(" "),v("th",[t._v("relationship")]),t._v(" "),v("th",[t._v("race")]),t._v(" "),v("th",[t._v("education-num_na")]),t._v(" "),v("th",[t._v("age")]),t._v(" "),v("th",[t._v("fnlwgt")]),t._v(" "),v("th",[t._v("education-num")]),t._v(" "),v("th",[t._v("salary")])])]),t._v(" "),v("tbody",[v("tr",[v("th",[t._v("0")]),t._v(" "),v("td",[t._v("Private")]),t._v(" "),v("td",[t._v("Assoc-acdm")]),t._v(" "),v("td",[t._v("Married-civ-spouse")]),t._v(" "),v("td",[t._v("#na#")]),t._v(" "),v("td",[t._v("Wife")]),t._v(" "),v("td",[t._v("White")]),t._v(" "),v("td",[t._v("False")]),t._v(" "),v("td",[t._v("49.0")]),t._v(" "),v("td",[t._v("101319.99788")]),t._v(" "),v("td",[t._v("12.0")]),t._v(" "),v("td",[t._v(">=50k")])])])]),t._v(" "),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[t._v("clas"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" probs\n")])])]),v("div",{staticClass:"language- extra-class"},[v("pre",[v("code",[t._v("(tensor(1), tensor([0.4995, 0.5005]))\n")])])]),v("p",[t._v("To get prediction on a new dataframe, you can use the "),v("code",[t._v("test_dl")]),t._v(" method of the "),v("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[v("code",[t._v("DataLoaders")])]),t._v(". That dataframe does not need to have the dependent variable in its column.")],1),t._v(" "),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[t._v("test_df "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("copy"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_df"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("drop"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),v("span",{pre:!0,attrs:{class:"token string"}},[t._v("'salary'")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),v("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inplace"),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),v("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndl "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" learn"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dls"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("test_dl"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_df"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),v("p",[t._v("Then "),v("RouterLink",{attrs:{to:"/learner.html#Learner.get_preds"}},[v("code",[t._v("Learner.get_preds")])]),t._v(" will give you the predictions:")],1),t._v(" "),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[t._v("learn"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_preds"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dl"),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("dl"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),v("div",{staticClass:"language- extra-class"},[v("pre",[v("code",[t._v("(tensor([[0.4995, 0.5005],\n         [0.4882, 0.5118],\n         [0.9824, 0.0176],\n         ...,\n         [0.5324, 0.4676],\n         [0.7628, 0.2372],\n         [0.5934, 0.4066]]), None)\n")])])]),v("h2",{attrs:{id:"fastai-with-other-libraries"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#fastai-with-other-libraries"}},[t._v("#")]),t._v(" "),v("code",[t._v("fastai")]),t._v(" with Other Libraries")]),t._v(" "),v("p",[t._v("As mentioned earlier, "),v("RouterLink",{attrs:{to:"/tabular.core.html#TabularPandas"}},[v("code",[t._v("TabularPandas")])]),t._v(" is a powerful and easy preprocessing tool for tabular data. Integration with libraries such as Random Forests and XGBoost requires only one extra step, that the "),v("code",[t._v(".dataloaders")]),t._v(" call did for us. Let's look at our "),v("code",[t._v("to")]),t._v(" again. It's values are stored in a "),v("code",[t._v("DataFrame")]),t._v(" like object, where we can extract the "),v("code",[t._v("cats")]),t._v(", "),v("code",[t._v("conts,")]),t._v(" "),v("code",[t._v("xs")]),t._v(" and "),v("code",[t._v("ys")]),t._v(" if we want to:")],1),t._v(" "),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[t._v("to"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xs"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),v("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),v("div",[v("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),v("table",{staticClass:"dataframe",attrs:{border:"1"}},[v("thead",[v("tr",{staticStyle:{"text-align":"right"}},[v("th"),t._v(" "),v("th",[t._v("workclass")]),t._v(" "),v("th",[t._v("education")]),t._v(" "),v("th",[t._v("marital-status")]),t._v(" "),v("th",[t._v("occupation")]),t._v(" "),v("th",[t._v("relationship")]),t._v(" "),v("th",[t._v("race")]),t._v(" "),v("th",[t._v("education-num_na")]),t._v(" "),v("th",[t._v("age")]),t._v(" "),v("th",[t._v("fnlwgt")]),t._v(" "),v("th",[t._v("education-num")])])]),t._v(" "),v("tbody",[v("tr",[v("th",[t._v("25387")]),t._v(" "),v("td",[t._v("5")]),t._v(" "),v("td",[t._v("16")]),t._v(" "),v("td",[t._v("3")]),t._v(" "),v("td",[t._v("5")]),t._v(" "),v("td",[t._v("1")]),t._v(" "),v("td",[t._v("5")]),t._v(" "),v("td",[t._v("1")]),t._v(" "),v("td",[t._v("0.471582")]),t._v(" "),v("td",[t._v("-1.467756")]),t._v(" "),v("td",[t._v("-0.030907")])]),t._v(" "),v("tr",[v("th",[t._v("16872")]),t._v(" "),v("td",[t._v("1")]),t._v(" "),v("td",[t._v("16")]),t._v(" "),v("td",[t._v("5")]),t._v(" "),v("td",[t._v("1")]),t._v(" "),v("td",[t._v("4")]),t._v(" "),v("td",[t._v("5")]),t._v(" "),v("td",[t._v("1")]),t._v(" "),v("td",[t._v("-1.215622")]),t._v(" "),v("td",[t._v("-0.649792")]),t._v(" "),v("td",[t._v("-0.030907")])]),t._v(" "),v("tr",[v("th",[t._v("25852")]),t._v(" "),v("td",[t._v("5")]),t._v(" "),v("td",[t._v("16")]),t._v(" "),v("td",[t._v("3")]),t._v(" "),v("td",[t._v("5")]),t._v(" "),v("td",[t._v("1")]),t._v(" "),v("td",[t._v("5")]),t._v(" "),v("td",[t._v("1")]),t._v(" "),v("td",[t._v("1.865358")]),t._v(" "),v("td",[t._v("-0.218915")]),t._v(" "),v("td",[t._v("-0.030907")])])])])]),t._v(" "),v("p",[t._v("To then preprocess our data, all we need to do is call "),v("code",[t._v("process")]),t._v(" to apply all of our "),v("code",[t._v("procs")]),t._v(" inplace:")]),t._v(" "),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[t._v("to"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nto"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xs"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),v("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),v("div",[v("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),v("table",{staticClass:"dataframe",attrs:{border:"1"}},[v("thead",[v("tr",{staticStyle:{"text-align":"right"}},[v("th"),t._v(" "),v("th",[t._v("workclass")]),t._v(" "),v("th",[t._v("education")]),t._v(" "),v("th",[t._v("marital-status")]),t._v(" "),v("th",[t._v("occupation")]),t._v(" "),v("th",[t._v("relationship")]),t._v(" "),v("th",[t._v("race")]),t._v(" "),v("th",[t._v("education-num_na")]),t._v(" "),v("th",[t._v("age")]),t._v(" "),v("th",[t._v("fnlwgt")]),t._v(" "),v("th",[t._v("education-num")])])]),t._v(" "),v("tbody",[v("tr",[v("th",[t._v("25387")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("1")]),t._v(" "),v("td",[t._v("-3.034491")]),t._v(" "),v("td",[t._v("-1.792679")]),t._v(" "),v("td",[t._v("-5.524377")])]),t._v(" "),v("tr",[v("th",[t._v("16872")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("1")]),t._v(" "),v("td",[t._v("-3.043570")]),t._v(" "),v("td",[t._v("-1.792679")]),t._v(" "),v("td",[t._v("-5.524377")])]),t._v(" "),v("tr",[v("th",[t._v("25852")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("0")]),t._v(" "),v("td",[t._v("1")]),t._v(" "),v("td",[t._v("-3.026991")]),t._v(" "),v("td",[t._v("-1.792679")]),t._v(" "),v("td",[t._v("-5.524377")])])])])]),t._v(" "),v("p",[t._v("Now that everything is encoded, you can then send this off to XGBoost or Random Forests by extracting the train and validation sets and their values:")]),t._v(" "),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[t._v("X_train"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xs"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ys"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ravel"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nX_test"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_test "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("valid"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xs"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("valid"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ys"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ravel"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),v("p",[t._v("And now we can directly send this in!")])])}),[],!1,null,null,null);a.default=s.exports}}]);