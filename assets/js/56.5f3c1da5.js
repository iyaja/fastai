(window.webpackJsonp=window.webpackJsonp||[]).push([[56],{361:function(t,a,s){"use strict";s.r(a);var e=s(42),n=Object(e.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"tutorial-training-a-model-on-imagenette"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#tutorial-training-a-model-on-imagenette"}},[t._v("#")]),t._v(" Tutorial - Training a model on Imagenette")]),t._v(" "),s("blockquote",[s("p",[t._v("A dive into the layered API of fastai in computer vision")])]),t._v(" "),s("p",[t._v("The fastai library as a layered API as summarized by this graph:")]),t._v(" "),s("p",[s("img",{attrs:{src:"/images/layered.png",alt:"A layered API"}})]),t._v(" "),s("p",[t._v("If you are following this tutorial, you are probably already familiar with the applications, here we will see how they are powered by the high-level and mid-level API.")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://github.com/fastai/imagenette",target:"_blank",rel:"noopener noreferrer"}},[t._v("Imagenette"),s("OutboundLink")],1),t._v(" is a subset of ImageNet with 10 very different classes. It's great to quickly experiment before trying a fleshed-out technique on the full ImageNet dataset. We will show in this tutorial how to train a model on it, using the usual high-level APIs, then delving inside the fastai library to show you how to use the mid-level APIs we designed. This way you'll be able to customize your own data collection or trainings as needed.")]),t._v(" "),s("h2",{attrs:{id:"assemble-the-data"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#assemble-the-data"}},[t._v("#")]),t._v(" Assemble the data")]),t._v(" "),s("p",[t._v("We will look at several ways to get our data in "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[t._v("DataLoaders")])]),t._v(": first we will use "),s("RouterLink",{attrs:{to:"/vision.data.html#ImageDataLoaders"}},[s("code",[t._v("ImageDataLoaders")])]),t._v(" factory methods (application layer), then the data block API (high level API) and lastly, how to do the same thing with the mid-level API.")],1),t._v(" "),s("h3",{attrs:{id:"loading-the-data-with-a-factory-method"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#loading-the-data-with-a-factory-method"}},[t._v("#")]),t._v(" Loading the data with a factory method")]),t._v(" "),s("p",[t._v("This is the most basic way of assembling the data that we have presented in all the beginner tutorials, so hopefully it should be familiar to you by now.")]),t._v(" "),s("p",[t._v("First, we import everything inside the vision application:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" fastai"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vision"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("all")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n")])])]),s("p",[t._v("Then we download the dataset and decompress it (if needed) and get its location:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("path "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("IMAGENETTE_160"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("We use "),s("RouterLink",{attrs:{to:"/vision.data.html#ImageDataLoaders.from_folder"}},[s("code",[t._v("ImageDataLoaders.from_folder")])]),t._v(" to get everything (since our data is organized in an imageNet-style format):")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ImageDataLoaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_folder"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valid"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'val'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    item_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("RandomResizedCrop"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" min_scale"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.35")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" batch_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("Normalize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_stats"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("imagenet_stats"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("And we can have a look at our data:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_12_0.png",alt:"png"}})]),t._v(" "),s("h3",{attrs:{id:"loading-the-data-with-the-data-block-api"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#loading-the-data-with-the-data-block-api"}},[t._v("#")]),t._v(" Loading the data with the data block API")]),t._v(" "),s("p",[t._v("And as we saw in previous tutorials, the "),s("RouterLink",{attrs:{to:"/data.transforms.html#get_image_files"}},[s("code",[t._v("get_image_files")])]),t._v(" function helps get all the images in subfolders:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("fnames "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Let's begin with an empty "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock"}},[s("code",[t._v("DataBlock")])]),t._v(".")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dblock "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("By itself, a "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock"}},[s("code",[t._v("DataBlock")])]),t._v(" is just a blue print on how to assemble your data. It does not do anything until you pass it a source. You can choose to then convert that source into a "),s("RouterLink",{attrs:{to:"/data.core.html#Datasets"}},[s("code",[t._v("Datasets")])]),t._v(" or a "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[t._v("DataLoaders")])]),t._v(" by using the "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock.datasets"}},[s("code",[t._v("DataBlock.datasets")])]),t._v(" or "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock.dataloaders"}},[s("code",[t._v("DataBlock.dataloaders")])]),t._v(" method. Since we haven't done anything to get our data ready for batches, the "),s("code",[t._v("dataloaders")]),t._v(" method will fail here, but we can have a look at how it gets converted in "),s("RouterLink",{attrs:{to:"/data.core.html#Datasets"}},[s("code",[t._v("Datasets")])]),t._v(". This is where we pass the source of our data, here all of our filenames:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dsets "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dblock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("datasets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fnames"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndsets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(Path('/home/sgugger/.fastai/data/imagenette2-160/train/n03000684/n03000684_14453.JPEG'),\n Path('/home/sgugger/.fastai/data/imagenette2-160/train/n03000684/n03000684_14453.JPEG'))\n")])])]),s("p",[t._v("By default, the data block API assumes we have an input and a target, which is why we see our filename repeated twice.")]),t._v(" "),s("p",[t._v("The first thing we can do is to use a "),s("code",[t._v("get_items")]),t._v(" function to actually assemble our items inside the data block:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dblock "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("get_items "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("The difference is that you then pass as a source the folder with the images and not all the filenames:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dsets "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dblock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("datasets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndsets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(Path('/home/sgugger/.fastai/data/imagenette2-160/train/n03425413/n03425413_16978.JPEG'),\n Path('/home/sgugger/.fastai/data/imagenette2-160/train/n03425413/n03425413_16978.JPEG'))\n")])])]),s("p",[t._v("Our inputs are ready to be processed as images (since images can be built from filenames), but our target is not. We need to convert that filename to a class name. For this, fastai provides "),s("RouterLink",{attrs:{to:"/data.transforms.html#parent_label"}},[s("code",[t._v("parent_label")])]),t._v(":")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("parent_label"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fnames"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("'n03425413'\n")])])]),s("p",[t._v("This is not very readable, so since we can actually make the function we want, let's convert those obscure labels to something we can read:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("lbl_dict "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    n01440764"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'tench'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    n02102040"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'English springer'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    n02979186"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cassette player'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    n03000684"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'chain saw'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    n03028079"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'church'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    n03394916"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'French horn'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    n03417042"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'garbage truck'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    n03425413"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'gas pump'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    n03445777"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'golf ball'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    n03888257"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'parachute'")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("label_func")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fname"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" lbl_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("parent_label"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fname"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("p",[t._v("We can then tell our data block to use it to label our target by passing it as "),s("code",[t._v("get_y")]),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dblock "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("get_items "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   get_y     "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" label_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ndsets "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dblock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("datasets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndsets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(Path('/home/sgugger/.fastai/data/imagenette2-160/val/n01440764/n01440764_9931.JPEG'),\n 'tench')\n")])])]),s("p",[t._v("Now that our inputs and targets are ready, we can specify types to tell the data block API that our inputs are images and our targets are categories. Types are represented by blocks in the data block API, here we use "),s("RouterLink",{attrs:{to:"/vision.data.html#ImageBlock"}},[s("code",[t._v("ImageBlock")])]),t._v(" and "),s("RouterLink",{attrs:{to:"/data.block.html#CategoryBlock"}},[s("code",[t._v("CategoryBlock")])]),t._v(":")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dblock "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blocks    "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" CategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   get_items "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   get_y     "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" label_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ndsets "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dblock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("datasets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndsets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(PILImage mode=RGB size=240x160, TensorCategory(0))\n")])])]),s("p",[t._v("We can see how the "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock"}},[s("code",[t._v("DataBlock")])]),t._v(' automatically added the transforms necessary to open the image, or how it changed the name "cat" to an index (with a special tensor type). To do this, it created a mapping from categories to index called "vocab" that we can access this way:')],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dsets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vocab\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(#10) ['English springer','French horn','cassette player','chain saw','church','garbage truck','gas pump','golf ball','parachute','tench']\n")])])]),s("p",[t._v("Note that you can mix and match any block for input and targets, which is why the API is named data block API. You can also have more than two blocks (if you have multiple inputs and/or targets), you would just need to pass "),s("code",[t._v("n_inp")]),t._v(" to the "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock"}},[s("code",[t._v("DataBlock")])]),t._v(" to tell the library how many inputs there are (the rest would be targets) and pass a list of functions to "),s("code",[t._v("get_x")]),t._v(" and/or "),s("code",[t._v("get_y")]),t._v(" (to explain how to process each item to be ready for its type). See the object detection below for such an example.")],1),t._v(" "),s("p",[t._v("The next step is to control how our validation set is created. We do this by passing a "),s("code",[t._v("splitter")]),t._v(" to "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock"}},[s("code",[t._v("DataBlock")])]),t._v(". For instance, here is how we split by grandparent folder.")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dblock "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blocks    "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" CategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   get_items "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   get_y     "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" label_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   splitter  "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" GrandparentSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ndsets "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dblock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("datasets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndsets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(PILImage mode=RGB size=160x357, TensorCategory(6))\n")])])]),s("p",[t._v("The last step is to specify item transforms and batch transforms (the same way as we do it in "),s("RouterLink",{attrs:{to:"/vision.data.html#ImageDataLoaders"}},[s("code",[t._v("ImageDataLoaders")])]),t._v(" factory methods):")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dblock "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blocks    "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" CategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   get_items "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   get_y     "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" label_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   splitter  "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" GrandparentSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   item_tfms "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" RandomResizedCrop"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" min_scale"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.35")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                   batch_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("Normalize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_stats"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("imagenet_stats"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("With that resize, we are now able to batch items together and can finally call "),s("code",[t._v("dataloaders")]),t._v(" to convert our "),s("RouterLink",{attrs:{to:"/data.block.html#DataBlock"}},[s("code",[t._v("DataBlock")])]),t._v(" to a "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[t._v("DataLoaders")])]),t._v(" object:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dblock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_40_0.png",alt:"png"}})]),t._v(" "),s("p",[t._v("Another way to compose several functions for "),s("code",[t._v("get_y")]),t._v(" is to put them in a "),s("a",{attrs:{href:"https://fastcore.fast.ai/transform#Pipeline",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Pipeline")]),s("OutboundLink")],1),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("imagenette "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blocks "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" CategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                       get_items "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                       get_y "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("parent_label"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lbl_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__getitem__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                       splitter "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" GrandparentSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("valid_name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'val'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                       item_tfms "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" RandomResizedCrop"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" min_scale"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.35")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                       batch_tfms "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Normalize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_stats"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("imagenet_stats"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" imagenette"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_43_0.png",alt:"png"}})]),t._v(" "),s("p",[t._v("To learn more about the data block API, checkout the "),s("a",{attrs:{href:"http://docs.fast.ai/tutorial.datablock",target:"_blank",rel:"noopener noreferrer"}},[t._v("data block tutorial"),s("OutboundLink")],1),t._v("!")]),t._v(" "),s("h3",{attrs:{id:"loading-the-data-with-the-mid-level-api"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#loading-the-data-with-the-mid-level-api"}},[t._v("#")]),t._v(" Loading the data with the mid-level API")]),t._v(" "),s("p",[t._v("Now let's see how we can load the data with the medium-level API: we will learn about "),s("a",{attrs:{href:"https://fastcore.fast.ai/transform#Transform",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Transform")]),s("OutboundLink")],1),t._v("s and "),s("RouterLink",{attrs:{to:"/data.core.html#Datasets"}},[s("code",[t._v("Datasets")])]),t._v(". The beginning is the same as before: we download our data and get all our filenames:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("source "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("IMAGENETTE_160"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfnames "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("source"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Every bit of transformation we apply to our raw items (here the filenames) is called a "),s("a",{attrs:{href:"https://fastcore.fast.ai/transform#Transform",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Transform")]),s("OutboundLink")],1),t._v(" in fastai. It's basically a function with a bit of added functionality:")]),t._v(" "),s("ul",[s("li",[t._v("it can have different behavior depending on the type it receives (this is called type dispatch)")]),t._v(" "),s("li",[t._v("it will generally be applied on each element of a tuple")])]),t._v(" "),s("p",[t._v("This way, when you have a "),s("a",{attrs:{href:"https://fastcore.fast.ai/transform#Transform",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Transform")]),s("OutboundLink")],1),t._v(" like resize, you can apply it on a tuple (image, label) and it will resize the image but not the categorical label (since there is no implementation of resize for categories). The exact same transform applied on a tuple (image, mask) will resize the image and the target, using bilinear interpolation on the image and nearest neighbor on the mask. This is how the library manages to always apply data augmentation transforms on every computer vision application (segmentation, point localization or object detection).")]),t._v(" "),s("p",[t._v("Aditionnaly, a transform can have")]),t._v(" "),s("ul",[s("li",[t._v("a setup executed on the whole set (or the whole training set). This is how "),s("RouterLink",{attrs:{to:"/data.transforms.html#Categorize"}},[s("code",[t._v("Categorize")])]),t._v(" builds it vocabulary automatically.")],1),t._v(" "),s("li",[t._v("a decodes that can undo what the transform does for showing purposes (for instance "),s("RouterLink",{attrs:{to:"/data.transforms.html#Categorize"}},[s("code",[t._v("Categorize")])]),t._v(" will convert back an index into a category).")],1)]),t._v(" "),s("p",[t._v("We won't delve into those bits of the low level API here, but you can check out the "),s("a",{attrs:{href:"http://docs.fast.ai/tutorial.pets",target:"_blank",rel:"noopener noreferrer"}},[t._v("pets tutorial"),s("OutboundLink")],1),t._v(" or the more advanced "),s("a",{attrs:{href:"http://docs.fast.ai/tutorial.siamese",target:"_blank",rel:"noopener noreferrer"}},[t._v("siamese tutorial"),s("OutboundLink")],1),t._v(" for more information.")]),t._v(" "),s("p",[t._v("To open an image, we use the "),s("RouterLink",{attrs:{to:"/vision.core.html#PILImage.create"}},[s("code",[t._v("PILImage.create")])]),t._v(" transform. It will open the image and make it of the fastai type "),s("RouterLink",{attrs:{to:"/vision.core.html#PILImage"}},[s("code",[t._v("PILImage")])]),t._v(":")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("PILImage"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("create"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fnames"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_50_0.png",alt:"png"}})]),t._v(" "),s("p",[t._v("In parallel, we have already seen how to get the label of our image, using "),s("RouterLink",{attrs:{to:"/data.transforms.html#parent_label"}},[s("code",[t._v("parent_label")])]),t._v(" and "),s("code",[t._v("lbl_dict")]),t._v(":")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("lbl_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("parent_label"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fnames"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("'gas pump'\n")])])]),s("p",[t._v("To make them proper categories that are mapped to an index before being fed to the model, we need to add the "),s("RouterLink",{attrs:{to:"/data.transforms.html#Categorize"}},[s("code",[t._v("Categorize")])]),t._v(" transform. If we want to apply it directly, we need to give it a vocab (so that it knows how to associate a string with an int). We already saw that we can compose several transforms by using a "),s("a",{attrs:{href:"https://fastcore.fast.ai/transform#Pipeline",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Pipeline")]),s("OutboundLink")],1),t._v(":")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("tfm "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("parent_label"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lbl_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__getitem__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Categorize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vocab "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" lbl_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntfm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fnames"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("TensorCategory(6)\n")])])]),s("p",[t._v("Now to build our "),s("RouterLink",{attrs:{to:"/data.core.html#Datasets"}},[s("code",[t._v("Datasets")])]),t._v(" object, we need to specify:")],1),t._v(" "),s("ul",[s("li",[t._v("our raw items")]),t._v(" "),s("li",[t._v("the list of transforms that builds our inputs from the raw items")]),t._v(" "),s("li",[t._v("the list of transforms that builds our targets from the raw items")]),t._v(" "),s("li",[t._v("the split for training and validation")])]),t._v(" "),s("p",[t._v("We have everything apart from the split right now, which we can build this way:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("splits "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" GrandparentSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("valid_name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'val'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fnames"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("We can then pass all of this information to "),s("RouterLink",{attrs:{to:"/data.core.html#Datasets"}},[s("code",[t._v("Datasets")])]),t._v(".")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dsets "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Datasets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fnames"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("PILImage"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("create"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("parent_label"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lbl_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__getitem__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Categorize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" splits"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("splits"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("The main difference with what we had before is that we can just pass along "),s("RouterLink",{attrs:{to:"/data.transforms.html#Categorize"}},[s("code",[t._v("Categorize")])]),t._v(" without passing it the vocab: it will build it from the training data (which it knows from "),s("code",[t._v("items")]),t._v(" and "),s("code",[t._v("splits")]),t._v(") during its setup phase. Let's have a look at the first element:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dsets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(PILImage mode=RGB size=213x160, TensorCategory(6))\n")])])]),s("p",[t._v("We can also use our "),s("RouterLink",{attrs:{to:"/data.core.html#Datasets"}},[s("code",[t._v("Datasets")])]),t._v(" object to represent it:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dsets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dsets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_62_0.png",alt:"png"}})]),t._v(" "),s("p",[t._v("Now if we want to build a "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[t._v("DataLoaders")])]),t._v(" from this object, we need to add a few transforms that will be applied at the item level> As we saw before, those transforms will be applied separately on the inputs and targets, using the appropriate implementation for each type (which can very well be don't do anything).")],1),t._v(" "),s("p",[t._v("Here we need to:")]),t._v(" "),s("ul",[s("li",[t._v("resize our images")]),t._v(" "),s("li",[t._v("convert them to tensors")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("item_tfms "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("ToTensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" RandomResizedCrop"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" min_scale"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.35")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("p",[t._v("Additionally we will need to apply a few transforms on the batch level, namely:")]),t._v(" "),s("ul",[s("li",[t._v("convert the int tensors from images to floats, and divide every pixel by 255")]),t._v(" "),s("li",[t._v("normalize using the imagenet statistics")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("batch_tfms "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("IntToFloatTensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Normalize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_stats"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("imagenet_stats"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("p",[t._v("Those two bits could be done per item as well, but it's way more efficient to do it on a full batch.")]),t._v(" "),s("p",[t._v("Note that we have more transforms than in the data block API: there was no need to think of "),s("RouterLink",{attrs:{to:"/data.transforms.html#ToTensor"}},[s("code",[t._v("ToTensor")])]),t._v(" or "),s("RouterLink",{attrs:{to:"/data.transforms.html#IntToFloatTensor"}},[s("code",[t._v("IntToFloatTensor")])]),t._v(" there. This is because data blocks come with default item transforms and batch transforms when it concerns transforms you will always need with that type.")],1),t._v(" "),s("p",[t._v("When passing those transforms to the "),s("code",[t._v(".dataloaders")]),t._v(" method, the corresponding arguments have a slightly different name: the "),s("code",[t._v("item_tfms")]),t._v(" are passed to "),s("code",[t._v("after_item")]),t._v(" (because they are applied after the item has been formed) and the "),s("code",[t._v("batch_tfms")]),t._v(" are passed to "),s("code",[t._v("after_batch")]),t._v(" (because they are applied after the batch has been formed).")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dsets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("after_item"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("item_tfms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" after_batch"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("batch_tfms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" bs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_workers"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("We can then use the traditional "),s("code",[t._v("show_batch")]),t._v(" method:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_70_0.png",alt:"png"}})]),t._v(" "),s("h2",{attrs:{id:"training"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#training"}},[t._v("#")]),t._v(" Training")]),t._v(" "),s("p",[t._v("We will start with the usual "),s("RouterLink",{attrs:{to:"/vision.learner.html#cnn_learner"}},[s("code",[t._v("cnn_learner")])]),t._v(" function we used in the "),s("a",{attrs:{href:"http://docs.fast.ai/tutorial.vision",target:"_blank",rel:"noopener noreferrer"}},[t._v("vision tutorial"),s("OutboundLink")],1),t._v(", we will see how one can build a "),s("RouterLink",{attrs:{to:"/learner.html#Learner"}},[s("code",[t._v("Learner")])]),t._v(" object in fastai. Then we will learn how to customize")],1),t._v(" "),s("ul",[s("li",[t._v("the loss function and how to write one that works fully with fastai,")]),t._v(" "),s("li",[t._v("the optimizer function and how to use PyTorch optimizers,")]),t._v(" "),s("li",[t._v("the training loop and how to write a basic "),s("RouterLink",{attrs:{to:"/callback.core.html#Callback"}},[s("code",[t._v("Callback")])]),t._v(".")],1)]),t._v(" "),s("h3",{attrs:{id:"building-a-learner"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#building-a-learner"}},[t._v("#")]),t._v(" Building a "),s("RouterLink",{attrs:{to:"/learner.html#Learner"}},[s("code",[t._v("Learner")])])],1),t._v(" "),s("p",[t._v("The easiest way to build a "),s("RouterLink",{attrs:{to:"/learner.html#Learner"}},[s("code",[t._v("Learner")])]),t._v(" for image classification, as we have seen, is to use "),s("RouterLink",{attrs:{to:"/vision.learner.html#cnn_learner"}},[s("code",[t._v("cnn_learner")])]),t._v(". We can specify that we don't want a pretrained model by passing "),s("code",[t._v("pretrained=False")]),t._v(" (here the goal is to train a model from scratch):")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cnn_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" resnet34"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("accuracy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pretrained"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("And we can fit our model as usual:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_one_cycle"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("2.465585")]),t._v(" "),s("td",[t._v("2.208060")]),t._v(" "),s("td",[t._v("0.294777")]),t._v(" "),s("td",[t._v("00:11")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("2.404608")]),t._v(" "),s("td",[t._v("2.001794")]),t._v(" "),s("td",[t._v("0.298854")]),t._v(" "),s("td",[t._v("00:11")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("2.034411")]),t._v(" "),s("td",[t._v("2.155173")]),t._v(" "),s("td",[t._v("0.370191")]),t._v(" "),s("td",[t._v("00:10")])]),t._v(" "),s("tr",[s("td",[t._v("3")]),t._v(" "),s("td",[t._v("1.763775")]),t._v(" "),s("td",[t._v("1.583585")]),t._v(" "),s("td",[t._v("0.487643")]),t._v(" "),s("td",[t._v("00:11")])]),t._v(" "),s("tr",[s("td",[t._v("4")]),t._v(" "),s("td",[t._v("1.543254")]),t._v(" "),s("td",[t._v("1.421870")]),t._v(" "),s("td",[t._v("0.528408")]),t._v(" "),s("td",[t._v("00:13")])])])]),t._v(" "),s("p",[t._v("That's a start. But since we are not using a pretrained model, why not use a different architecture? fastai comes with a version of the resnets models that have all the tricks from modern research incorporated. While there is no pretrained model using those at the time of writing this tutorial, we can certainly use them here. For this, we just need to use the "),s("RouterLink",{attrs:{to:"/learner.html#Learner"}},[s("code",[t._v("Learner")])]),t._v(" class. It takes our "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[t._v("DataLoaders")])]),t._v(" and a PyTorch model, at the minimum. Here we can use "),s("RouterLink",{attrs:{to:"/vision.models.xresnet.html#xresnet34"}},[s("code",[t._v("xresnet34")])]),t._v(" and since we have 10 classes, we specify "),s("code",[t._v("n_out=10")]),t._v(":")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" xresnet34"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n_out"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("accuracy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("We can find a good learning rate with the learning rate finder:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lr_find"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("SuggestedLRs(lr_min=0.0013182567432522773, lr_steep=0.0010000000474974513)\n")])])]),s("p",[s("img",{attrs:{src:"output_81_2.png",alt:"png"}})]),t._v(" "),s("p",[t._v("Then fit our model:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_one_cycle"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("1.563880")]),t._v(" "),s("td",[t._v("1.668477")]),t._v(" "),s("td",[t._v("0.480764")]),t._v(" "),s("td",[t._v("00:11")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("1.187707")]),t._v(" "),s("td",[t._v("1.145329")]),t._v(" "),s("td",[t._v("0.622930")]),t._v(" "),s("td",[t._v("00:12")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("0.969200")]),t._v(" "),s("td",[t._v("0.961843")]),t._v(" "),s("td",[t._v("0.692229")]),t._v(" "),s("td",[t._v("00:11")])]),t._v(" "),s("tr",[s("td",[t._v("3")]),t._v(" "),s("td",[t._v("0.777063")]),t._v(" "),s("td",[t._v("0.785314")]),t._v(" "),s("td",[t._v("0.748280")]),t._v(" "),s("td",[t._v("00:11")])]),t._v(" "),s("tr",[s("td",[t._v("4")]),t._v(" "),s("td",[t._v("0.673000")]),t._v(" "),s("td",[t._v("0.715555")]),t._v(" "),s("td",[t._v("0.767134")]),t._v(" "),s("td",[t._v("00:12")])])])]),t._v(" "),s("p",[t._v("Wow this is a huge improvement! As we saw in all the application tutorials, we can then look at some results with:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_results"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_85_1.png",alt:"png"}})]),t._v(" "),s("p",[t._v("Now let's see how to customize each bit of the training.")]),t._v(" "),s("h3",{attrs:{id:"changing-the-loss-function"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#changing-the-loss-function"}},[t._v("#")]),t._v(" Changing the loss function")]),t._v(" "),s("p",[t._v("The loss function you pass to a "),s("RouterLink",{attrs:{to:"/learner.html#Learner"}},[s("code",[t._v("Learner")])]),t._v(" is expected to take an output and target, then return the loss. It can be any regular PyTorch function and the training loop will work without any problem. What may cause problems is when you use fastai functions like "),s("RouterLink",{attrs:{to:"/learner.html#Learner.get_preds"}},[s("code",[t._v("Learner.get_preds")])]),t._v(", "),s("RouterLink",{attrs:{to:"/learner.html#Learner.predict"}},[s("code",[t._v("Learner.predict")])]),t._v(" or "),s("RouterLink",{attrs:{to:"/learner.html#Learner.show_results"}},[s("code",[t._v("Learner.show_results")])]),t._v(".")],1),t._v(" "),s("p",[t._v("If you want "),s("RouterLink",{attrs:{to:"/learner.html#Learner.get_preds"}},[s("code",[t._v("Learner.get_preds")])]),t._v(" to work with the argument "),s("code",[t._v("with_loss=True")]),t._v(" (which is also used when you run"),s("RouterLink",{attrs:{to:"/interpret.html#ClassificationInterpretation.plot_top_losses"}},[s("code",[t._v("ClassificationInterpretation.plot_top_losses")])]),t._v(" for instance), your loss function will need a "),s("code",[t._v("reduction")]),t._v(' attribute (or argument) that you can set to "none" (this is standard for all PyTorch loss functions or classes). With a reduction of "none", the loss function does not return a single number (like a mean or sum) but something the same size as the target.')],1),t._v(" "),s("p",[t._v("As for "),s("RouterLink",{attrs:{to:"/learner.html#Learner.predict"}},[s("code",[t._v("Learner.predict")])]),t._v(" or "),s("RouterLink",{attrs:{to:"/learner.html#Learner.show_results"}},[s("code",[t._v("Learner.show_results")])]),t._v(", they internally rely on two methods your loss function should have:")],1),t._v(" "),s("ul",[s("li",[t._v("if you have a loss that combines activation and loss function (such as "),s("code",[t._v("nn.CrossEntropyLoss")]),t._v("), an "),s("code",[t._v("activation")]),t._v(" function.")]),t._v(" "),s("li",[t._v("a "),s("code",[t._v("decodes")]),t._v(" function that converts your predictions to the same format your targets are: for instance in the case of "),s("code",[t._v("nn.CrossEntropyLoss")]),t._v(", the "),s("code",[t._v("decodes")]),t._v(" function should take the argmax.it's")])]),t._v(" "),s("p",[t._v("As an example, let's look at how to implement a custom loss function doing label smoothing (this is already in fastai as "),s("RouterLink",{attrs:{to:"/layers.html#LabelSmoothingCrossEntropy"}},[s("code",[t._v("LabelSmoothingCrossEntropy")])]),t._v(").")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("LabelSmoothingCE")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Module"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" eps"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" reduction"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'mean'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("eps"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reduction "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" eps"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("reduction\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" target"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        c "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        log_preds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" F"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("log_softmax"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dim"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reduction"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sum'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" loss "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("log_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            loss "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("log_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dim"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#We divide by that size at the return line so sum and not mean")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reduction"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'mean'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("  loss "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" loss"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" loss"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("eps"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("c "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("eps"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" F"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nll_loss"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("log_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" target"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("long")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" reduction"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reduction"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("activation")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" F"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("softmax"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dim"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("decodes")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("argmax"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dim"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("We won't comment on the "),s("code",[t._v("forward")]),t._v(" pass that just implements the loss in itself. What is important is to notice how the "),s("code",[t._v("reduction")]),t._v(" attribute plays in how the final result is computed.")]),t._v(" "),s("p",[t._v("Then since this loss function combines activation (softmax) with the actual loss, we implement "),s("code",[t._v("activation")]),t._v(" that take the softmax of the output. This is what will make "),s("RouterLink",{attrs:{to:"/learner.html#Learner.get_preds"}},[s("code",[t._v("Learner.get_preds")])]),t._v(" or "),s("RouterLink",{attrs:{to:"/learner.html#Learner.predict"}},[s("code",[t._v("Learner.predict")])]),t._v(" return the actual predictions instead of the final activations.")],1),t._v(" "),s("p",[t._v("Lastly, "),s("code",[t._v("decodes")]),t._v(" changes the outputs of the model to put them in the same format as the targets (one int for each sample in the batch size) by taking the argmax of the predictions. We can pass this loss function to "),s("RouterLink",{attrs:{to:"/learner.html#Learner"}},[s("code",[t._v("Learner")])]),t._v(":")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" xresnet34"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n_out"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" loss_func"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("LabelSmoothingCE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("accuracy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_one_cycle"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("1.752499")]),t._v(" "),s("td",[t._v("1.620845")]),t._v(" "),s("td",[t._v("0.535796")]),t._v(" "),s("td",[t._v("00:11")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("1.427922")]),t._v(" "),s("td",[t._v("1.445637")]),t._v(" "),s("td",[t._v("0.610701")]),t._v(" "),s("td",[t._v("00:11")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("1.253892")]),t._v(" "),s("td",[t._v("1.305840")]),t._v(" "),s("td",[t._v("0.666242")]),t._v(" "),s("td",[t._v("00:11")])]),t._v(" "),s("tr",[s("td",[t._v("3")]),t._v(" "),s("td",[t._v("1.116652")]),t._v(" "),s("td",[t._v("1.121115")]),t._v(" "),s("td",[t._v("0.752102")]),t._v(" "),s("td",[t._v("00:12")])]),t._v(" "),s("tr",[s("td",[t._v("4")]),t._v(" "),s("td",[t._v("1.037727")]),t._v(" "),s("td",[t._v("1.076632")]),t._v(" "),s("td",[t._v("0.769427")]),t._v(" "),s("td",[t._v("00:12")])])])]),t._v(" "),s("p",[t._v("It's not training as well as before because label smoothing is a regularizing technique, so it needs more epochs to really kick in and give better results.")]),t._v(" "),s("p",[t._v("After training our model, we can indeed use "),s("code",[t._v("predict")]),t._v(" and "),s("code",[t._v("show_results")]),t._v(" and get proper results:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fnames"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("('gas pump',\n tensor(6),\n tensor([0.0110, 0.0205, 0.0441, 0.0137, 0.0504, 0.0360, 0.7745, 0.0230, 0.0195,\n         0.0072]))\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_results"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_95_1.png",alt:"png"}})]),t._v(" "),s("h3",{attrs:{id:"changing-the-optimizer"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#changing-the-optimizer"}},[t._v("#")]),t._v(" Changing the optimizer")]),t._v(" "),s("p",[t._v("fastai uses its own class of "),s("RouterLink",{attrs:{to:"/optimizer.html#Optimizer"}},[s("code",[t._v("Optimizer")])]),t._v(" built with various callbacks to refactor common functionality and provide a unique naming of hyperparameters playing the same role (like momentum in SGD, which is the same as alpha in RMSProp and beta0 in Adam) which makes it easier to schedule them (such as in "),s("RouterLink",{attrs:{to:"/callback.schedule.html#Learner.fit_one_cycle"}},[s("code",[t._v("Learner.fit_one_cycle")])]),t._v(").")],1),t._v(" "),s("p",[t._v("It implements all optimizers supported by PyTorch (and much more) so you should never need to use one coming from PyTorch. Checkout the "),s("RouterLink",{attrs:{to:"/optimizer.html"}},[s("code",[t._v("optimizer")])]),t._v(" module to see all the optimizers natively available.")],1),t._v(" "),s("p",[t._v("However in some circumstances, you might need to use an optimizer that is not in fastai (if for instance it's a new one only implemented in PyTorch). Before learning how to port the code to our internal "),s("RouterLink",{attrs:{to:"/optimizer.html#Optimizer"}},[s("code",[t._v("Optimizer")])]),t._v(" (checkout the "),s("RouterLink",{attrs:{to:"/optimizer.html"}},[s("code",[t._v("optimizer")])]),t._v(" module to discover how), you can use the "),s("RouterLink",{attrs:{to:"/optimizer.html#OptimWrapper"}},[s("code",[t._v("OptimWrapper")])]),t._v(" class to wrap your PyTorch optimizer and train with it:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@delegates")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("AdamW"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("pytorch_adamw")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("param_groups"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("kwargs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" OptimWrapper"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("AdamW"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'params'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" ps"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("kwargs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" ps "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" param_groups"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("We write an optimizer function that expects "),s("code",[t._v("param_groups")]),t._v(", which is a list of list of parameters. Then we pass those to the PyTorch optimizer we want to use.")]),t._v(" "),s("p",[t._v("We can use this function and pass it to the "),s("code",[t._v("opt_func")]),t._v(" argument of "),s("RouterLink",{attrs:{to:"/learner.html#Learner"}},[s("code",[t._v("Learner")])]),t._v(":")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" xresnet18"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lr"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("accuracy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                loss_func"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("LabelSmoothingCrossEntropy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                opt_func"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("partial"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pytorch_adamw"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wd"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.01")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" eps"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("We can then use the usual learning rate finder:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lr_find"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("SuggestedLRs(lr_min=0.09120108485221863, lr_steep=0.004365158267319202)\n")])])]),s("p",[s("img",{attrs:{src:"output_102_2.png",alt:"png"}})]),t._v(" "),s("p",[t._v("Or "),s("code",[t._v("fit_one_cycle")]),t._v(" (and thanks to the wrapper, fastai will properly schedule the beta0 of AdamW).")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_one_cycle"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("3.222441")]),t._v(" "),s("td",[t._v("2.555031")]),t._v(" "),s("td",[t._v("0.419108")]),t._v(" "),s("td",[t._v("00:09")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("2.290207")]),t._v(" "),s("td",[t._v("2.202564")]),t._v(" "),s("td",[t._v("0.575796")]),t._v(" "),s("td",[t._v("00:08")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("2.075831")]),t._v(" "),s("td",[t._v("2.144120")]),t._v(" "),s("td",[t._v("0.603567")]),t._v(" "),s("td",[t._v("00:08")])]),t._v(" "),s("tr",[s("td",[t._v("3")]),t._v(" "),s("td",[t._v("1.925448")]),t._v(" "),s("td",[t._v("1.902397")]),t._v(" "),s("td",[t._v("0.704713")]),t._v(" "),s("td",[t._v("00:08")])]),t._v(" "),s("tr",[s("td",[t._v("4")]),t._v(" "),s("td",[t._v("1.881001")]),t._v(" "),s("td",[t._v("1.880926")]),t._v(" "),s("td",[t._v("0.713121")]),t._v(" "),s("td",[t._v("00:09")])])])]),t._v(" "),s("h3",{attrs:{id:"changing-the-training-loop-with-a-callback"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#changing-the-training-loop-with-a-callback"}},[t._v("#")]),t._v(" Changing the training loop with a "),s("RouterLink",{attrs:{to:"/callback.core.html#Callback"}},[s("code",[t._v("Callback")])])],1),t._v(" "),s("p",[t._v("The base training loop in fastai is the same as PyTorch's:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" xb"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("yb "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" dl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    pred "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xb"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    loss "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" loss_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" yb"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    loss"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("backward"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    opt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("step"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    opt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zero_grad"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("where "),s("code",[t._v("model")]),t._v(", "),s("code",[t._v("loss_func")]),t._v(" and "),s("code",[t._v("opt")]),t._v(" are all attributes of our "),s("RouterLink",{attrs:{to:"/learner.html#Learner"}},[s("code",[t._v("Learner")])]),t._v(". To easily allow you to add new behavior in that training loop without needing to rewrite it yourself (along with all the fastai pieces you might want like mixed precision, 1cycle schedule, distributed training...), you can customize what happens in the training loop by writing a callback.")],1),t._v(" "),s("p",[s("RouterLink",{attrs:{to:"/callback.core.html#Callback"}},[s("code",[t._v("Callback")])]),t._v("s will be fully explained in an upcoming tutorial, but the basics are that:")],1),t._v(" "),s("ul",[s("li",[t._v("a "),s("RouterLink",{attrs:{to:"/callback.core.html#Callback"}},[s("code",[t._v("Callback")])]),t._v(" can read every piece of a "),s("RouterLink",{attrs:{to:"/learner.html#Learner"}},[s("code",[t._v("Learner")])]),t._v(", hence knowing everything happening in the training loop")],1),t._v(" "),s("li",[t._v("a "),s("RouterLink",{attrs:{to:"/callback.core.html#Callback"}},[s("code",[t._v("Callback")])]),t._v(" can change any piece of the "),s("RouterLink",{attrs:{to:"/learner.html#Learner"}},[s("code",[t._v("Learner")])]),t._v(", allowing it to alter the behavior of the training loop")],1),t._v(" "),s("li",[t._v("a "),s("RouterLink",{attrs:{to:"/callback.core.html#Callback"}},[s("code",[t._v("Callback")])]),t._v(" can even raise special exceptions that will allow breaking points (skipping a step, a validation phase, an epoch or even cancelling training entirely)")],1)]),t._v(" "),s("p",[t._v("Here we will write a simple "),s("RouterLink",{attrs:{to:"/callback.core.html#Callback"}},[s("code",[t._v("Callback")])]),t._v(" applying "),s("a",{attrs:{href:"https://arxiv.org/abs/1710.09412",target:"_blank",rel:"noopener noreferrer"}},[t._v("mixup"),s("OutboundLink")],1),t._v(" to our training (the version we will write is specific to our problem, use fastai's "),s("RouterLink",{attrs:{to:"/callback.mixup.html#MixUp"}},[s("code",[t._v("MixUp")])]),t._v(" in other settings).")],1),t._v(" "),s("p",[t._v("Mixup consists in changing the inputs by mixing two different inputs and making a linear combination of them:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("input")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" t "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" x2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Where "),s("code",[t._v("t")]),t._v(" is a random number between 0 and 1. Then, if the targets are one-hot encoded, we change the target to be")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("target "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" y1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" t "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" y2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("In practice though, targets are not one-hot encoded in PyTorch, but it's equivalent to change the part of the loss dealing with "),s("code",[t._v("y1")]),t._v(" and "),s("code",[t._v("y2")]),t._v(" by")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("loss "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" loss_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" t "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" loss_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("because the loss function used is linear with respect to y.")]),t._v(" "),s("p",[t._v("We just need to use the version with "),s("code",[t._v("reduction='none'")]),t._v(" of the loss to do this linear combination, then take the mean.")]),t._v(" "),s("p",[t._v("Here is how we write mixup in a "),s("RouterLink",{attrs:{to:"/callback.core.html#Callback"}},[s("code",[t._v("Callback")])]),t._v(":")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distributions"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("beta "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Beta\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Mixup")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Callback"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    run_valid "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" alpha"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distrib "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Beta"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("alpha"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("alpha"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("before_batch")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("t "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distrib"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sample"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("squeeze"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("device"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        shuffle "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("randperm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("device"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        x1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("y1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("shuffle"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("shuffle"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xb "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("after_loss")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" NoneReduce"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loss_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" lf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            loss "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" lf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pred"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("y1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" lf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pred"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("t\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loss "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" loss"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("We can see we write two events:")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("before_batch")]),t._v(" is executed just after drawing a batch and before the model is run on the input. We first draw our random numbers "),s("code",[t._v("t")]),t._v(", following a beta distribution (like advised in the paper) and get a shuffled version of the batch (instead of drawing a second version of the batch, we mix one batch with a shuffled version of itself). Then we set "),s("code",[t._v("self.learn.xb")]),t._v(" to the new input, which will be the on fed to the model.")]),t._v(" "),s("li",[s("code",[t._v("after_loss")]),t._v(" is executed just after the loss is computed and before the backward pass. We replace "),s("code",[t._v("self.learn.loss")]),t._v(" by the correct value. "),s("RouterLink",{attrs:{to:"/layers.html#NoneReduce"}},[s("code",[t._v("NoneReduce")])]),t._v(" is a context manager that temporarily sets the reduction attribute of a loss to 'none'.")],1)]),t._v(" "),s("p",[t._v("Also, we tell the "),s("RouterLink",{attrs:{to:"/callback.core.html#Callback"}},[s("code",[t._v("Callback")])]),t._v(" it should not run during the validation phase with "),s("code",[t._v("run_valid=False")]),t._v(".")],1),t._v(" "),s("p",[t._v("To pass a "),s("RouterLink",{attrs:{to:"/callback.core.html#Callback"}},[s("code",[t._v("Callback")])]),t._v(" to a "),s("RouterLink",{attrs:{to:"/learner.html#Learner"}},[s("code",[t._v("Learner")])]),t._v(", we use "),s("code",[t._v("cbs=")]),t._v(":")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" xresnet18"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lr"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("accuracy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                loss_func"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("LabelSmoothingCrossEntropy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cbs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("Mixup"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                opt_func"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("partial"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pytorch_adamw"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wd"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.01")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" eps"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Then we can combine this new callback with the learning rate finder:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lr_find"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("SuggestedLRs(lr_min=0.07585775852203369, lr_steep=0.005248074419796467)\n")])])]),s("p",[s("img",{attrs:{src:"output_114_2.png",alt:"png"}})]),t._v(" "),s("p",[t._v("And combine it with "),s("code",[t._v("fit_one_cycle")]),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_one_cycle"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("2.987924")]),t._v(" "),s("td",[t._v("2.508427")]),t._v(" "),s("td",[t._v("0.451465")]),t._v(" "),s("td",[t._v("00:09")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("2.480293")]),t._v(" "),s("td",[t._v("2.232779")]),t._v(" "),s("td",[t._v("0.593885")]),t._v(" "),s("td",[t._v("00:08")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("2.286732")]),t._v(" "),s("td",[t._v("2.082053")]),t._v(" "),s("td",[t._v("0.635159")]),t._v(" "),s("td",[t._v("00:10")])]),t._v(" "),s("tr",[s("td",[t._v("3")]),t._v(" "),s("td",[t._v("2.162914")]),t._v(" "),s("td",[t._v("1.828538")]),t._v(" "),s("td",[t._v("0.726115")]),t._v(" "),s("td",[t._v("00:09")])]),t._v(" "),s("tr",[s("td",[t._v("4")]),t._v(" "),s("td",[t._v("2.101327")]),t._v(" "),s("td",[t._v("1.752887")]),t._v(" "),s("td",[t._v("0.762293")]),t._v(" "),s("td",[t._v("00:09")])])])]),t._v(" "),s("p",[t._v("Like label smoothing, this is a callback that provides more regularization, so you need to run more epochs before seeing any benefit. Also, our simple implementation does not have all the tricks of the fastai's implementation, so make sure to check the official one in "),s("RouterLink",{attrs:{to:"/callback.mixup.html"}},[s("code",[t._v("callback.mixup")])]),t._v("!")],1)])}),[],!1,null,null,null);a.default=n.exports}}]);