(window.webpackJsonp=window.webpackJsonp||[]).push([[54],{379:function(t,a,s){"use strict";s.r(a);var n=s(42),e=Object(n.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"collaborative-filtering-tutorial"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#collaborative-filtering-tutorial"}},[t._v("#")]),t._v(" Collaborative filtering tutorial")]),t._v(" "),s("blockquote",[s("p",[t._v("Using the fastai library for collaborative filtering.")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" fastai"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tabular"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("all")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" fastai"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("collab "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n")])])]),s("p",[t._v("This tutorial highlights on how to quickly build a "),s("RouterLink",{attrs:{to:"/learner.html#Learner"}},[s("code",[t._v("Learner")])]),t._v(" and train a model on collaborative filtering tasks.")],1),t._v(" "),s("h2",{attrs:{id:"training-a-model"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#training-a-model"}},[t._v("#")]),t._v(" Training a model")]),t._v(" "),s("p",[t._v("For this tutorial, we will use the "),s("a",{attrs:{href:"https://grouplens.org/datasets/movielens/100k/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Movielens 100k data dataset"),s("OutboundLink")],1),t._v(". We can download it easily and decompress it with the following function:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("path "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ML_100k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("The main table is in "),s("code",[t._v("u.data")]),t._v(". Since it's not a proper csv, we have to specify a few things while opening it: the tab delimiter, the columns we want to keep and their names.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("ratings "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'u.data'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" delimiter"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" header"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      usecols"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" names"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'user'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'movie'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'rating'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nratings"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("user")]),t._v(" "),s("th",[t._v("movie")]),t._v(" "),s("th",[t._v("rating")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("196")]),t._v(" "),s("td",[t._v("242")]),t._v(" "),s("td",[t._v("3")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("186")]),t._v(" "),s("td",[t._v("302")]),t._v(" "),s("td",[t._v("3")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("22")]),t._v(" "),s("td",[t._v("377")]),t._v(" "),s("td",[t._v("1")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("244")]),t._v(" "),s("td",[t._v("51")]),t._v(" "),s("td",[t._v("2")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("166")]),t._v(" "),s("td",[t._v("346")]),t._v(" "),s("td",[t._v("1")])])])])]),t._v(" "),s("p",[t._v("Movie ids are not ideal to look at things, so we load the corresponding movie id to the title that is in the table "),s("code",[t._v("u.item")]),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("movies "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'u.item'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  delimiter"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'|'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" encoding"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'latin-1'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                     usecols"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" names"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'movie'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'title'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" header"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmovies"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("movie")]),t._v(" "),s("th",[t._v("title")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v("Toy Story (1995)")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("2")]),t._v(" "),s("td",[t._v("GoldenEye (1995)")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("3")]),t._v(" "),s("td",[t._v("Four Rooms (1995)")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("4")]),t._v(" "),s("td",[t._v("Get Shorty (1995)")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("Copycat (1995)")])])])])]),t._v(" "),s("p",[t._v("Next we merge it to our ratings table:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("ratings "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ratings"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("merge"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("movies"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nratings"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("user")]),t._v(" "),s("th",[t._v("movie")]),t._v(" "),s("th",[t._v("rating")]),t._v(" "),s("th",[t._v("title")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("196")]),t._v(" "),s("td",[t._v("242")]),t._v(" "),s("td",[t._v("3")]),t._v(" "),s("td",[t._v("Kolya (1996)")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("63")]),t._v(" "),s("td",[t._v("242")]),t._v(" "),s("td",[t._v("3")]),t._v(" "),s("td",[t._v("Kolya (1996)")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("226")]),t._v(" "),s("td",[t._v("242")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("Kolya (1996)")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("154")]),t._v(" "),s("td",[t._v("242")]),t._v(" "),s("td",[t._v("3")]),t._v(" "),s("td",[t._v("Kolya (1996)")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("306")]),t._v(" "),s("td",[t._v("242")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("Kolya (1996)")])])])])]),t._v(" "),s("p",[t._v("We can then build a "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[t._v("DataLoaders")])]),t._v(" object from this table. By default, it takes the first column for user, the second column for the item (here our movies) and the third column for the ratings. We need to change the value of "),s("code",[t._v("item_name")]),t._v(" in our case, to use the titles instead of the ids:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" CollabDataLoaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ratings"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" item_name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'title'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" bs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("In all applications, when the data has been assembled in a "),s("RouterLink",{attrs:{to:"/data.core.html#DataLoaders"}},[s("code",[t._v("DataLoaders")])]),t._v(", you can have a look at it with the "),s("code",[t._v("show_batch")]),t._v(" method:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("user")]),t._v(" "),s("th",[t._v("title")]),t._v(" "),s("th",[t._v("rating")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("181")]),t._v(" "),s("td",[t._v("Substitute, The (1996)")]),t._v(" "),s("td",[t._v("1")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("189")]),t._v(" "),s("td",[t._v("Ulee's Gold (1997)")]),t._v(" "),s("td",[t._v("3")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("6")]),t._v(" "),s("td",[t._v("L.A. Confidential (1997)")]),t._v(" "),s("td",[t._v("4")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("849")]),t._v(" "),s("td",[t._v("Net, The (1995)")]),t._v(" "),s("td",[t._v("5")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("435")]),t._v(" "),s("td",[t._v("Blade Runner (1982)")]),t._v(" "),s("td",[t._v("4")])]),t._v(" "),s("tr",[s("th",[t._v("5")]),t._v(" "),s("td",[t._v("718")]),t._v(" "),s("td",[t._v("My Best Friend's Wedding (1997)")]),t._v(" "),s("td",[t._v("4")])]),t._v(" "),s("tr",[s("th",[t._v("6")]),t._v(" "),s("td",[t._v("279")]),t._v(" "),s("td",[t._v("I Love Trouble (1994)")]),t._v(" "),s("td",[t._v("2")])]),t._v(" "),s("tr",[s("th",[t._v("7")]),t._v(" "),s("td",[t._v("561")]),t._v(" "),s("td",[t._v("Clockwork Orange, A (1971)")]),t._v(" "),s("td",[t._v("4")])]),t._v(" "),s("tr",[s("th",[t._v("8")]),t._v(" "),s("td",[t._v("87")]),t._v(" "),s("td",[t._v("Fish Called Wanda, A (1988)")]),t._v(" "),s("td",[t._v("5")])]),t._v(" "),s("tr",[s("th",[t._v("9")]),t._v(" "),s("td",[t._v("774")]),t._v(" "),s("td",[t._v("Crow, The (1994)")]),t._v(" "),s("td",[t._v("3")])])])]),t._v(" "),s("p",[t._v("fastai can create and train a collaborative filtering model by using "),s("RouterLink",{attrs:{to:"/collab.html#collab_learner"}},[s("code",[t._v("collab_learner")])]),t._v(":")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" collab_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_factors"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_range"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5.5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("It uses a simple dot product model with 50 latent factors. To train it using the 1cycle policy, we just run this command:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_one_cycle"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wd"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.967653")]),t._v(" "),s("td",[t._v("0.942309")]),t._v(" "),s("td",[t._v("00:10")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.843426")]),t._v(" "),s("td",[t._v("0.869254")]),t._v(" "),s("td",[t._v("00:10")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("0.733788")]),t._v(" "),s("td",[t._v("0.823143")]),t._v(" "),s("td",[t._v("00:10")])]),t._v(" "),s("tr",[s("td",[t._v("3")]),t._v(" "),s("td",[t._v("0.593507")]),t._v(" "),s("td",[t._v("0.811050")]),t._v(" "),s("td",[t._v("00:10")])]),t._v(" "),s("tr",[s("td",[t._v("4")]),t._v(" "),s("td",[t._v("0.480942")]),t._v(" "),s("td",[t._v("0.811475")]),t._v(" "),s("td",[t._v("00:10")])])])]),t._v(" "),s("p",[t._v("Here's "),s("a",{attrs:{href:"https://www.librec.net/release/v1.3/example.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("some benchmarks"),s("OutboundLink")],1),t._v(" on the same dataset for the popular Librec system for collaborative filtering. They show best results based on RMSE of 0.91 (scroll down to the 100k dataset), which corresponds to an MSE of "),s("code",[t._v("0.91**2 = 0.83")]),t._v(". So in less than a minute, we got pretty good results!")]),t._v(" "),s("h2",{attrs:{id:"interpretation"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#interpretation"}},[t._v("#")]),t._v(" Interpretation")]),t._v(" "),s("p",[t._v("Let's analyze the results of our previous model. We will keep the 1000 most rated movies for this:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("g "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ratings"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("groupby"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'title'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'rating'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("count"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntop_movies "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" g"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sort_values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ascending"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ntop_movies"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("array(['Star Wars (1977)', 'Contact (1997)', 'Fargo (1996)',\n       'Return of the Jedi (1983)', 'Liar Liar (1997)',\n       'English Patient, The (1996)', 'Scream (1996)', 'Toy Story (1995)',\n       'Air Force One (1997)', 'Independence Day (ID4) (1996)'],\n      dtype=object)\n")])])]),s("h3",{attrs:{id:"movie-bias"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#movie-bias"}},[t._v("#")]),t._v(" Movie bias")]),t._v(" "),s("p",[t._v('Our model has learned one bias per movie, a unique number independent of users that can be interpreted as the intrinsic "value" of the movie. We can grab the bias of each movie in our '),s("code",[t._v("top_movies")]),t._v(" list with the following command:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("movie_bias "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bias"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("top_movies"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_item"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmovie_bias"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("torch.Size([1000])\n")])])]),s("p",[t._v("Let's compare those biases with the average ratings:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("mean_ratings "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ratings"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("groupby"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'title'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'rating'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmovie_ratings "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mean_ratings"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("b "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("zip")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("top_movies"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("movie_bias"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("p",[t._v("Now let's have a look at the movies with the worst bias:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("item0 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sorted")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("movie_ratings"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("item0"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("[(tensor(-0.3489),\n  'Children of the Corn: The Gathering (1996)',\n  1.3157894736842106),\n (tensor(-0.3407), 'Leave It to Beaver (1997)', 1.8409090909090908),\n (tensor(-0.3304), 'Cable Guy, The (1996)', 2.339622641509434),\n (tensor(-0.2763),\n  'Lawnmower Man 2: Beyond Cyberspace (1996)',\n  1.7142857142857142),\n (tensor(-0.2607), \"McHale's Navy (1997)\", 2.1884057971014492),\n (tensor(-0.2572), 'Grease 2 (1982)', 2.0),\n (tensor(-0.2482), 'Kansas City (1996)', 2.260869565217391),\n (tensor(-0.2479), 'Crow: City of Angels, The (1996)', 1.9487179487179487),\n (tensor(-0.2388), 'Free Willy 3: The Rescue (1997)', 1.7407407407407407),\n (tensor(-0.2338), 'Keys to Tulsa (1997)', 2.24),\n (tensor(-0.2305), 'Beautician and the Beast, The (1997)', 2.313953488372093),\n (tensor(-0.2205), 'Escape from L.A. (1996)', 2.4615384615384617),\n (tensor(-0.2192), 'Beverly Hills Ninja (1997)', 2.3125),\n (tensor(-0.2179), 'Mortal Kombat: Annihilation (1997)', 1.9534883720930232),\n (tensor(-0.2150), 'Thinner (1996)', 2.4489795918367347)]\n")])])]),s("p",[t._v("Or the ones with the best bias:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sorted")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("movie_ratings"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" reverse"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("[(tensor(0.6052), 'As Good As It Gets (1997)', 4.196428571428571),\n (tensor(0.5778), 'Titanic (1997)', 4.2457142857142856),\n (tensor(0.5565), 'Shawshank Redemption, The (1994)', 4.445229681978798),\n (tensor(0.5460), 'L.A. Confidential (1997)', 4.161616161616162),\n (tensor(0.5264), 'Silence of the Lambs, The (1991)', 4.28974358974359),\n (tensor(0.5125), 'Star Wars (1977)', 4.3584905660377355),\n (tensor(0.4862), \"Schindler's List (1993)\", 4.466442953020135),\n (tensor(0.4851), 'Rear Window (1954)', 4.3875598086124405),\n (tensor(0.4671), 'Godfather, The (1972)', 4.283292978208232),\n (tensor(0.4668), 'Apt Pupil (1998)', 4.1),\n (tensor(0.4614), \"One Flew Over the Cuckoo's Nest (1975)\", 4.291666666666667),\n (tensor(0.4606), 'Good Will Hunting (1997)', 4.262626262626263),\n (tensor(0.4572), 'Contact (1997)', 3.8035363457760316),\n (tensor(0.4529), 'Close Shave, A (1995)', 4.491071428571429),\n (tensor(0.4410), 'Wrong Trousers, The (1993)', 4.466101694915254)]\n")])])]),s("p",[t._v("There is certainly a strong correlation!")]),t._v(" "),s("h3",{attrs:{id:"movie-weights"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#movie-weights"}},[t._v("#")]),t._v(" Movie weights")]),t._v(" "),s("p",[t._v("Now let's try to analyze the latent factors our model has learned. We can grab the weights for each movie in "),s("code",[t._v("top_movies")]),t._v(" the same way as we did for the bias before.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("movie_w "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weight"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("top_movies"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_item"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmovie_w"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("torch.Size([1000, 50])\n")])])]),s("p",[t._v("Let's try a PCA to reduce the dimensions and see if we can see what the model learned:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("movie_pca "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" movie_w"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pca"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmovie_pca"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("torch.Size([1000, 3])\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("fac0"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("fac1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("fac2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" movie_pca"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmovie_comp "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("i "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("zip")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fac0"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" top_movies"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("p",[t._v("Here are the highest score on the first dimension:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sorted")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("movie_comp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("itemgetter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" reverse"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("[(tensor(1.1481), 'Casablanca (1942)'),\n (tensor(1.0816), 'Chinatown (1974)'),\n (tensor(1.0486), 'Lawrence of Arabia (1962)'),\n (tensor(1.0459), 'Wrong Trousers, The (1993)'),\n (tensor(1.0282), 'Secrets & Lies (1996)'),\n (tensor(1.0245), '12 Angry Men (1957)'),\n (tensor(1.0095), 'Some Folks Call It a Sling Blade (1993)'),\n (tensor(0.9874), 'Close Shave, A (1995)'),\n (tensor(0.9800), 'Wallace & Gromit: The Best of Aardman Animation (1996)'),\n (tensor(0.9791), 'Citizen Kane (1941)')]\n")])])]),s("p",[t._v("And the worst:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sorted")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("movie_comp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("itemgetter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("[(tensor(-1.2520), 'Home Alone 3 (1997)'),\n (tensor(-1.2118), 'Jungle2Jungle (1997)'),\n (tensor(-1.1282), 'Stupids, The (1996)'),\n (tensor(-1.1229), 'Free Willy 3: The Rescue (1997)'),\n (tensor(-1.1161), 'Leave It to Beaver (1997)'),\n (tensor(-1.0821), 'Children of the Corn: The Gathering (1996)'),\n (tensor(-1.0703), \"McHale's Navy (1997)\"),\n (tensor(-1.0695), 'Bio-Dome (1996)'),\n (tensor(-1.0652), 'Batman & Robin (1997)'),\n (tensor(-1.0627), 'Cowboy Way, The (1994)')]\n")])])]),s("p",[t._v("Same thing for our second dimension:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("movie_comp "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("i "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("zip")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fac1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" top_movies"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sorted")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("movie_comp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("itemgetter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" reverse"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("[(tensor(1.1196), 'Braveheart (1995)'),\n (tensor(1.0969), 'Raiders of the Lost Ark (1981)'),\n (tensor(1.0365), 'Independence Day (ID4) (1996)'),\n (tensor(0.9631), 'Titanic (1997)'),\n (tensor(0.9450), 'American President, The (1995)'),\n (tensor(0.8893), 'Forrest Gump (1994)'),\n (tensor(0.8757), 'Hunt for Red October, The (1990)'),\n (tensor(0.8638), 'Pretty Woman (1990)'),\n (tensor(0.8019), 'Miracle on 34th Street (1994)'),\n (tensor(0.7956), 'True Lies (1994)')]\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sorted")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("movie_comp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("itemgetter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("[(tensor(-0.9231), 'Ready to Wear (Pret-A-Porter) (1994)'),\n (tensor(-0.8948), 'Dead Man (1995)'),\n (tensor(-0.8816), 'Clockwork Orange, A (1971)'),\n (tensor(-0.8697), 'Three Colors: Blue (1993)'),\n (tensor(-0.8425), 'Beavis and Butt-head Do America (1996)'),\n (tensor(-0.8047), 'Cable Guy, The (1996)'),\n (tensor(-0.7832), 'Nosferatu (Nosferatu, eine Symphonie des Grauens) (1922)'),\n (tensor(-0.7662), 'Exotica (1994)'),\n (tensor(-0.7546), 'Spice World (1997)'),\n (tensor(-0.7491), 'Heavenly Creatures (1994)')]\n")])])]),s("p",[t._v("And we can even plot the movies according to their scores on those dimensions:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("idxs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("choice"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("top_movies"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" replace"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nidxs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nX "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" fac0"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("idxs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nY "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" fac2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("idxs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nplt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("figure"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("figsize"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nplt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("scatter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("zip")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("top_movies"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("idxs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" X"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    plt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" color"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rand"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.7")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fontsize"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("11")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nplt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"output_49_0.png",alt:"png"}})])])}),[],!1,null,null,null);a.default=e.exports}}]);